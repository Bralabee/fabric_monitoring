{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ VERIFY INSTALLATION\n",
    "# Since you have uploaded the .whl to your Fabric Environment, it should be installed automatically.\n",
    "# Run this cell to confirm the correct version (v0.1.1) is loaded.\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    version = importlib.metadata.version(\"usf_fabric_monitoring\")\n",
    "    print(f\"‚úÖ Library found: usf_fabric_monitoring v{version}\")\n",
    "    \n",
    "    if version == \"0.1.1\":\n",
    "        print(\"   You are using the correct version.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Expected v0.1.1 but found v{version}.\")\n",
    "        print(\"   Please check your Fabric Environment settings and ensure the new wheel is published.\")\n",
    "        \n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"‚ùå Library NOT found.\")\n",
    "    print(\"   Please ensure you have attached the 'Fabric Environment' containing the .whl file to this notebook.\")\n",
    "    print(\"   Alternatively, upload the .whl file to the Lakehouse 'Files' section and pip install it from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf136",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook executes the **Monitor Hub Analysis Pipeline**, which is designed to provide deep insights into Microsoft Fabric activity. It extracts historical data, calculates key performance metrics, and generates comprehensive reports to help identify:\n",
    "- Constant failures and reliability issues.\n",
    "- Excess activity by users, locations, or domains.\n",
    "- Historical performance trends over the last 90 days.\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package**: The first cell installs the `usf_fabric_monitoring` package into the current session.\n",
    "2. **Configure Credentials**: Ensure your Service Principal credentials (`AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`) are available. The notebook attempts to load them from a `.env` file in the Lakehouse or you can configure Azure Key Vault.\n",
    "3. **Set Parameters**:\n",
    "    - `DAYS_TO_ANALYZE`: Number of days of history to fetch (default: 90).\n",
    "    - `OUTPUT_DIR`: Lakehouse path where reports will be saved.\n",
    "4. **Run Analysis**: Execute the pipeline cell. It will:\n",
    "    - Fetch data from Fabric APIs.\n",
    "    - Process and enrich the data.\n",
    "    - Save CSV reports to the specified `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import usf_fabric_monitoring\n",
    "\n",
    "print(f\"üì¶ Package Location: {os.path.dirname(usf_fabric_monitoring.__file__)}\")\n",
    "\n",
    "# Verify we are running the NEW code (v0.1.1)\n",
    "try:\n",
    "    src = inspect.getsource(MonitorHubPipeline._prepare_extraction_directory)\n",
    "    if \"raw_data\" in src:\n",
    "        print(\"‚úÖ SUCCESS: You are running the updated code (v0.1.1).\")\n",
    "        print(\"   Output will go to: .../raw_data/ (Persistent Cache)\")\n",
    "    else:\n",
    "        print(\"‚ùå WARNING: You are still running the OLD code (v0.1.0).\")\n",
    "        print(\"   Output will go to: .../extracted/TIMESTAMP/ (Legacy)\")\n",
    "        print(\"   üëâ ACTION: Restart the kernel and run the install cell above again.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not verify source code: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CREDENTIAL MANAGEMENT ---\n",
    "\n",
    "# Option 1: Load from .env file in Lakehouse (Easiest migration)\n",
    "# Upload your .env file to the 'Files' section of your Lakehouse\n",
    "ENV_PATH = \"/lakehouse/default/Files/dot_env_files/.env\"\n",
    "if os.path.exists(ENV_PATH):\n",
    "    print(f\"Loading configuration from {ENV_PATH}\")\n",
    "    load_dotenv(ENV_PATH)\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {ENV_PATH}\")\n",
    "\n",
    "# Option 2: Load from Azure Key Vault (Best Practice)\n",
    "# Uncomment and configure this section to use Azure Key Vault\n",
    "# try:\n",
    "#     from notebookutils import mssparkutils\n",
    "#     KEY_VAULT_NAME = \"YourKeyVaultName\"\n",
    "#     os.environ[\"AZURE_CLIENT_ID\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Client-ID\")\n",
    "#     os.environ[\"AZURE_CLIENT_SECRET\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Client-Secret\")\n",
    "#     os.environ[\"AZURE_TENANT_ID\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Tenant-ID\")\n",
    "# except ImportError:\n",
    "#     pass # Not running in Fabric or notebookutils not available\n",
    "# except Exception as e:\n",
    "#     print(f\"Key Vault access failed: {e}\")\n",
    "\n",
    "# Verify credentials are present\n",
    "required_vars = [\"AZURE_CLIENT_ID\", \"AZURE_CLIENT_SECRET\", \"AZURE_TENANT_ID\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"‚úÖ Credentials configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DAYS_TO_ANALYZE = 28\n",
    "OUTPUT_DIR = \"/lakehouse/default/Files/monitor_hub_analysis\" # Adjust for Fabric Lakehouse path if needed\n",
    "# Or use local path for testing\n",
    "# OUTPUT_DIR = \"exports/monitor_hub_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "results = pipeline.run_complete_analysis(days=DAYS_TO_ANALYZE)\n",
    "pipeline.print_results_summary(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
