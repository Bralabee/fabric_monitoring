{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5790268",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis\n",
    "\n",
    "This notebook provides an interactive environment to run the Monitor Hub Analysis pipeline and explore the results.\n",
    "\n",
    "## Recent Updates (v0.1.14)\n",
    "- **CSV-Based Analysis**: The notebook has been reverted to use **CSV Reports** (`activities_master_*.csv`) as the source for analysis, ensuring compatibility with existing workflows and avoiding Parquet mount issues.\n",
    "- **Strict Authentication Enforcement**: The authentication module now **strictly enforces** the use of Service Principal credentials if they are provided in the `.env` file.\n",
    "- **Smart Scope Detection**: The pipeline attempts **Tenant-Wide** extraction first (Admin APIs). If permissions are missing (403 Forbidden), it automatically falls back to **Member-Only** scope.\n",
    "- **Pipeline Integration**: The notebook uses the updated `MonitorHubPipeline` class for end-to-end execution.\n",
    "\n",
    "## Usage\n",
    "1. Ensure your environment is activated: `conda activate fabric-monitoring`\n",
    "2. Run the cells below to execute the analysis.\n",
    "3. The pipeline will:\n",
    "    - Extract historical data (Tenant-Wide with Fallback).\n",
    "    - Enrich data with job details.\n",
    "    - Generate CSV reports in the `exports/monitor_hub_analysis` directory (or configured output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1bad86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Library found: usf_fabric_monitoring v0.1.6\n",
      "   You are using the correct version.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ VERIFY INSTALLATION\n",
    "# Since we have uploaded the .whl to your Fabric Environment, it should be installed automatically.\n",
    "# Run this cell to confirm the correct version (v0.1.14) is loaded.\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    version = importlib.metadata.version(\"usf_fabric_monitoring\")\n",
    "    print(f\"‚úÖ Library found: usf_fabric_monitoring v{version}\")\n",
    "    \n",
    "    if version >= \"0.1.14\":\n",
    "        print(\"   You are using the correct version.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Expected v0.1.14+ but found v{version}.\")\n",
    "        print(\"   Please check your Fabric Environment settings and ensure the new wheel is published.\")\n",
    "        \n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"‚ùå Library NOT found.\")\n",
    "    print(\"   Please ensure you have attached the 'Fabric Environment' containing the .whl file to this notebook.\")\n",
    "    print(\"   Alternatively, upload the .whl file to the Lakehouse 'Files' section and pip install it from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf136",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook executes the **Monitor Hub Analysis Pipeline**, which is designed to provide deep insights into Microsoft Fabric activity. It extracts historical data, calculates key performance metrics, and generates comprehensive reports to help identify:\n",
    "- Constant failures and reliability issues.\n",
    "- Excess activity by users, locations, or domains.\n",
    "- Historical performance trends over the last 90 days.\n",
    "\n",
    "## Key Features & Recent Updates (v0.1.14)\n",
    "The pipeline has been enhanced to support enterprise-grade monitoring workflows:\n",
    "\n",
    "1.  **CSV-Based Analysis (v0.1.14)**:\n",
    "    -   **Source of Truth**: The notebook now loads data from the generated `activities_master_*.csv` reports.\n",
    "    -   **Benefit**: Ensures consistent analysis using the same data that is exported to stakeholders, avoiding format discrepancies.\n",
    "\n",
    "2.  **Strict Authentication (v0.1.13)**:\n",
    "    -   **Problem**: Previous versions would silently fall back to a restricted identity if the Service Principal login failed.\n",
    "    -   **Solution**: The system now raises an immediate error if configured credentials fail, forcing you to fix the root cause.\n",
    "\n",
    "3.  **Smart Scope Detection (v0.1.12)**:\n",
    "    -   **Primary Strategy**: Attempts to use Power BI Admin APIs for full **Tenant-Wide** visibility.\n",
    "    -   **Automatic Fallback**: If Admin permissions are missing (401/403), it gracefully reverts to **Member-Only** mode.\n",
    "\n",
    "4.  **Automatic Persistence & Path Resolution**:\n",
    "    -   **Automatic Lakehouse Resolution**: Relative paths (e.g., `exports/`) are automatically mapped to `/lakehouse/default/Files/` in Fabric.\n",
    "    -   **Sequential Orchestration**: Handles the entire data lifecycle (Activity Extraction -> Job Detail Extraction -> Merging -> Analysis).\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package**: The first cell installs the `usf_fabric_monitoring` package into the current session.\n",
    "2. **Configure Credentials**: Ensure your Service Principal credentials (`AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`) are available.\n",
    "3. **Set Parameters**:\n",
    "    - `DAYS_TO_ANALYZE`: Number of days of history to fetch (default: 90).\n",
    "    - `OUTPUT_DIR`: Path where reports will be saved (can now be relative!).\n",
    "4. **Run Analysis**: Execute the pipeline cell. It will:\n",
    "    - Fetch data from Fabric APIs.\n",
    "    - Process and enrich the data.\n",
    "    - Save CSV reports and Parquet files to the specified `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15df1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cce96",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "\n",
    "print(f\"üì¶ Package Location: {os.path.dirname(usf_fabric_monitoring.__file__)}\")\n",
    "\n",
    "# Verify we are running the NEW code (v0.1.14)\n",
    "try:\n",
    "    # Check for the new _save_to_parquet method in pipeline which indicates v0.1.8+\n",
    "    src = inspect.getsource(MonitorHubPipeline)\n",
    "    if \"_save_to_parquet\" in src:\n",
    "        print(\"‚úÖ SUCCESS: You are running the updated code (v0.1.14).\")\n",
    "        print(\"   Feature Verified: CSV Analysis & Strict Auth\")\n",
    "    else:\n",
    "        print(\"‚ùå WARNING: You are still running the OLD code.\")\n",
    "        print(\"   üëâ ACTION: Restart the kernel and run the install cell above again.\")\n",
    "except AttributeError:\n",
    "    print(\"‚ùå WARNING: Could not inspect source code. You might be running an optimized .pyc version.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not verify source code: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349952b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CREDENTIAL MANAGEMENT ---\n",
    "\n",
    "# Option 1: Load from .env file (Lakehouse or Local)\n",
    "# We check the Lakehouse path first, then fallback to local .env\n",
    "LAKEHOUSE_ENV_PATH = \"/lakehouse/default/Files/dot_env_files/.env\"\n",
    "LOCAL_ENV_PATH = \".env\"\n",
    "\n",
    "# Force override=True to ensure we pick up changes to the file even if env vars are already set\n",
    "if os.path.exists(LAKEHOUSE_ENV_PATH):\n",
    "    print(f\"Loading configuration from Lakehouse: {LAKEHOUSE_ENV_PATH}\")\n",
    "    load_dotenv(LAKEHOUSE_ENV_PATH, override=True)\n",
    "elif os.path.exists(LOCAL_ENV_PATH):\n",
    "    print(f\"Loading configuration from Local: {os.path.abspath(LOCAL_ENV_PATH)}\")\n",
    "    load_dotenv(LOCAL_ENV_PATH, override=True)\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {LAKEHOUSE_ENV_PATH} or {LOCAL_ENV_PATH}\")\n",
    "\n",
    "# Verify credentials are present\n",
    "required_vars = [\"AZURE_CLIENT_ID\", \"AZURE_CLIENT_SECRET\", \"AZURE_TENANT_ID\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "\n",
    "print(\"\\nüîê IDENTITY CHECK:\")\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"   ‚ö†Ô∏è  System will fallback to DefaultAzureCredential (User Identity or Managed Identity)\")\n",
    "else:\n",
    "    client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    masked_id = f\"{client_id[:4]}...{client_id[-4:]}\" if client_id and len(client_id) > 8 else \"********\"\n",
    "    print(f\"‚úÖ Service Principal Configured in Environment\")\n",
    "    print(f\"   Client ID: {masked_id}\")\n",
    "    print(f\"   Tenant ID: {os.getenv('AZURE_TENANT_ID')}\")\n",
    "\n",
    "# --- TOKEN IDENTITY INSPECTION ---\n",
    "# This block decodes the actual token being used to prove identity\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.auth import create_authenticator_from_env\n",
    "    auth = create_authenticator_from_env()\n",
    "    token = auth.get_fabric_token()\n",
    "    \n",
    "    # Decode JWT (no signature verification needed for inspection)\n",
    "    parts = token.split('.')\n",
    "    if len(parts) > 1:\n",
    "        # Add padding if needed\n",
    "        payload_part = parts[1]\n",
    "        padded = payload_part + '=' * (4 - len(payload_part) % 4)\n",
    "        decoded = base64.urlsafe_b64decode(padded)\n",
    "        claims = json.loads(decoded)\n",
    "        \n",
    "        print(\"\\nüïµÔ∏è  ACTIVE TOKEN IDENTITY:\")\n",
    "        if 'upn' in claims:\n",
    "            print(f\"   User Principal Name: {claims['upn']}\")\n",
    "            print(\"   üëâ You are logged in as a USER.\")\n",
    "        elif 'appid' in claims:\n",
    "            print(f\"   Application ID: {claims['appid']}\")\n",
    "            if client_id and claims['appid'] == client_id:\n",
    "                print(\"   üëâ You are logged in as the CONFIGURED SERVICE PRINCIPAL.\")\n",
    "            else:\n",
    "                print(\"   üëâ You are logged in as a DIFFERENT Service Principal/Managed Identity.\")\n",
    "        else:\n",
    "            print(f\"   Subject: {claims.get('sub', 'Unknown')}\")\n",
    "            \n",
    "        print(f\"   Audience: {claims.get('aud', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not inspect token identity: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37982a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DAYS_TO_ANALYZE = 28\n",
    "\n",
    "# OUTPUT_DIR: Where to save the reports.\n",
    "# v0.1.6+ Update: You can now provide a relative path (e.g., \"monitor_hub_analysis\") \n",
    "# and it will automatically resolve to \"/lakehouse/default/Files/monitor_hub_analysis\" \n",
    "# when running in Fabric.\n",
    "OUTPUT_DIR = \"monitor_hub_analysis\" \n",
    "\n",
    "# If you prefer an explicit absolute path, you can still use it:\n",
    "# OUTPUT_DIR = \"/lakehouse/default/Files/monitor_hub_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b70a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "results = pipeline.run_complete_analysis(days=DAYS_TO_ANALYZE)\n",
    "pipeline.print_results_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c61f0",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis & Visualization (Spark)\n",
    "The following cells use PySpark to load the raw data generated by the pipeline and provide interactive visualizations of failures, error codes, and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Spark & Paths\n",
    "import os\n",
    "import glob\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# Initialize Spark Session (if not already active)\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, to_timestamp, when, count, desc, lit, unix_timestamp, coalesce, abs as abs_val, split, initcap, regexp_replace, element_at, substring, avg, max, min\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "    if 'spark' not in locals() or spark is None:\n",
    "        print(\"‚öôÔ∏è Initializing Spark Session...\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"FabricFailureAnalysis\") \\\n",
    "            .getOrCreate()\n",
    "        print(f\"‚úÖ Spark Session Created: {spark.version}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PySpark not installed or configured. Skipping Spark-based analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to initialize Spark: {e}. Skipping Spark-based analysis.\")\n",
    "\n",
    "# Resolve the output directory to an absolute path\n",
    "# This ensures that if you used a relative path like \"monitor_hub_analysis\",\n",
    "# it is correctly resolved to \"/lakehouse/default/Files/monitor_hub_analysis\" for Spark.\n",
    "resolved_output_dir = str(resolve_path(OUTPUT_DIR))\n",
    "\n",
    "BASE_PATH = os.path.join(resolved_output_dir, \"fabric_item_details\")\n",
    "AUDIT_LOG_PATH = os.path.join(resolved_output_dir, \"raw_data/daily\")\n",
    "\n",
    "print(f\"üìÇ Analysis Paths:\")\n",
    "print(f\"  - Item Details: {BASE_PATH}\")\n",
    "print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec636d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data from CSV (Aggregated Reports)\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, coalesce, initcap, regexp_replace, element_at, split, when, lit\n",
    "\n",
    "# Use relative path for CSVs to avoid mount issues\n",
    "CSV_PATH = \"Files/monitor_hub_analysis\"\n",
    "\n",
    "def load_csv_data():\n",
    "    \"\"\"Loads the activity data from the generated CSV reports.\"\"\"\n",
    "    try:\n",
    "        # Match the master activities report\n",
    "        path_pattern = f\"{CSV_PATH}/activities_master_*.csv\"\n",
    "        print(f\"üìÇ Loading CSV files from {path_pattern}...\")\n",
    "        \n",
    "        # Read CSV with header\n",
    "        # inferSchema=True allows Spark to detect dates and numbers automatically\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_pattern)\n",
    "        \n",
    "        # Filter for Failures\n",
    "        if \"status\" in df.columns:\n",
    "            return df.filter(col(\"status\") == \"Failed\")\n",
    "        elif \"Status\" in df.columns:\n",
    "            return df.filter(col(\"Status\") == \"Failed\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è 'status' column not found in CSV data.\")\n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load CSV data: {str(e)}\")\n",
    "        print(\"   Tip: Ensure the pipeline ran successfully and generated CSV reports.\")\n",
    "        return None\n",
    "\n",
    "# Execute Loading\n",
    "final_df = load_csv_data()\n",
    "\n",
    "if final_df:\n",
    "    print(f\"‚úÖ Successfully loaded {final_df.count()} failure records from CSV.\")\n",
    "    \n",
    "    # Helper to safely get column or null\n",
    "    def safe_col(c):\n",
    "        return col(c) if c in final_df.columns else lit(None)\n",
    "\n",
    "    # Map CSV columns to expected analysis columns\n",
    "    final_df = final_df.select(\n",
    "        # Try to get workspace name, fallback to ID if name missing in older CSVs\n",
    "        coalesce(safe_col(\"workspace_name\"), safe_col(\"WorkSpaceName\"), safe_col(\"workspace_id\")).alias(\"Workspace\"),\n",
    "        coalesce(safe_col(\"item_name\"), safe_col(\"ItemName\")).alias(\"Item Name\"),\n",
    "        coalesce(safe_col(\"item_type\"), safe_col(\"ItemType\")).alias(\"Item Type\"),\n",
    "        coalesce(safe_col(\"activity_type\"), safe_col(\"Operation\")).alias(\"Invoke Type\"),\n",
    "        coalesce(safe_col(\"start_time\"), safe_col(\"CreationTime\")).alias(\"Start Time\"),\n",
    "        coalesce(safe_col(\"end_time\"), safe_col(\"EndTime\")).alias(\"End Time\"),\n",
    "        coalesce(safe_col(\"duration_seconds\"), safe_col(\"Duration\")).alias(\"Duration (s)\"),\n",
    "        coalesce(safe_col(\"submitted_by\"), safe_col(\"UserId\")).alias(\"User ID\"),\n",
    "        \n",
    "        # User Name Extraction\n",
    "        coalesce(\n",
    "            initcap(regexp_replace(element_at(split(coalesce(safe_col(\"submitted_by\"), safe_col(\"UserId\")), \"@\"), 1), \"\\\\.\", \" \")),\n",
    "            safe_col(\"submitted_by\"), \n",
    "            safe_col(\"UserId\")\n",
    "        ).alias(\"User Name\"),\n",
    "        \n",
    "        # Error Details - CSV might not have structured failure_reason\n",
    "        # We use a placeholder or look for error columns if they exist\n",
    "        lit(None).alias(\"Error Code\"), \n",
    "        lit(None).alias(\"Error Message\")\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå No failure data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b851589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analysis & Display\n",
    "\n",
    "if final_df:\n",
    "    # --- 1. Summary Statistics ---\n",
    "    total_failures = final_df.count()\n",
    "    unique_workspaces = final_df.select(\"Workspace\").distinct().count()\n",
    "    unique_items = final_df.select(\"Item Name\").distinct().count()\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY STATISTICS\")\n",
    "    print(f\"Total Failures: {total_failures}\")\n",
    "    print(f\"Affected Workspaces: {unique_workspaces}\")\n",
    "    print(f\"Affected Items: {unique_items}\")\n",
    "\n",
    "    # --- 2. Top 10 Failing Items ---\n",
    "    print(\"\\nüèÜ TOP 10 FAILING ITEMS\")\n",
    "    top_items = final_df.groupBy(\"Workspace\", \"Item Name\", \"Item Type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(10)\n",
    "    top_items.show(truncate=False)\n",
    "\n",
    "    # --- 3. Failures by User ---\n",
    "    print(\"\\nüë§ FAILURES BY USER\")\n",
    "    user_stats = final_df.groupBy(\"User Name\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    user_stats.show(truncate=False)\n",
    "\n",
    "    # --- 4. Error Code Distribution ---\n",
    "    print(\"\\n‚ö†Ô∏è ERROR CODE DISTRIBUTION\")\n",
    "    error_stats = final_df.groupBy(\"Error Code\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    error_stats.show(truncate=False)\n",
    "\n",
    "    # --- 5. Recent Failures (Last 20) ---\n",
    "    print(\"\\nüïí MOST RECENT FAILURES\")\n",
    "    final_df.select(\"Start Time\", \"Workspace\", \"Item Name\", \"User Name\", \"Error Message\") \\\n",
    "        .orderBy(col(\"Start Time\").desc()) \\\n",
    "        .show(20, truncate=50)\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "e818f402-5fea-490c-9eeb-2e9258522102",
    "workspaceId": "944ae69e-1c99-4ad7-973f-c296c778a5c5"
   },
   "lakehouse": {
    "default_lakehouse": "5cb78b9b-b153-4771-b309-65ec4848433a",
    "default_lakehouse_name": "lh_01_bronze",
    "default_lakehouse_workspace_id": "944ae69e-1c99-4ad7-973f-c296c778a5c5",
    "known_lakehouses": [
     {
      "id": "5cb78b9b-b153-4771-b309-65ec4848433a"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
