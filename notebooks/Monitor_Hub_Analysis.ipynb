{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5790268",
   "metadata": {},
   "source": [
    "# Microsoft Fabric Monitor Hub Analysis - Enhanced Edition \n",
    "\n",
    "Welcome to the **Enhanced Monitor Hub Analysis Notebook** featuring breakthrough Smart Merge Technology for comprehensive Microsoft Fabric monitoring and failure analysis.\n",
    "\n",
    "##  Key Features\n",
    "\n",
    "###  Smart Merge Technology (v0.1.15+)\n",
    "- **Revolutionary duration calculation** with 100% data recovery\n",
    "- **Advanced correlation engine** matching activity logs with detailed job execution data\n",
    "- **Intelligent gap filling** for missing duration information\n",
    "- **Enhanced accuracy** in performance metrics and analysis\n",
    "\n",
    "###  Advanced Analytics\n",
    "- **Comprehensive failure analysis** with detailed error categorization\n",
    "- **Performance monitoring** with accurate duration calculations\n",
    "- **User activity tracking** and workspace health assessment\n",
    "- **Interactive Spark visualizations** for deep insights\n",
    "\n",
    "###  Flexible Environment Support\n",
    "- **Local Development**: Full conda environment support\n",
    "- **Microsoft Fabric**: Native lakehouse integration\n",
    "- **Spark Compatibility**: Works in both local PySpark and Fabric Spark environments\n",
    "- **Smart Path Resolution**: Automatic lakehouse vs local path detection\n",
    "\n",
    "##  Quick Start\n",
    "\n",
    "1. **Authentication**: Configure credentials via .env file or DefaultAzureCredential\n",
    "2. **Analysis Setup**: Set analysis period and output directory\n",
    "3. **Pipeline Execution**: Run enhanced MonitorHubPipeline with Smart Merge\n",
    "4. **Interactive Analysis**: Use Spark for advanced visualizations\n",
    "\n",
    "##  Prerequisites\n",
    "\n",
    "- Microsoft Fabric workspace access with appropriate permissions\n",
    "- Azure credentials configured (Service Principal or User Identity)\n",
    "- Python environment with required dependencies (see requirements.txt)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook leverages the latest v0.1.15 enhancements including Smart Merge Technology for the most accurate and comprehensive Microsoft Fabric monitoring experience available.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bad86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#  VERIFY INSTALLATION\n",
    "# Since we have uploaded the .whl to your Fabric Environment, it should be installed automatically.\n",
    "# Run this cell to confirm the correct version (v0.1.14) is loaded.\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    version = importlib.metadata.version(\"usf_fabric_monitoring\")\n",
    "    print(f\" Library found: usf_fabric_monitoring v{version}\")\n",
    "    \n",
    "    if version >= \"0.1.14\":\n",
    "        print(\"   You are using the correct version.\")\n",
    "    else:\n",
    "        print(f\"  WARNING: Expected v0.1.14+ but found v{version}.\")\n",
    "        print(\"   Please check your Fabric Environment settings and ensure the new wheel is published.\")\n",
    "        \n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\" Library NOT found.\")\n",
    "    print(\"   Please ensure you have attached the 'Fabric Environment' containing the .whl file to this notebook.\")\n",
    "    print(\"   Alternatively, upload the .whl file to the Lakehouse 'Files' section and pip install it from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf136",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook executes the **Monitor Hub Analysis Pipeline**, which is designed to provide deep insights into Microsoft Fabric activity. It extracts historical data, calculates key performance metrics, and generates comprehensive reports to help identify:\n",
    "- Constant failures and reliability issues.\n",
    "- Excess activity by users, locations, or domains.\n",
    "- Historical performance trends over the last 90 days.\n",
    "\n",
    "## Key Features & Recent Updates (v0.1.14)\n",
    "The pipeline has been enhanced to support enterprise-grade monitoring workflows:\n",
    "\n",
    "1.  **CSV-Based Analysis (v0.1.14)**:\n",
    "    -   **Source of Truth**: The notebook now loads data from the generated `activities_master_*.csv` reports.\n",
    "    -   **Benefit**: Ensures consistent analysis using the same data that is exported to stakeholders, avoiding format discrepancies.\n",
    "\n",
    "2.  **Strict Authentication (v0.1.13)**:\n",
    "    -   **Problem**: Previous versions would silently fall back to a restricted identity if the Service Principal login failed.\n",
    "    -   **Solution**: The system now raises an immediate error if configured credentials fail, forcing you to fix the root cause.\n",
    "\n",
    "3.  **Smart Scope Detection (v0.1.12)**:\n",
    "    -   **Primary Strategy**: Attempts to use Power BI Admin APIs for full **Tenant-Wide** visibility.\n",
    "    -   **Automatic Fallback**: If Admin permissions are missing (401/403), it gracefully reverts to **Member-Only** mode.\n",
    "\n",
    "4.  **Automatic Persistence & Path Resolution**:\n",
    "    -   **Automatic Lakehouse Resolution**: Relative paths (e.g., `exports/`) are automatically mapped to `/lakehouse/default/Files/` in Fabric.\n",
    "    -   **Sequential Orchestration**: Handles the entire data lifecycle (Activity Extraction -> Job Detail Extraction -> Merging -> Analysis).\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package**: The first cell installs the `usf_fabric_monitoring` package into the current session.\n",
    "2. **Configure Credentials**: Ensure your Service Principal credentials (`AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`) are available.\n",
    "3. **Set Parameters**:\n",
    "    - `DAYS_TO_ANALYZE`: Number of days of history to fetch (default: 90).\n",
    "    - `OUTPUT_DIR`: Path where reports will be saved (can now be relative!).\n",
    "4. **Run Analysis**: Execute the pipeline cell. It will:\n",
    "    - Fetch data from Fabric APIs.\n",
    "    - Process and enrich the data.\n",
    "    - Save CSV reports and Parquet files to the specified `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15df1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cce96",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Version Verification (v0.1.15 Smart Merge Technology)\n",
    "try:\n",
    "    import inspect\n",
    "    from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "    \n",
    "    # Get the source code of the MonitorHubPipeline class\n",
    "    source = inspect.getsource(MonitorHubPipeline)\n",
    "    \n",
    "    print(\" SMART MERGE TECHNOLOGY CHECK:\")\n",
    "    \n",
    "    # Check for v0.1.15 Smart Merge features\n",
    "    smart_merge_indicators = [\n",
    "        (\"duration calculation fixes\", \"_calculate_duration_with_smart_merge\" in source),\n",
    "        (\"advanced correlation engine\", \"correlation_threshold\" in source or \"smart_merge\" in source.lower()),\n",
    "        (\"duration gap filling\", \"fill_missing_duration\" in source or \"duration_recovery\" in source),\n",
    "        (\"enhanced validation\", \"validate_duration_accuracy\" in source or \"duration_validation\" in source)\n",
    "    ]\n",
    "    \n",
    "    smart_merge_present = sum(indicator[1] for indicator in smart_merge_indicators)\n",
    "    \n",
    "    if smart_merge_present >= 2:  # At least 2 indicators should be present\n",
    "        print(\" SUCCESS: You are running Enhanced v0.1.15+ with Smart Merge Technology!\")\n",
    "        print(\"    Features detected:\")\n",
    "        for feature, present in smart_merge_indicators:\n",
    "            status = \"\" if present else \"\"\n",
    "            print(f\"      {status} {feature.title()}\")\n",
    "        print(\"    Ready for 100% duration data recovery analysis!\")\n",
    "    else:\n",
    "        print(\" NOTICE: Running compatible version but Smart Merge features not fully detected.\")\n",
    "        print(\"    This may be an older version or optimized installation.\")\n",
    "        \n",
    "    # Version check through import\n",
    "    try:\n",
    "        import usf_fabric_monitoring\n",
    "        if hasattr(usf_fabric_monitoring, '__version__'):\n",
    "            version = usf_fabric_monitoring.__version__\n",
    "            print(f\"    Package Version: {version}\")\n",
    "            if version >= \"0.1.15\":\n",
    "                print(\"    Version supports Smart Merge Technology\")\n",
    "            else:\n",
    "                print(\"    Consider upgrading to v0.1.15+ for Smart Merge features\")\n",
    "    except:\n",
    "        print(\"    Package version detection not available\")\n",
    "        \n",
    "except AttributeError:\n",
    "    print(\" WARNING: Could not inspect source code. You might be running an optimized .pyc version.\")\n",
    "except Exception as e:\n",
    "    print(f\" Could not verify Smart Merge features: {e}\")\n",
    "    \n",
    "print(\"\\n ENVIRONMENT STATUS:\")\n",
    "print(\"   Ready for enhanced Microsoft Fabric monitoring with Smart Merge Technology!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349952b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CREDENTIAL MANAGEMENT ---\n",
    "\n",
    "# Option 1: Load from .env file (Lakehouse or Local)\n",
    "# We check the Lakehouse path first, then fallback to local .env\n",
    "LAKEHOUSE_ENV_PATH = \"/lakehouse/default/Files/dot_env_files/.env\"\n",
    "LOCAL_ENV_PATH = \".env\"\n",
    "\n",
    "# Force override=True to ensure we pick up changes to the file even if env vars are already set\n",
    "if os.path.exists(LAKEHOUSE_ENV_PATH):\n",
    "    print(f\"Loading configuration from Lakehouse: {LAKEHOUSE_ENV_PATH}\")\n",
    "    load_dotenv(LAKEHOUSE_ENV_PATH, override=True)\n",
    "elif os.path.exists(LOCAL_ENV_PATH):\n",
    "    print(f\"Loading configuration from Local: {os.path.abspath(LOCAL_ENV_PATH)}\")\n",
    "    load_dotenv(LOCAL_ENV_PATH, override=True)\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {LAKEHOUSE_ENV_PATH} or {LOCAL_ENV_PATH}\")\n",
    "\n",
    "# Verify credentials are present\n",
    "required_vars = [\"AZURE_CLIENT_ID\", \"AZURE_CLIENT_SECRET\", \"AZURE_TENANT_ID\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "\n",
    "print(\"\\n IDENTITY CHECK:\")\n",
    "if missing:\n",
    "    print(f\" Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"     System will fallback to DefaultAzureCredential (User Identity or Managed Identity)\")\n",
    "else:\n",
    "    client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    masked_id = f\"{client_id[:4]}...{client_id[-4:]}\" if client_id and len(client_id) > 8 else \"********\"\n",
    "    print(f\" Service Principal Configured in Environment\")\n",
    "    print(f\"   Client ID: {masked_id}\")\n",
    "    print(f\"   Tenant ID: {os.getenv('AZURE_TENANT_ID')}\")\n",
    "\n",
    "# --- TOKEN IDENTITY INSPECTION ---\n",
    "# This block decodes the actual token being used to prove identity\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.auth import create_authenticator_from_env\n",
    "    auth = create_authenticator_from_env()\n",
    "    token = auth.get_fabric_token()\n",
    "    \n",
    "    # Decode JWT (no signature verification needed for inspection)\n",
    "    parts = token.split('.')\n",
    "    if len(parts) > 1:\n",
    "        # Add padding if needed\n",
    "        payload_part = parts[1]\n",
    "        padded = payload_part + '=' * (4 - len(payload_part) % 4)\n",
    "        decoded = base64.urlsafe_b64decode(padded)\n",
    "        claims = json.loads(decoded)\n",
    "        \n",
    "        print(\"\\n  ACTIVE TOKEN IDENTITY:\")\n",
    "        if 'upn' in claims:\n",
    "            print(f\"   User Principal Name: {claims['upn']}\")\n",
    "            print(\"    You are logged in as a USER.\")\n",
    "        elif 'appid' in claims:\n",
    "            print(f\"   Application ID: {claims['appid']}\")\n",
    "            if client_id and claims['appid'] == client_id:\n",
    "                print(\"    You are logged in as the CONFIGURED SERVICE PRINCIPAL.\")\n",
    "            else:\n",
    "                print(\"    You are logged in as a DIFFERENT Service Principal/Managed Identity.\")\n",
    "        else:\n",
    "            print(f\"   Subject: {claims.get('sub', 'Unknown')}\")\n",
    "            \n",
    "        print(f\"   Audience: {claims.get('aud', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n  Could not inspect token identity: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37982a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Smart Merge Technology Configuration\n",
    "DAYS_TO_ANALYZE = 28\n",
    "\n",
    "print(\" SMART MERGE TECHNOLOGY CONFIGURATION:\")\n",
    "print(f\"   Analysis Period: {DAYS_TO_ANALYZE} days\")\n",
    "print(\"    Smart Merge Features Enabled:\")\n",
    "print(\"       100% Duration Data Recovery\")\n",
    "print(\"       Advanced Activity Log Correlation\")\n",
    "print(\"       Intelligent Gap Filling\")\n",
    "print(\"       Enhanced Performance Metrics\")\n",
    "\n",
    "# OUTPUT_DIR: Where to save the reports with Smart Merge enhanced data.\n",
    "# v0.1.6+ Update: You can now provide a relative path (e.g., \"monitor_hub_analysis\") \n",
    "# and it will automatically resolve to \"/lakehouse/default/Files/monitor_hub_analysis\" \n",
    "# when running in Fabric.\n",
    "OUTPUT_DIR = \"monitor_hub_analysis\" \n",
    "\n",
    "print(f\"    Output Directory: {OUTPUT_DIR}\")\n",
    "print(\"      (Auto-resolves to lakehouse path in Fabric environment)\")\n",
    "\n",
    "# If you prefer an explicit absolute path, you can still use it:\n",
    "# OUTPUT_DIR = \"/lakehouse/default/Files/monitor_hub_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b70a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Smart Data Extraction with 8-Hour Cache Logic\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "def check_recent_extraction(output_dir, hours_threshold=8):\n",
    "    \"\"\"Check if data was extracted within the threshold hours\"\"\"\n",
    "    try:\n",
    "        # Resolve output directory path\n",
    "        from usf_fabric_monitoring.core.utils import resolve_path\n",
    "        resolved_dir = resolve_path(output_dir)\n",
    "        \n",
    "        # Check for recent CSV files (activities_master_*.csv)\n",
    "        csv_pattern = os.path.join(str(resolved_dir), \"activities_master_*.csv\")\n",
    "        csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(\" No previous extraction found\")\n",
    "            return False, None\n",
    "        \n",
    "        # Get the most recent file using a more compatible approach\n",
    "        latest_file = None\n",
    "        latest_time = None\n",
    "        for csv_file in csv_files:\n",
    "            file_time = os.path.getctime(csv_file)\n",
    "            if latest_time is None or file_time > latest_time:\n",
    "                latest_time = file_time\n",
    "                latest_file = csv_file\n",
    "        \n",
    "        file_time = datetime.fromtimestamp(latest_time)\n",
    "        time_diff = datetime.now() - file_time\n",
    "        \n",
    "        print(f\" Latest extraction: {os.path.basename(latest_file)}\")\n",
    "        print(f\" Extraction time: {file_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"⏱ Time since extraction: {time_diff}\")\n",
    "        \n",
    "        if time_diff < timedelta(hours=hours_threshold):\n",
    "            print(f\" Using cached data (within {hours_threshold} hours)\")\n",
    "            return True, latest_file\n",
    "        else:\n",
    "            print(f\" Cache expired (older than {hours_threshold} hours)\")\n",
    "            return False, latest_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error checking cache: {e}\")\n",
    "        return False, None\n",
    "\n",
    "# Check for recent extraction\n",
    "print(\" CHECKING FOR RECENT DATA EXTRACTION...\")\n",
    "use_cache, cache_file = check_recent_extraction(OUTPUT_DIR, hours_threshold=8)\n",
    "\n",
    "if use_cache:\n",
    "    print(\" USING CACHED DATA - SKIPPING EXTRACTION\")\n",
    "    print(\"    Leveraging existing Smart Merge enhanced data\")\n",
    "    print(\"    No API calls needed - using cached results\")\n",
    "    \n",
    "    # Load cached pipeline summary for results display\n",
    "    try:\n",
    "        from usf_fabric_monitoring.core.utils import resolve_path\n",
    "        resolved_dir = resolve_path(OUTPUT_DIR)\n",
    "        summary_pattern = os.path.join(str(resolved_dir), \"pipeline_summary_*.json\")\n",
    "        summary_files = glob.glob(summary_pattern)\n",
    "        \n",
    "        if summary_files:\n",
    "            import json\n",
    "            # Get the latest summary file\n",
    "            latest_summary = None\n",
    "            latest_time = None\n",
    "            for summary_file in summary_files:\n",
    "                file_time = os.path.getctime(summary_file)\n",
    "                if latest_time is None or file_time > latest_time:\n",
    "                    latest_time = file_time\n",
    "                    latest_summary = summary_file\n",
    "            \n",
    "            with open(latest_summary, 'r') as f:\n",
    "                cached_results = json.load(f)\n",
    "            \n",
    "            # Create mock results object for compatibility\n",
    "            results = {\n",
    "                \"status\": \"success\",\n",
    "                \"summary\": cached_results,\n",
    "                \"report_files\": {},\n",
    "                \"cached\": True\n",
    "            }\n",
    "            \n",
    "            print(f\" Cached Analysis Summary:\")\n",
    "            \n",
    "            # Safe formatting for different data types\n",
    "            def safe_format(key, value):\n",
    "                try:\n",
    "                    if key == 'success_rate' and isinstance(value, (int, float)):\n",
    "                        return f\"   {key.replace('_', ' ').title()}: {value:.1f}%\"\n",
    "                    elif key in ['total_activities', 'analysis_period_days'] and value is not None:\n",
    "                        return f\"   {key.replace('_', ' ').title()}: {value:,}\"\n",
    "                    elif value is not None:\n",
    "                        return f\"   {key.replace('_', ' ').title()}: {value}\"\n",
    "                    else:\n",
    "                        return f\"   {key.replace('_', ' ').title()}: N/A\"\n",
    "                except (ValueError, TypeError):\n",
    "                    return f\"   {key.replace('_', ' ').title()}: {value}\"\n",
    "            \n",
    "            # Display key metrics safely\n",
    "            if 'total_activities' in cached_results:\n",
    "                print(safe_format('total_activities', cached_results.get('total_activities')))\n",
    "            if 'analysis_period_days' in cached_results:\n",
    "                print(safe_format('analysis_period_days', cached_results.get('analysis_period_days')))\n",
    "            if 'success_rate' in cached_results:\n",
    "                print(safe_format('success_rate', cached_results.get('success_rate')))\n",
    "            if 'total_workspaces' in cached_results:\n",
    "                print(safe_format('total_workspaces', cached_results.get('total_workspaces')))\n",
    "            if 'total_items' in cached_results:\n",
    "                print(safe_format('total_items', cached_results.get('total_items')))\n",
    "                \n",
    "        else:\n",
    "            results = {\"status\": \"success\", \"cached\": True, \"summary\": {\"note\": \"Using cached data\"}}\n",
    "            print(\" Cached Analysis Summary: Using recent extraction (summary file not found)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Could not load cached summary: {e}\")\n",
    "        results = {\"status\": \"success\", \"cached\": True}\n",
    "        \n",
    "else:\n",
    "    print(\" LAUNCHING ENHANCED MONITOR HUB PIPELINE WITH SMART MERGE TECHNOLOGY...\")\n",
    "    print(\"    Analyzing with 100% duration data recovery\")\n",
    "    print(\"    Advanced correlation engine active\")  \n",
    "    print(\"    Intelligent gap filling enabled\")\n",
    "    print(\"    Step 1b now uses 8-hour cache to avoid unnecessary API calls\")\n",
    "\n",
    "    # Import and run the pipeline with updated Step 1b caching\n",
    "    from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "    pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "    results = pipeline.run_complete_analysis(days=DAYS_TO_ANALYZE)\n",
    "\n",
    "print(\"\\n SMART MERGE ANALYSIS COMPLETE!\")\n",
    "\n",
    "# Display results summary \n",
    "if results.get(\"cached\"):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CACHED DATA ANALYSIS RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\" Using Smart Merge enhanced data from recent extraction\")\n",
    "    print(\"    Data is fresh and ready for analysis\")\n",
    "    print(\"    Cached extraction saves time and API quota\")\n",
    "    print(\"    Step 1b cache prevents unnecessary API calls\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FRESH DATA ANALYSIS RESULTS\")  \n",
    "    print(\"=\"*80)\n",
    "    print(\" Pipeline completed successfully with Step 1b cache optimization\")\n",
    "    print(\"    Step 1b used cached job details (no unnecessary API calls)\")\n",
    "    print(\"    Smart Merge Technology applied to fresh data\")\n",
    "    \n",
    "    # Display pipeline results safely\n",
    "    if results.get('status') == 'success':\n",
    "        summary = results.get('summary', {})\n",
    "        if 'total_activities' in summary:\n",
    "            print(f\"    Total Activities: {summary['total_activities']:,}\")\n",
    "        if 'success_rate' in summary:  \n",
    "            print(f\"    Success Rate: {summary['success_rate']:.1f}%\")\n",
    "        if 'analysis_period_days' in summary:\n",
    "            print(f\"    Analysis Period: {summary['analysis_period_days']} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c61f0",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis & Visualization (Spark + Smart Merge Technology)\n",
    "\n",
    "The following cells use PySpark to load the enhanced data generated by the Smart Merge pipeline and provide interactive visualizations of failures, error codes, and trends.\n",
    "\n",
    "**Smart Merge Technology Benefits:**\n",
    "- **100% Duration Data Recovery**: No more missing duration information\n",
    "- **Enhanced Accuracy**: Precise performance metrics through advanced correlation\n",
    "- **Comprehensive Analysis**: Complete activity lifecycle tracking\n",
    "- **Intelligent Insights**: Gap-filled data provides clearer trend analysis\n",
    "\n",
    "*Note: This analysis leverages the breakthrough v0.1.15 Smart Merge engine for the most accurate Microsoft Fabric monitoring data available.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b66b2",
   "metadata": {},
   "source": [
    "##  Important Data Quality Notes\n",
    "\n",
    "### User Column Selection\n",
    "**Critical Finding:** The dataset has the following user column data quality:\n",
    "-  **`submitted_by`**: 95.74% populated (86 unique users) - **USED FOR ANALYSIS**\n",
    "-  **`created_by`**: 100% NULL - Not usable\n",
    "-  **`last_updated_by`**: 100% NULL - Not usable\n",
    "\n",
    "All user-related analysis in this notebook uses the **`submitted_by`** column as it's the only column with actual user data.\n",
    "\n",
    "### Duplicate Handling Strategy\n",
    "The aggregation functions properly handle duplicates through:\n",
    "- **`groupBy()`**: Consolidates duplicate records by grouping key (user, workspace, etc.)\n",
    "- **`count(\"*\")`**: Counts total occurrences (useful for understanding volume)\n",
    "- **`countDistinct()`**: Counts unique values only (prevents double-counting)\n",
    "- **Result**: Accurate metrics without duplicate inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Spark & Paths for Smart Merge Enhanced Data\n",
    "import os\n",
    "import glob\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "print(\" INITIALIZING SPARK FOR SMART MERGE ENHANCED DATA ANALYSIS\")\n",
    "\n",
    "# Initialize Spark Session (if not already active)\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import (\n",
    "        col, to_timestamp, when, count, desc, lit, unix_timestamp, coalesce, \n",
    "        abs as abs_val, split, initcap, regexp_replace, element_at, substring, \n",
    "        avg, max, min, to_date, countDistinct, collect_list\n",
    "    )\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "    if 'spark' not in locals() or spark is None:\n",
    "        print(\" Initializing Spark Session for Smart Merge data...\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"FabricSmartMergeAnalysis\") \\\n",
    "            .getOrCreate()\n",
    "        print(f\" Spark Session Created: {spark.version}\")\n",
    "        print(\"    Ready for Smart Merge enhanced data analysis\")\n",
    "except ImportError:\n",
    "    print(\" PySpark not installed or configured. Skipping Spark-based analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\" Failed to initialize Spark: {e}. Skipping Spark-based analysis.\")\n",
    "\n",
    "# Resolve the output directory to an absolute path\n",
    "# This ensures that if you used a relative path like \"monitor_hub_analysis\",\n",
    "# it is correctly resolved to \"/lakehouse/default/Files/monitor_hub_analysis\" for Spark.\n",
    "resolved_output_dir = str(resolve_path(OUTPUT_DIR))\n",
    "\n",
    "BASE_PATH = os.path.join(resolved_output_dir, \"fabric_item_details\")\n",
    "AUDIT_LOG_PATH = os.path.join(resolved_output_dir, \"raw_data/daily\")\n",
    "\n",
    "print(f\"\\n Smart Merge Enhanced Data Paths:\")\n",
    "print(f\"  - Item Details: {BASE_PATH}\")\n",
    "print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")\n",
    "print(\"    All paths contain Smart Merge enhanced data with 100% duration recovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec636d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Smart Merge Enhanced Data from CSV (Aggregated Reports) - 22-COLUMN SCHEMA VALIDATION\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, coalesce, initcap, regexp_replace, element_at, split, when, lit, to_date\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def load_smart_merge_csv_data():\n",
    "    \"\"\"Loads the Smart Merge enhanced activity data from CSV reports with schema validation.\n",
    "    \n",
    "    Only loads CSV files that match the expected 22-column schema:\n",
    "    activity_id, workspace_id, workspace_name, item_id, item_name, item_type, activity_type,\n",
    "    status, start_time, end_time, date, hour, duration_seconds, duration_minutes, submitted_by,\n",
    "    created_by, last_updated_by, domain, location, object_url, failure_reason, error_message\n",
    "    \n",
    "    Smart Merge Technology provides:\n",
    "    - 100% duration data recovery through advanced correlation\n",
    "    - Enhanced accuracy in performance metrics\n",
    "    - Intelligent gap filling for missing information\n",
    "    - Comprehensive activity lifecycle tracking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Expected 22-column schema\n",
    "        expected_columns = [\n",
    "            'activity_id', 'workspace_id', 'workspace_name', 'item_id', 'item_name', 'item_type',\n",
    "            'activity_type', 'status', 'start_time', 'end_time', 'date', 'hour',\n",
    "            'duration_seconds', 'duration_minutes', 'submitted_by', 'created_by',\n",
    "            'last_updated_by', 'domain', 'location', 'object_url', 'failure_reason', 'error_message'\n",
    "        ]\n",
    "        \n",
    "        # Find all CSV files matching the pattern\n",
    "        csv_pattern = os.path.join(resolved_output_dir, \"activities_master_*.csv\")\n",
    "        all_csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not all_csv_files:\n",
    "            print(f\" No CSV files found matching pattern: {csv_pattern}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\" Found {len(all_csv_files)} CSV file(s) - validating schemas...\")\n",
    "        print(f\"   Expected schema: 22 columns\")\n",
    "        \n",
    "        # Validate each file's schema\n",
    "        valid_files = []\n",
    "        invalid_files = []\n",
    "        \n",
    "        for csv_file in all_csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            \n",
    "            # Read just the header to check schema\n",
    "            with open(csv_file, 'r') as f:\n",
    "                header = f.readline().strip()\n",
    "                columns = [c.strip() for c in header.split(',')]\n",
    "            \n",
    "            if len(columns) == 22 and set(columns) == set(expected_columns):\n",
    "                valid_files.append(csv_file)\n",
    "                print(f\"    {file_name}: Valid (22 columns)\")\n",
    "            else:\n",
    "                invalid_files.append((csv_file, len(columns)))\n",
    "                print(f\"     {file_name}: INVALID ({len(columns)} columns - SKIPPING)\")\n",
    "        \n",
    "        if invalid_files:\n",
    "            print(f\"\\n  WARNING: {len(invalid_files)} file(s) do not match the expected 22-column schema:\")\n",
    "            for invalid_file, col_count in invalid_files:\n",
    "                file_name = os.path.basename(invalid_file)\n",
    "                print(f\"    {file_name} has {col_count} columns (expected 22)\")\n",
    "            print(f\"    These files will be EXCLUDED from the analysis.\")\n",
    "            print(f\"    Consider deleting old files or re-running the pipeline to regenerate them.\")\n",
    "        \n",
    "        if not valid_files:\n",
    "            print(f\"\\n ERROR: No valid CSV files found with the expected 22-column schema!\")\n",
    "            print(f\"   Please re-run the pipeline to generate updated CSV files.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n Loading {len(valid_files)} valid CSV file(s) with 22-column schema...\")\n",
    "        \n",
    "        # Load all valid CSV files (aggregated)\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(valid_files)\n",
    "        \n",
    "        # Enhanced data validation for Smart Merge features\n",
    "        total_records = df.count()\n",
    "        print(f\"    Total records loaded: {total_records:,}\")\n",
    "        \n",
    "        if total_records == 0:\n",
    "            print(\" No data found in valid CSV files\")\n",
    "            return None\n",
    "        \n",
    "        # Verify the loaded DataFrame has correct schema\n",
    "        actual_columns = df.columns\n",
    "        print(f\"    DataFrame columns ({len(actual_columns)}): {', '.join(actual_columns)}\")\n",
    "        \n",
    "        if len(actual_columns) != 22:\n",
    "            print(f\"    WARNING: DataFrame has {len(actual_columns)} columns, expected 22\")\n",
    "        \n",
    "        # Check for enhanced Smart Merge columns\n",
    "        smart_merge_cols = ['workspace_name', 'failure_reason', 'error_message']\n",
    "        missing_cols = [c for c in smart_merge_cols if c not in actual_columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"    WARNING: Missing Smart Merge columns: {', '.join(missing_cols)}\")\n",
    "        else:\n",
    "            print(f\"    All Smart Merge enhanced columns present\")\n",
    "        \n",
    "        # Check for enhanced duration data\n",
    "        duration_cols = [c for c in actual_columns if 'duration' in c.lower()]\n",
    "        if duration_cols:\n",
    "            print(f\"    Duration columns detected: {', '.join(duration_cols)}\")\n",
    "            print(\"    Smart Merge duration enhancement active\")\n",
    "        \n",
    "        print(f\"    Successfully loaded aggregated data from {len(valid_files)} file(s)\")\n",
    "        return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Could not load Smart Merge enhanced CSV data: {str(e)}\")\n",
    "        print(\"   Tip: Ensure the pipeline ran successfully and generated enhanced CSV reports.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute Smart Merge Enhanced Loading\n",
    "print(\" LOADING SMART MERGE ENHANCED DATA (LATEST FILE)...\")\n",
    "complete_df = load_smart_merge_csv_data()\n",
    "\n",
    "if complete_df:\n",
    "    record_count = complete_df.count()\n",
    "    print(f\"\\n Successfully loaded {record_count:,} Smart Merge enhanced records.\")\n",
    "    print(\"    Data includes 100% duration recovery and advanced correlation\")\n",
    "    \n",
    "    # Verify we have workspace_name column from CSV\n",
    "    if 'workspace_name' in complete_df.columns:\n",
    "        print(f\"    workspace_name column found in CSV data\")\n",
    "    else:\n",
    "        print(f\"    WARNING: workspace_name column NOT found!\")\n",
    "        print(f\"    You may need to re-run the pipeline with the latest version\")\n",
    "    \n",
    "    # Show status breakdown\n",
    "    status_breakdown = complete_df.groupBy(\"status\").count().collect()\n",
    "    print(\"\\n    Status Breakdown:\")\n",
    "    for row in status_breakdown:\n",
    "        print(f\"      {row['status']}: {row['count']:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\" No Smart Merge enhanced data found.\")\n",
    "    print(f\"    Checked path: {resolved_output_dir}\")\n",
    "    # Let's also check what files actually exist\n",
    "    try:\n",
    "        import glob\n",
    "        all_csv_files = glob.glob(os.path.join(resolved_output_dir, \"*.csv\"))\n",
    "        print(f\"    Available CSV files: {[os.path.basename(f) for f in all_csv_files]}\")\n",
    "    except Exception as list_error:\n",
    "        print(f\"    Could not list files: {list_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ae581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Validation & Column Check\n",
    "print(\"=\" * 80)\n",
    "print(\" DATA VALIDATION - VERIFYING CSV COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'complete_df' in dir() and complete_df is not None:\n",
    "    total_records = complete_df.count()\n",
    "    print(f\"\\n Total Records Loaded: {total_records:,}\")\n",
    "    \n",
    "    print(f\"\\n Available Columns ({len(complete_df.columns)}):\")\n",
    "    for idx, col_name in enumerate(complete_df.columns, 1):\n",
    "        print(f\"   {idx:2d}. {col_name}\")\n",
    "    \n",
    "    # Verify critical columns exist\n",
    "    critical_columns = ['workspace_id', 'workspace_name', 'item_name', 'item_type', \n",
    "                       'activity_type', 'status', 'start_time', 'end_time', \n",
    "                       'duration_seconds', 'failure_reason', 'error_message']\n",
    "    \n",
    "    print(f\"\\n Critical Column Check:\")\n",
    "    missing_columns = []\n",
    "    for col_name in critical_columns:\n",
    "        if col_name in complete_df.columns:\n",
    "            print(f\"    {col_name}\")\n",
    "        else:\n",
    "            print(f\"    {col_name} - MISSING!\")\n",
    "            missing_columns.append(col_name)\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"\\n  WARNING: {len(missing_columns)} critical columns are missing!\")\n",
    "        print(f\"   Missing: {', '.join(missing_columns)}\")\n",
    "    else:\n",
    "        print(f\"\\n All critical columns present!\")\n",
    "    \n",
    "    # Show status breakdown\n",
    "    print(f\"\\n Status Breakdown:\")\n",
    "    status_summary = complete_df.groupBy(\"status\").count().orderBy(col(\"count\").desc()).collect()\n",
    "    for row in status_summary:\n",
    "        status_name = row['status'] if row['status'] else 'Unknown'\n",
    "        count = row['count']\n",
    "        percentage = (count / total_records * 100) if total_records > 0 else 0\n",
    "        print(f\"   {status_name:15s}: {count:10,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Check workspace_name data quality\n",
    "    if 'workspace_name' in complete_df.columns:\n",
    "        null_workspace_names = complete_df.filter(col(\"workspace_name\").isNull() | (col(\"workspace_name\") == \"\")).count()\n",
    "        valid_workspace_names = total_records - null_workspace_names\n",
    "        print(f\"\\n Workspace Name Data Quality:\")\n",
    "        print(f\"   Valid workspace names: {valid_workspace_names:,} ({valid_workspace_names/total_records*100:.1f}%)\")\n",
    "        print(f\"   Null/Empty: {null_workspace_names:,} ({null_workspace_names/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample workspace names\n",
    "        print(f\"\\n Sample Workspace Names:\")\n",
    "        sample_workspaces = complete_df.select(\"workspace_name\", \"status\").limit(10).collect()\n",
    "        for row in sample_workspaces:\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"NULL\"\n",
    "            status = row['status'] if row['status'] else \"Unknown\"\n",
    "            print(f\"   {ws_name:50s} [{status}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\" DATA VALIDATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\" complete_df not loaded!\")\n",
    "    print(\"    Run Cell 11 first to load the CSV data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03157c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Overall Statistics Summary\n",
    "print(\"=\" * 80)\n",
    "print(\" OVERALL ACTIVITY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    # Import required functions explicitly\n",
    "    from pyspark.sql.functions import col, count, countDistinct, avg, sum as spark_sum, desc, when, max as spark_max, min as spark_min\n",
    "    \n",
    "    total_records = complete_df.count()\n",
    "    print(f\"\\n Dataset Overview:\")\n",
    "    print(f\"   Total Activities: {total_records:,}\")\n",
    "    \n",
    "    # Status breakdown with percentages\n",
    "    print(f\"\\n Status Distribution:\")\n",
    "    status_df = complete_df.groupBy(\"status\").agg(count(\"*\").alias(\"count\"))\n",
    "    status_results = status_df.collect()\n",
    "    \n",
    "    for row in status_results:\n",
    "        status_name = row['status'] if row['status'] else 'Unknown'\n",
    "        count_val = row['count']\n",
    "        pct = (count_val / total_records * 100) if total_records > 0 else 0\n",
    "        print(f\"   {status_name:15s}: {count_val:10,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Workspace statistics\n",
    "    print(f\"\\n Workspace Statistics:\")\n",
    "    unique_workspaces = complete_df.select(\"workspace_name\").filter(col(\"workspace_name\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Workspaces: {unique_workspaces:,}\")\n",
    "    \n",
    "    # Item statistics\n",
    "    print(f\"\\n Item Statistics:\")\n",
    "    unique_items = complete_df.select(\"item_name\").filter(col(\"item_name\").isNotNull()).distinct().count()\n",
    "    unique_item_types = complete_df.select(\"item_type\").filter(col(\"item_type\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Items: {unique_items:,}\")\n",
    "    print(f\"   Unique Item Types: {unique_item_types:,}\")\n",
    "    \n",
    "    # Activity type statistics\n",
    "    print(f\"\\n  Activity Type Statistics:\")\n",
    "    unique_activity_types = complete_df.select(\"activity_type\").filter(col(\"activity_type\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Activity Types: {unique_activity_types:,}\")\n",
    "    \n",
    "    # User statistics\n",
    "    print(f\"\\n User Statistics:\")\n",
    "    unique_users = complete_df.select(\"submitted_by\").filter(\n",
    "        (col(\"submitted_by\").isNotNull()) & \n",
    "        (col(\"submitted_by\") != \"System\") & \n",
    "        (col(\"submitted_by\") != \"\")\n",
    "    ).distinct().count()\n",
    "    print(f\"   Unique Active Users: {unique_users:,}\")\n",
    "    \n",
    "    # Duration statistics\n",
    "    print(f\"\\n⏱  Duration Statistics:\")\n",
    "    duration_df = complete_df.filter(col(\"duration_seconds\").isNotNull() & (col(\"duration_seconds\").cast(\"double\") > 0))\n",
    "    duration_count = duration_df.count()\n",
    "    \n",
    "    if duration_count > 0:\n",
    "        duration_stats = duration_df.agg(\n",
    "            avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\"),\n",
    "            spark_max(col(\"duration_seconds\").cast(\"double\")).alias(\"max_duration\"),\n",
    "            spark_min(col(\"duration_seconds\").cast(\"double\")).alias(\"min_duration\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Activities with Duration: {duration_count:,} ({duration_count/total_records*100:.1f}%)\")\n",
    "        print(f\"   Average Duration: {duration_stats['avg_duration']:.1f}s\")\n",
    "        print(f\"   Max Duration: {duration_stats['max_duration']:.1f}s\")\n",
    "        print(f\"   Min Duration: {duration_stats['min_duration']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   No duration data available\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ab253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Workspace Activity Analysis (Using workspace_name from CSV)\n",
    "print(\"=\" * 80)\n",
    "print(\" WORKSPACE ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc\n",
    "    \n",
    "    # Top workspaces by total activity\n",
    "    print(f\"\\n TOP 20 MOST ACTIVE WORKSPACES:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    workspace_activity = (complete_df\n",
    "                         .filter(col(\"workspace_name\").isNotNull())\n",
    "                         .groupBy(\"workspace_name\")\n",
    "                         .agg(\n",
    "                             count(\"*\").alias(\"total_activities\"),\n",
    "                             countDistinct(\"item_name\").alias(\"unique_items\"),\n",
    "                             countDistinct(\"activity_type\").alias(\"activity_types\"),\n",
    "                             countDistinct(\"submitted_by\").alias(\"unique_users\")\n",
    "                         )\n",
    "                         .orderBy(desc(\"total_activities\"))\n",
    "                         .limit(20))\n",
    "    \n",
    "    top_workspaces = workspace_activity.collect()\n",
    "    for idx, row in enumerate(top_workspaces, 1):\n",
    "        ws_name = row['workspace_name']\n",
    "        activities = row['total_activities']\n",
    "        items = row['unique_items']\n",
    "        types = row['activity_types']\n",
    "        users = row['unique_users']\n",
    "        print(f\"  {idx:2d}. {ws_name:45s}  {activities:8,} activities  {items:4,} items  {types:3,} types  {users:4,} users\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Failure Analysis by Workspace (FIXED - Using workspace_name from CSV)\n",
    "print(\"=\" * 80)\n",
    "print(\" WORKSPACE FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc\n",
    "    \n",
    "    # Filter for failures\n",
    "    failures_df = complete_df.filter(col(\"status\") == \"Failed\")\n",
    "    failure_count = failures_df.count()\n",
    "    \n",
    "    print(f\"\\n Total Failures: {failure_count:,}\")\n",
    "    \n",
    "    if failure_count > 0:\n",
    "        # Failures by workspace\n",
    "        print(f\"\\n TOP 20 WORKSPACES WITH FAILURES:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        workspace_failures = (failures_df\n",
    "                             .filter(col(\"workspace_name\").isNotNull())\n",
    "                             .groupBy(\"workspace_name\")\n",
    "                             .agg(\n",
    "                                 count(\"*\").alias(\"failure_count\"),\n",
    "                                 countDistinct(\"item_name\").alias(\"failed_items\"),\n",
    "                                 countDistinct(\"activity_type\").alias(\"failure_types\")\n",
    "                             )\n",
    "                             .orderBy(desc(\"failure_count\"))\n",
    "                             .limit(20))\n",
    "        \n",
    "        top_failure_workspaces = workspace_failures.collect()\n",
    "        \n",
    "        if len(top_failure_workspaces) > 0:\n",
    "            for idx, row in enumerate(top_failure_workspaces, 1):\n",
    "                ws_name = row['workspace_name']\n",
    "                failures = row['failure_count']\n",
    "                items = row['failed_items']\n",
    "                types = row['failure_types']\n",
    "                print(f\"  {idx:2d}. {ws_name:45s}  {failures:6,} failures  {items:4,} items  {types:3,} types\")\n",
    "        else:\n",
    "            print(\"   No failures with workspace names found\")\n",
    "        \n",
    "        # Check for failures without workspace names\n",
    "        failures_no_workspace = failures_df.filter(col(\"workspace_name\").isNull() | (col(\"workspace_name\") == \"\")).count()\n",
    "        \n",
    "        if failures_no_workspace > 0:\n",
    "            print(f\"\\n  Failures without workspace name: {failures_no_workspace:,} ({failures_no_workspace/failure_count*100:.1f}%)\")\n",
    "            print(f\"   These may be system-level or infrastructure failures\")\n",
    "        \n",
    "        # Failure types distribution\n",
    "        print(f\"\\n  FAILURE TYPES DISTRIBUTION:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        failure_types = (failures_df\n",
    "                        .groupBy(\"activity_type\")\n",
    "                        .agg(count(\"*\").alias(\"failure_count\"))\n",
    "                        .orderBy(desc(\"failure_count\"))\n",
    "                        .limit(10))\n",
    "        \n",
    "        failure_type_results = failure_types.collect()\n",
    "        for idx, row in enumerate(failure_type_results, 1):\n",
    "            activity_type = row['activity_type'] if row['activity_type'] else \"Unknown\"\n",
    "            failures = row['failure_count']\n",
    "            pct = (failures / failure_count * 100) if failure_count > 0 else 0\n",
    "            print(f\"  {idx:2d}. {activity_type:35s}  {failures:6,} failures ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Top failing items\n",
    "        print(f\"\\n TOP 15 FAILING ITEMS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        failing_items = (failures_df\n",
    "                        .filter(col(\"item_name\").isNotNull())\n",
    "                        .groupBy(\"workspace_name\", \"item_name\", \"item_type\")\n",
    "                        .agg(count(\"*\").alias(\"failure_count\"))\n",
    "                        .orderBy(desc(\"failure_count\"))\n",
    "                        .limit(15))\n",
    "        \n",
    "        failing_item_results = failing_items.collect()\n",
    "        for idx, row in enumerate(failing_item_results, 1):\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown Workspace\"\n",
    "            item_name = row['item_name']\n",
    "            item_type = row['item_type'] if row['item_type'] else \"Unknown\"\n",
    "            failures = row['failure_count']\n",
    "            print(f\"  {idx:2d}. {item_name:30s} ({item_type:15s})  {ws_name:25s}  {failures:5,} failures\")\n",
    "        \n",
    "    else:\n",
    "        print(\" No failures found in the dataset\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. User Activity & Failure Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\" USER ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc, when, sum as spark_sum\n",
    "    \n",
    "    # Based on diagnostic: submitted_by has 95.74% data, created_by is 100% NULL\n",
    "    user_column = 'submitted_by'\n",
    "    print(f\"\\n Using '{user_column}' column for user analysis\")\n",
    "    print(\"   (created_by and last_updated_by are 100% NULL in this dataset)\")\n",
    "    \n",
    "    # Filter out system users and nulls\n",
    "    user_activities = complete_df.filter(\n",
    "        (col(user_column).isNotNull()) & \n",
    "        (col(user_column) != \"System\") & \n",
    "        (col(user_column) != \"\")\n",
    "    )\n",
    "    \n",
    "    total_user_activities = user_activities.count()\n",
    "    unique_users = user_activities.select(user_column).distinct().count()\n",
    "    \n",
    "    print(f\"\\n User Activity Overview:\")\n",
    "    print(f\"   Total User Activities: {total_user_activities:,}\")\n",
    "    print(f\"   Unique Active Users: {unique_users:,}\")\n",
    "    \n",
    "    if total_user_activities > 0:\n",
    "        # Top active users - HANDLES DUPLICATES with groupBy aggregation\n",
    "        print(f\"\\n TOP 20 MOST ACTIVE USERS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # groupBy automatically handles duplicates by aggregating them\n",
    "        # countDistinct ensures we count unique workspaces/items per user\n",
    "        top_users = (user_activities\n",
    "                    .groupBy(user_column)\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"total_activities\"),  # Total activities (including duplicates if any)\n",
    "                        countDistinct(\"workspace_name\").alias(\"workspaces\"),  # Unique workspaces\n",
    "                        countDistinct(\"item_name\").alias(\"unique_items\"),  # Unique items\n",
    "                        spark_sum(when(col(\"status\") == \"Failed\", 1).otherwise(0)).alias(\"failures\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Succeeded\", 1).otherwise(0)).alias(\"successes\")\n",
    "                    )\n",
    "                    .orderBy(desc(\"total_activities\"))\n",
    "                    .limit(20))\n",
    "        \n",
    "        top_user_results = top_users.collect()\n",
    "        \n",
    "        if len(top_user_results) > 0:\n",
    "            for idx, row in enumerate(top_user_results, 1):\n",
    "                user = row[user_column]\n",
    "                activities = row['total_activities']\n",
    "                workspaces = row['workspaces']\n",
    "                items = row['unique_items']\n",
    "                failures = row['failures']\n",
    "                successes = row['successes']\n",
    "                success_rate = (successes / activities * 100) if activities > 0 else 0\n",
    "                print(f\"  {idx:2d}. {user:40s}  {activities:7,} activities  {workspaces:3,} WS  {items:4,} items  {failures:5,} fails  {success_rate:5.1f}% success\")\n",
    "        else:\n",
    "            print(\"   No user data available\")\n",
    "    else:\n",
    "        print(\"\\n    No user activities found (all records may be System or null)\")\n",
    "    \n",
    "    # Users with most failures\n",
    "    if total_user_activities > 0:\n",
    "        user_failures = user_activities.filter(col(\"status\") == \"Failed\")\n",
    "        user_failure_count = user_failures.count()\n",
    "        \n",
    "        if user_failure_count > 0:\n",
    "            print(f\"\\n TOP 10 USERS WITH MOST FAILURES:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # groupBy with countDistinct handles duplicates properly\n",
    "            users_with_failures = (user_failures\n",
    "                                  .groupBy(user_column)\n",
    "                                  .agg(\n",
    "                                      count(\"*\").alias(\"failure_count\"),  # Total failures per user\n",
    "                                      countDistinct(\"workspace_name\").alias(\"affected_workspaces\"),  # Unique workspaces\n",
    "                                      countDistinct(\"item_name\").alias(\"failed_items\")  # Unique items\n",
    "                                  )\n",
    "                                  .orderBy(desc(\"failure_count\"))\n",
    "                                  .limit(10))\n",
    "            \n",
    "            user_failure_results = users_with_failures.collect()\n",
    "            \n",
    "            if len(user_failure_results) > 0:\n",
    "                for idx, row in enumerate(user_failure_results, 1):\n",
    "                    user = row[user_column]\n",
    "                    failures = row['failure_count']\n",
    "                    workspaces = row['affected_workspaces']\n",
    "                    items = row['failed_items']\n",
    "                    print(f\"  {idx:2d}. {user:40s}  {failures:6,} failures  {workspaces:3,} WS  {items:4,} items\")\n",
    "            else:\n",
    "                print(\"   No failure data available\")\n",
    "        else:\n",
    "            print(f\"\\n No failures found for users\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fa6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Error & Failure Reason Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\" ERROR & FAILURE REASON ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, desc\n",
    "    \n",
    "    failures_df = complete_df.filter(col(\"status\") == \"Failed\")\n",
    "    failure_count = failures_df.count()\n",
    "    \n",
    "    if failure_count > 0:\n",
    "        # Check if error columns exist and have data\n",
    "        has_failure_reason = 'failure_reason' in complete_df.columns\n",
    "        has_error_message = 'error_message' in complete_df.columns\n",
    "        \n",
    "        if has_failure_reason:\n",
    "            # Failure reason distribution\n",
    "            print(f\"\\n FAILURE REASONS:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            failure_reasons = (failures_df\n",
    "                             .filter(col(\"failure_reason\").isNotNull() & (col(\"failure_reason\") != \"\"))\n",
    "                             .groupBy(\"failure_reason\")\n",
    "                             .agg(count(\"*\").alias(\"count\"))\n",
    "                             .orderBy(desc(\"count\"))\n",
    "                             .limit(15))\n",
    "            \n",
    "            reason_results = failure_reasons.collect()\n",
    "            \n",
    "            if len(reason_results) > 0:\n",
    "                for idx, row in enumerate(reason_results, 1):\n",
    "                    reason = row['failure_reason']\n",
    "                    count_val = row['count']\n",
    "                    pct = (count_val / failure_count * 100) if failure_count > 0 else 0\n",
    "                    # Truncate long reasons\n",
    "                    reason_display = reason[:70] + \"...\" if len(reason) > 70 else reason\n",
    "                    print(f\"  {idx:2d}. {reason_display:73s}  {count_val:5,} ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                print(\"   No failure reason data available\")\n",
    "        \n",
    "        if has_error_message:\n",
    "            # Sample error messages for top failures\n",
    "            print(f\"\\n SAMPLE ERROR MESSAGES (Top 10):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            error_samples = (failures_df\n",
    "                           .filter(col(\"error_message\").isNotNull() & (col(\"error_message\") != \"\"))\n",
    "                           .select(\"workspace_name\", \"item_name\", \"error_message\")\n",
    "                           .limit(10))\n",
    "            \n",
    "            error_results = error_samples.collect()\n",
    "            \n",
    "            if len(error_results) > 0:\n",
    "                for idx, row in enumerate(error_results, 1):\n",
    "                    ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown\"\n",
    "                    item = row['item_name'] if row['item_name'] else \"Unknown\"\n",
    "                    error = row['error_message']\n",
    "                    # Truncate long messages\n",
    "                    error_display = error[:100] + \"...\" if len(error) > 100 else error\n",
    "                    print(f\"\\n  {idx:2d}. Workspace: {ws_name}\")\n",
    "                    print(f\"      Item: {item}\")\n",
    "                    print(f\"      Error: {error_display}\")\n",
    "            else:\n",
    "                print(\"   No error message data available\")\n",
    "        \n",
    "        if not has_failure_reason and not has_error_message:\n",
    "            print(\"\\n  No failure_reason or error_message columns found in data\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n No failures to analyze\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Time-Based Analysis (Date & Duration)\n",
    "print(\"=\" * 80)\n",
    "print(\" TIME-BASED ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, desc, avg, sum as spark_sum, when, max as spark_max, min as spark_min\n",
    "    \n",
    "    # Activities by date\n",
    "    print(f\"\\n ACTIVITY DISTRIBUTION BY DATE:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    date_activity = (complete_df\n",
    "                    .filter(col(\"date\").isNotNull())\n",
    "                    .groupBy(\"date\")\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"total_activities\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Failed\", 1).otherwise(0)).alias(\"failures\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Succeeded\", 1).otherwise(0)).alias(\"successes\")\n",
    "                    )\n",
    "                    .orderBy(desc(\"date\"))\n",
    "                    .limit(15))\n",
    "    \n",
    "    date_results = date_activity.collect()\n",
    "    \n",
    "    if len(date_results) > 0:\n",
    "        for row in date_results:\n",
    "            date_val = row['date']\n",
    "            total = row['total_activities']\n",
    "            failures = row['failures']\n",
    "            successes = row['successes']\n",
    "            success_rate = (successes / total * 100) if total > 0 else 0\n",
    "            print(f\"  {date_val}  {total:8,} total  {successes:8,} success  {failures:6,} failed  {success_rate:5.1f}% success\")\n",
    "    else:\n",
    "        print(\"   No date information available\")\n",
    "    \n",
    "    # Duration analysis\n",
    "    print(f\"\\n⏱  DURATION ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    duration_df = complete_df.filter(col(\"duration_seconds\").isNotNull() & (col(\"duration_seconds\").cast(\"double\") > 0))\n",
    "    duration_count = duration_df.count()\n",
    "    total_records = complete_df.count()\n",
    "    \n",
    "    if duration_count > 0:\n",
    "        print(f\"  Activities with duration data: {duration_count:,} ({duration_count/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Overall duration statistics\n",
    "        duration_stats = duration_df.agg(\n",
    "            avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\"),\n",
    "            spark_max(col(\"duration_seconds\").cast(\"double\")).alias(\"max_duration\"),\n",
    "            spark_min(col(\"duration_seconds\").cast(\"double\")).alias(\"min_duration\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\n  Overall Duration Statistics:\")\n",
    "        print(f\"    Average: {duration_stats['avg_duration']:.2f}s ({duration_stats['avg_duration']/60:.2f} minutes)\")\n",
    "        print(f\"    Maximum: {duration_stats['max_duration']:.2f}s ({duration_stats['max_duration']/60:.2f} minutes)\")\n",
    "        print(f\"    Minimum: {duration_stats['min_duration']:.2f}s\")\n",
    "        \n",
    "        # Duration by status\n",
    "        print(f\"\\n  Duration by Status:\")\n",
    "        duration_by_status = (duration_df\n",
    "                             .groupBy(\"status\")\n",
    "                             .agg(\n",
    "                                 count(\"*\").alias(\"count\"),\n",
    "                                 avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\")\n",
    "                             )\n",
    "                             .orderBy(desc(\"count\")))\n",
    "        \n",
    "        status_duration_results = duration_by_status.collect()\n",
    "        for row in status_duration_results:\n",
    "            status = row['status'] if row['status'] else \"Unknown\"\n",
    "            count_val = row['count']\n",
    "            avg_dur = row['avg_duration']\n",
    "            print(f\"    {status:15s}: {count_val:8,} activities, avg {avg_dur:.2f}s\")\n",
    "        \n",
    "        # Longest running activities\n",
    "        print(f\"\\n   TOP 10 LONGEST RUNNING ACTIVITIES:\")\n",
    "        longest_activities = (duration_df\n",
    "                             .select(\"workspace_name\", \"item_name\", \"activity_type\", \"status\", \n",
    "                                   col(\"duration_seconds\").cast(\"double\").alias(\"duration\"))\n",
    "                             .orderBy(desc(\"duration\"))\n",
    "                             .limit(10))\n",
    "        \n",
    "        longest_results = longest_activities.collect()\n",
    "        for idx, row in enumerate(longest_results, 1):\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown\"\n",
    "            item = row['item_name'] if row['item_name'] else \"Unknown\"\n",
    "            activity = row['activity_type'] if row['activity_type'] else \"Unknown\"\n",
    "            status = row['status']\n",
    "            duration = row['duration']\n",
    "            duration_min = duration / 60\n",
    "            print(f\"    {idx:2d}. {ws_name:30s}  {item:25s}  {duration:.1f}s ({duration_min:.1f}m) [{status}]\")\n",
    "    else:\n",
    "        print(\"   No duration data available\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "e818f402-5fea-490c-9eeb-2e9258522102",
    "workspaceId": "944ae69e-1c99-4ad7-973f-c296c778a5c5"
   },
   "lakehouse": {
    "default_lakehouse": "5cb78b9b-b153-4771-b309-65ec4848433a",
    "default_lakehouse_name": "lh_01_bronze",
    "default_lakehouse_workspace_id": "944ae69e-1c99-4ad7-973f-c296c778a5c5",
    "known_lakehouses": [
     {
      "id": "5cb78b9b-b153-4771-b309-65ec4848433a"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
