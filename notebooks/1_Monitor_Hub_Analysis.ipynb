{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5790268",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#0078D4;\">Notebook Information</h2>\n",
    "\n",
    "| | |\n",
    "|:---|:---|\n",
    "| **Notebook 1** | Monitor Hub Analysis |\n",
    "| **Author** | Sanmi Ibitoye |\n",
    "| **Email** | Sanmi.Ibitoye@leit.ltd |\n",
    "| **Summary** | Runs the MonitorHubPipeline and analyzes exported CSV/Parquet outputs. Works in both Microsoft Fabric notebooks (paths auto-resolve under `/lakehouse/default/Files/`) and local development (writes under `exports/`). |\n",
    "| **Date** | Last Updated: 18 - 12 - 2025 |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Behaviors\n",
    "- **CSV-based reports**: The pipeline writes timestamped reports like `activities_master_YYYYMMDD_HHMMSS.csv`\n",
    "- **Strict auth when SP is provided**: If you set `AZURE_CLIENT_ID`/`AZURE_CLIENT_SECRET`/`AZURE_TENANT_ID` and they're wrong, auth fails rather than silently switching identities\n",
    "- **Tenant-wide + fallback**: When `TENANT_WIDE=True`, Power BI Admin APIs are attempted first and automatically fall back to member-only scope on 401/403\n",
    "- **Caching**: Daily extraction skips API calls if daily files already exist, and detailed job history uses an ~8 hour cache\n",
    "\n",
    "### How to Use\n",
    "1. Configure credentials (Service Principal or Fabric identity)\n",
    "2. Set `DAYS_TO_ANALYZE`, `TENANT_WIDE`, and `OUTPUT_DIR` in the config cell\n",
    "3. Run the pipeline cell; then run the analysis cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP LOCAL PATH (For Local Development)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to sys.path to allow importing the local package\n",
    "# This is necessary when running locally without installing the package\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# Check if we are in notebooks directory\n",
    "if current_dir.name == \"notebooks\":\n",
    "    src_path = current_dir.parent / \"src\"\n",
    "else:\n",
    "    # Assume we are in project root\n",
    "    src_path = current_dir / \"src\"\n",
    "\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bad86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Package / environment verification (safe: no Azure/API imports)\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "import importlib\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "try:\n",
    "    pkg_version = getattr(usf_fabric_monitoring, \"__version__\", None) or version(\"usf_fabric_monitoring\")\n",
    "except PackageNotFoundError:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "print(f\"usf_fabric_monitoring version: {pkg_version}\")\n",
    "print(f\"Resolved output dir example: {resolve_path('exports/monitor_hub_analysis')}\")\n",
    "\n",
    "# Optional dependency presence (does not import pipeline/auth)\n",
    "for mod in [\"azure.identity\", \"azure.core\", \"pyspark\", \"delta\", \"notebookutils\"]:\n",
    "    try:\n",
    "        importlib.import_module(mod)\n",
    "        print(f\"OK: import {mod}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SKIP: import {mod} ({type(e).__name__}: {e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f15df1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cce96",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Package / environment verification (safe: no Azure/API imports)\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "import importlib\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "try:\n",
    "    pkg_version = getattr(usf_fabric_monitoring, \"__version__\", None) or version(\"usf_fabric_monitoring\")\n",
    "except PackageNotFoundError:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "print(f\"usf_fabric_monitoring version: {pkg_version}\")\n",
    "print(f\"Resolved output dir example: {resolve_path('exports/monitor_hub_analysis')}\")\n",
    "\n",
    "# Optional dependency presence (does not import pipeline/auth)\n",
    "for mod in [\"azure.identity\", \"azure.core\", \"pyspark\", \"delta\", \"notebookutils\"]:\n",
    "    try:\n",
    "        importlib.import_module(mod)\n",
    "        print(f\"OK: import {mod}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SKIP: import {mod} ({type(e).__name__}: {e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349952b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CREDENTIAL MANAGEMENT ---\n",
    "\n",
    "# Option 1: Load from .env file (Lakehouse or Local)\n",
    "# We check the Lakehouse path first, then fallback to local .env\n",
    "LAKEHOUSE_ENV_PATH = \"/lakehouse/default/Files/dot_env_files/.env\"\n",
    "LOCAL_ENV_PATH = \".env\"\n",
    "\n",
    "# Force override=True to ensure we pick up changes to the file even if env vars are already set\n",
    "if os.path.exists(LAKEHOUSE_ENV_PATH):\n",
    "    print(f\"Loading configuration from Lakehouse: {LAKEHOUSE_ENV_PATH}\")\n",
    "    load_dotenv(LAKEHOUSE_ENV_PATH, override=True)\n",
    "elif os.path.exists(LOCAL_ENV_PATH):\n",
    "    print(f\"Loading configuration from Local: {os.path.abspath(LOCAL_ENV_PATH)}\")\n",
    "    load_dotenv(LOCAL_ENV_PATH, override=True)\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {LAKEHOUSE_ENV_PATH} or {LOCAL_ENV_PATH}\")\n",
    "\n",
    "# Verify credentials are present\n",
    "required_vars = [\"AZURE_CLIENT_ID\", \"AZURE_CLIENT_SECRET\", \"AZURE_TENANT_ID\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "\n",
    "print(\"\\n IDENTITY CHECK:\")\n",
    "if missing:\n",
    "    print(f\" Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"     System will fallback to DefaultAzureCredential (User Identity or Managed Identity)\")\n",
    "else:\n",
    "    client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    masked_id = f\"{client_id[:4]}...{client_id[-4:]}\" if client_id and len(client_id) > 8 else \"********\"\n",
    "    print(f\" Service Principal Configured in Environment\")\n",
    "    print(f\"   Client ID: {masked_id}\")\n",
    "    print(f\"   Tenant ID: {os.getenv('AZURE_TENANT_ID')}\")\n",
    "\n",
    "# --- TOKEN IDENTITY INSPECTION ---\n",
    "# This block decodes the actual token being used to prove identity\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.auth import create_authenticator_from_env\n",
    "    auth = create_authenticator_from_env()\n",
    "    token = auth.get_fabric_token()\n",
    "    \n",
    "    # Decode JWT (no signature verification needed for inspection)\n",
    "    parts = token.split('.')\n",
    "    if len(parts) > 1:\n",
    "        # Add padding if needed\n",
    "        payload_part = parts[1]\n",
    "        padded = payload_part + '=' * (4 - len(payload_part) % 4)\n",
    "        decoded = base64.urlsafe_b64decode(padded)\n",
    "        claims = json.loads(decoded)\n",
    "        \n",
    "        print(\"\\n  ACTIVE TOKEN IDENTITY:\")\n",
    "        if 'upn' in claims:\n",
    "            print(f\"   User Principal Name: {claims['upn']}\")\n",
    "            print(\"    You are logged in as a USER.\")\n",
    "        elif 'appid' in claims:\n",
    "            print(f\"   Application ID: {claims['appid']}\")\n",
    "            if client_id and claims['appid'] == client_id:\n",
    "                print(\"    You are logged in as the CONFIGURED SERVICE PRINCIPAL.\")\n",
    "            else:\n",
    "                print(\"    You are logged in as a DIFFERENT Service Principal/Managed Identity.\")\n",
    "        else:\n",
    "            print(f\"   Subject: {claims.get('sub', 'Unknown')}\")\n",
    "            \n",
    "        print(f\"   Audience: {claims.get('aud', 'Unknown')}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n  Could not inspect token identity: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37982a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Run configuration\n",
    "import os\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# Days of history to analyze (API max defaults to 28; see MAX_HISTORICAL_DAYS)\n",
    "DAYS_TO_ANALYZE = int(os.getenv(\"DEFAULT_ANALYSIS_DAYS\", \"7\"))\n",
    "\n",
    "# Scope: tenant-wide uses Power BI Admin APIs and auto-falls back to member-only on 401/403\n",
    "TENANT_WIDE = os.getenv(\"TENANT_WIDE\", \"1\") == \"1\"\n",
    "\n",
    "# Output directory for reports and parquet (relative paths auto-resolve in Fabric)\n",
    "OUTPUT_DIR = os.getenv(\"EXPORT_DIRECTORY\", \"exports/monitor_hub_analysis\")\n",
    "\n",
    "# Optional network knobs (helps avoid long retry backoffs in interactive runs)\n",
    "os.environ.setdefault(\"API_REQUEST_TIMEOUT\", \"30\")\n",
    "os.environ.setdefault(\"MAX_RETRIES\", \"2\")\n",
    "os.environ.setdefault(\"RETRY_BACKOFF_FACTOR\", \"1\")\n",
    "\n",
    "# Notebook-level cache control\n",
    "FORCE_REFRESH = os.getenv(\"FORCE_REFRESH\", \"0\") == \"1\"\n",
    "\n",
    "resolved_output_dir = resolve_path(OUTPUT_DIR)\n",
    "\n",
    "print(f\"DAYS_TO_ANALYZE: {DAYS_TO_ANALYZE} (max {os.getenv('MAX_HISTORICAL_DAYS', '28')})\")\n",
    "print(f\"TENANT_WIDE: {TENANT_WIDE}\")\n",
    "print(f\"FORCE_REFRESH: {FORCE_REFRESH}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n",
    "print(f\"Resolved output dir: {resolved_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b70a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Smart Data Extraction with 8-Hour Cache Logic\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "def check_recent_extraction(output_dir: str, hours_threshold: int = 8):\n",
    "    \"\"\"Check if reports were generated within the threshold hours.\"\"\"\n",
    "    try:\n",
    "        resolved_dir = resolve_path(output_dir)\n",
    "\n",
    "        csv_pattern = os.path.join(str(resolved_dir), \"activities_master_*.csv\")\n",
    "        csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "        # Backward-compatible: if the default output dir changed but legacy cache exists, use it.\n",
    "        if (\n",
    "            not csv_files\n",
    "            and output_dir == \"exports/monitor_hub_analysis\"\n",
    "            and glob.glob(str(resolve_path(\"monitor_hub_analysis\") / \"activities_master_*.csv\"))\n",
    "        ):\n",
    "            print(\"Found recent extraction under legacy OUTPUT_DIR; reusing it to avoid re-running APIs.\")\n",
    "            output_dir = \"monitor_hub_analysis\"\n",
    "            resolved_dir = resolve_path(output_dir)\n",
    "            csv_pattern = os.path.join(str(resolved_dir), \"activities_master_*.csv\")\n",
    "            csv_files = glob.glob(csv_pattern)\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No previous extraction found\")\n",
    "            return False, None, output_dir\n",
    "\n",
    "        latest_file = None\n",
    "        latest_time = None\n",
    "        for csv_file in csv_files:\n",
    "            file_time = os.path.getctime(csv_file)\n",
    "            if latest_time is None or file_time > latest_time:\n",
    "                latest_time = file_time\n",
    "                latest_file = csv_file\n",
    "\n",
    "        file_time = datetime.fromtimestamp(latest_time)\n",
    "        time_diff = datetime.now() - file_time\n",
    "\n",
    "        print(f\"Latest extraction: {os.path.basename(latest_file)}\")\n",
    "        print(f\"Extraction time: {file_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Time since extraction: {time_diff}\")\n",
    "\n",
    "        if time_diff < timedelta(hours=hours_threshold):\n",
    "            print(f\"Using cached data (within {hours_threshold} hours)\")\n",
    "            return True, latest_file, output_dir\n",
    "        else:\n",
    "            print(f\"Cache expired (older than {hours_threshold} hours)\")\n",
    "            return False, latest_file, output_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking cache: {e}\")\n",
    "        return False, None, output_dir\n",
    "\n",
    "print(\"CHECKING FOR RECENT DATA EXTRACTION...\")\n",
    "use_cache, cache_file, effective_output_dir = check_recent_extraction(OUTPUT_DIR, hours_threshold=8)\n",
    "if effective_output_dir != OUTPUT_DIR:\n",
    "    OUTPUT_DIR = effective_output_dir\n",
    "    resolved_output_dir = resolve_path(OUTPUT_DIR)\n",
    "    print(f\"Using OUTPUT_DIR: {OUTPUT_DIR} -> {resolved_output_dir}\")\n",
    "\n",
    "if use_cache and not FORCE_REFRESH:\n",
    "    print(\"USING CACHED DATA - SKIPPING EXTRACTION\")\n",
    "\n",
    "    # Load cached pipeline summary for results display\n",
    "    try:\n",
    "        resolved_dir = resolve_path(OUTPUT_DIR)\n",
    "        summary_pattern = os.path.join(str(resolved_dir), \"pipeline_summary_*.json\")\n",
    "        summary_files = glob.glob(summary_pattern)\n",
    "\n",
    "        if summary_files:\n",
    "            import json\n",
    "            latest_summary = None\n",
    "            latest_time = None\n",
    "            for summary_file in summary_files:\n",
    "                file_time = os.path.getctime(summary_file)\n",
    "                if latest_time is None or file_time > latest_time:\n",
    "                    latest_time = file_time\n",
    "                    latest_summary = summary_file\n",
    "\n",
    "            with open(latest_summary, \"r\", encoding=\"utf-8\") as f:\n",
    "                cached_results = json.load(f)\n",
    "\n",
    "            results = {\n",
    "                \"status\": \"success\",\n",
    "                \"summary\": cached_results,\n",
    "                \"report_files\": {},\n",
    "                \"cached\": True,\n",
    "            }\n",
    "\n",
    "            print(\"Cached Analysis Summary:\")\n",
    "\n",
    "            def safe_format(key, value):\n",
    "                try:\n",
    "                    if key == \"success_rate\" and isinstance(value, (int, float)):\n",
    "                        return f\"  {key.replace('_', ' ').title()}: {value:.1f}%\"\n",
    "                    elif key in [\"total_activities\", \"analysis_period_days\"] and value is not None:\n",
    "                        return f\"  {key.replace('_', ' ').title()}: {value:,}\"\n",
    "                    elif value is not None:\n",
    "                        return f\"  {key.replace('_', ' ').title()}: {value}\"\n",
    "                    else:\n",
    "                        return f\"  {key.replace('_', ' ').title()}: N/A\"\n",
    "                except (ValueError, TypeError):\n",
    "                    return f\"  {key.replace('_', ' ').title()}: {value}\"\n",
    "\n",
    "            for k in [\"total_activities\", \"analysis_period_days\", \"success_rate\", \"total_workspaces\", \"total_items\"]:\n",
    "                if k in cached_results:\n",
    "                    print(safe_format(k, cached_results.get(k)))\n",
    "        else:\n",
    "            results = {\"status\": \"success\", \"cached\": True, \"summary\": {\"note\": \"Using cached data\"}}\n",
    "            print(\"Cached Analysis Summary: Using recent extraction (summary file not found)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load cached summary: {e}\")\n",
    "        results = {\"status\": \"success\", \"cached\": True}\n",
    "else:\n",
    "    if FORCE_REFRESH:\n",
    "        print(\"FORCE_REFRESH is enabled: running fresh extraction\")\n",
    "    else:\n",
    "        print(\"No fresh cache found: running extraction\")\n",
    "\n",
    "    from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "    pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "    results = pipeline.run_complete_analysis(days=DAYS_TO_ANALYZE, tenant_wide=TENANT_WIDE)\n",
    "\n",
    "print(\"\\nPIPELINE COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b66b2",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis & Visualization (Spark)\n",
    "\n",
    "The following cells use PySpark to load the exported/enriched monitoring data and provide visualizations of failures, error codes, and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Spark & Paths for Smart Merge Enhanced Data\n",
    "import os\n",
    "import glob\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "print(\" INITIALIZING SPARK FOR SMART MERGE ENHANCED DATA ANALYSIS\")\n",
    "\n",
    "# Initialize Spark Session (if not already active)\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import (\n",
    "        col, to_timestamp, when, count, desc, lit, unix_timestamp, coalesce, \n",
    "        abs as abs_val, split, initcap, regexp_replace, element_at, substring, \n",
    "        avg, max, min, to_date, countDistinct, collect_list\n",
    "    )\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "    if 'spark' not in locals() or spark is None:\n",
    "        print(\" Initializing Spark Session for Smart Merge data...\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"FabricSmartMergeAnalysis\") \\\n",
    "            .getOrCreate()\n",
    "        print(f\" Spark Session Created: {spark.version}\")\n",
    "        print(\"    Ready for Smart Merge enhanced data analysis\")\n",
    "except ImportError:\n",
    "    print(\" PySpark not installed or configured. Skipping Spark-based analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\" Failed to initialize Spark: {e}. Skipping Spark-based analysis.\")\n",
    "\n",
    "# Resolve the output directory to an absolute path\n",
    "# This ensures that if you used a relative path like \"monitor_hub_analysis\",\n",
    "# it is correctly resolved to \"/lakehouse/default/Files/monitor_hub_analysis\" for Spark.\n",
    "resolved_output_dir = str(resolve_path(OUTPUT_DIR))\n",
    "\n",
    "BASE_PATH = os.path.join(resolved_output_dir, \"fabric_item_details\")\n",
    "AUDIT_LOG_PATH = os.path.join(resolved_output_dir, \"raw_data/daily\")\n",
    "\n",
    "print(f\"\\n Smart Merge Enhanced Data Paths:\")\n",
    "print(f\"  - Item Details: {BASE_PATH}\")\n",
    "print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")\n",
    "print(\"    All paths contain Smart Merge enhanced data with 100% duration recovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec636d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Smart Merge Enhanced Data from CSV (Aggregated Reports) - 22-COLUMN SCHEMA VALIDATION\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, coalesce, initcap, regexp_replace, element_at, split, when, lit, to_date\n",
    "from pyspark.sql.types import StringType\n",
    "from usf_fabric_monitoring.core.utils import is_fabric_environment\n",
    "\n",
    "def convert_to_spark_path(local_path: str) -> str:\n",
    "    \"\"\"Convert local filesystem path to Spark-compatible path.\n",
    "    \n",
    "    In Microsoft Fabric, local paths like /lakehouse/default/Files/...\n",
    "    need to be converted to abfss:// URIs for Spark to read them.\n",
    "    \n",
    "    Args:\n",
    "        local_path: Local filesystem path (e.g., /lakehouse/default/Files/exports/...)\n",
    "        \n",
    "    Returns:\n",
    "        Spark-compatible path (abfss:// in Fabric, unchanged locally)\n",
    "    \"\"\"\n",
    "    if not is_fabric_environment():\n",
    "        return local_path  # Local development - use path as-is\n",
    "    \n",
    "    # In Fabric, convert /lakehouse/default/Files/... to Files/...\n",
    "    # Spark automatically resolves relative paths from the Lakehouse root\n",
    "    lakehouse_prefix = \"/lakehouse/default/Files/\"\n",
    "    if local_path.startswith(lakehouse_prefix):\n",
    "        # Return the relative path from Files/ - Spark handles this correctly\n",
    "        relative_path = local_path[len(\"/lakehouse/default/\"):]\n",
    "        return relative_path\n",
    "    \n",
    "    # If path doesn't match expected pattern, return as-is\n",
    "    return local_path\n",
    "\n",
    "def load_smart_merge_csv_data():\n",
    "    \"\"\"Loads the Smart Merge enhanced activity data from CSV reports with schema validation.\n",
    "    \n",
    "    Only loads CSV files that match the expected 22-column schema:\n",
    "    activity_id, workspace_id, workspace_name, item_id, item_name, item_type, activity_type,\n",
    "    status, start_time, end_time, date, hour, duration_seconds, duration_minutes, submitted_by,\n",
    "    created_by, last_updated_by, domain, location, object_url, failure_reason, error_message\n",
    "    \n",
    "    Enrichment provides:\n",
    "    - 100% duration data recovery through advanced correlation\n",
    "    - Enhanced accuracy in performance metrics\n",
    "    - Intelligent gap filling for missing information\n",
    "    - Comprehensive activity lifecycle tracking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Expected 22-column schema\n",
    "        expected_columns = [\n",
    "            'activity_id', 'workspace_id', 'workspace_name', 'item_id', 'item_name', 'item_type',\n",
    "            'activity_type', 'status', 'start_time', 'end_time', 'date', 'hour',\n",
    "            'duration_seconds', 'duration_minutes', 'submitted_by', 'created_by',\n",
    "            'last_updated_by', 'domain', 'location', 'object_url', 'failure_reason', 'error_message'\n",
    "        ]\n",
    "        \n",
    "        # Find all CSV files matching the pattern using local filesystem\n",
    "        csv_pattern = os.path.join(resolved_output_dir, \"activities_master_*.csv\")\n",
    "        all_csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not all_csv_files:\n",
    "            print(f\" No CSV files found matching pattern: {csv_pattern}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\" Found {len(all_csv_files)} CSV file(s) - validating schemas...\")\n",
    "        print(f\"   Expected schema: 22 columns\")\n",
    "        \n",
    "        # Validate each file's schema using local filesystem read\n",
    "        valid_files = []\n",
    "        invalid_files = []\n",
    "        \n",
    "        for csv_file in all_csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            \n",
    "            # Read just the header to check schema (uses local filesystem)\n",
    "            with open(csv_file, 'r') as f:\n",
    "                header = f.readline().strip()\n",
    "                columns = [c.strip() for c in header.split(',')]\n",
    "            \n",
    "            if len(columns) == 22 and set(columns) == set(expected_columns):\n",
    "                valid_files.append(csv_file)\n",
    "                print(f\"    {file_name}: Valid (22 columns)\")\n",
    "            else:\n",
    "                invalid_files.append((csv_file, len(columns)))\n",
    "                print(f\"     {file_name}: INVALID ({len(columns)} columns - SKIPPING)\")\n",
    "        \n",
    "        if invalid_files:\n",
    "            print(f\"\\n  WARNING: {len(invalid_files)} file(s) do not match the expected 22-column schema:\")\n",
    "            for invalid_file, col_count in invalid_files:\n",
    "                file_name = os.path.basename(invalid_file)\n",
    "                print(f\"    {file_name} has {col_count} columns (expected 22)\")\n",
    "            print(f\"    These files will be EXCLUDED from the analysis.\")\n",
    "            print(f\"    Consider deleting old files or re-running the pipeline to regenerate them.\")\n",
    "        \n",
    "        if not valid_files:\n",
    "            print(f\"\\n ERROR: No valid CSV files found with the expected 22-column schema!\")\n",
    "            print(f\"   Please re-run the pipeline to generate updated CSV files.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n Loading {len(valid_files)} valid CSV file(s) with 22-column schema...\")\n",
    "        \n",
    "        # Convert local paths to Spark-compatible paths\n",
    "        # In Fabric, Spark needs relative paths from /lakehouse/default/ not absolute paths\n",
    "        spark_files = [convert_to_spark_path(f) for f in valid_files]\n",
    "        \n",
    "        if is_fabric_environment():\n",
    "            print(f\"   Fabric environment detected - using relative paths for Spark\")\n",
    "            for local_path, spark_path in zip(valid_files, spark_files):\n",
    "                print(f\"     {os.path.basename(local_path)} -> {spark_path}\")\n",
    "        \n",
    "        # Load all valid CSV files using Spark-compatible paths\n",
    "        df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"false\").csv(spark_files)\n",
    "        \n",
    "        # Enhanced data validation for Smart Merge features\n",
    "        total_records = df.count()\n",
    "        print(f\"    Total records loaded: {total_records:,}\")\n",
    "        \n",
    "        if total_records == 0:\n",
    "            print(\" No data found in valid CSV files\")\n",
    "            return None\n",
    "        \n",
    "        # Verify the loaded DataFrame has correct schema\n",
    "        actual_columns = df.columns\n",
    "        print(f\"    DataFrame columns ({len(actual_columns)}): {', '.join(actual_columns)}\")\n",
    "        \n",
    "        if len(actual_columns) != 22:\n",
    "            print(f\"    WARNING: DataFrame has {len(actual_columns)} columns, expected 22\")\n",
    "        \n",
    "        # Check for enhanced Smart Merge columns\n",
    "        smart_merge_cols = ['workspace_name', 'failure_reason', 'error_message']\n",
    "        missing_cols = [c for c in smart_merge_cols if c not in actual_columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"    WARNING: Missing Smart Merge columns: {', '.join(missing_cols)}\")\n",
    "        else:\n",
    "            print(f\"    All Smart Merge enhanced columns present\")\n",
    "        \n",
    "        # Check for enhanced duration data\n",
    "        duration_cols = [c for c in actual_columns if 'duration' in c.lower()]\n",
    "        if duration_cols:\n",
    "            print(f\"    Duration columns detected: {', '.join(duration_cols)}\")\n",
    "            print(\"    Smart Merge duration enhancement active\")\n",
    "        \n",
    "        print(f\"    Successfully loaded aggregated data from {len(valid_files)} file(s)\")\n",
    "        return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Could not load Smart Merge enhanced CSV data: {str(e)}\")\n",
    "        print(\"   Tip: Ensure the pipeline ran successfully and generated enhanced CSV reports.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute Smart Merge Enhanced Loading\n",
    "print(\" LOADING SMART MERGE ENHANCED DATA (LATEST FILE)...\")\n",
    "complete_df = load_smart_merge_csv_data()\n",
    "\n",
    "if complete_df:\n",
    "    record_count = complete_df.count()\n",
    "    print(f\"\\n Successfully loaded {record_count:,} Smart Merge enhanced records.\")\n",
    "    print(\"    Data includes 100% duration recovery and advanced correlation\")\n",
    "    \n",
    "    # Verify we have workspace_name column from CSV\n",
    "    if 'workspace_name' in complete_df.columns:\n",
    "        print(f\"    workspace_name column found in CSV data\")\n",
    "    else:\n",
    "        print(f\"    WARNING: workspace_name column NOT found!\")\n",
    "        print(f\"    You may need to re-run the pipeline with the latest version\")\n",
    "    \n",
    "    # Show status breakdown\n",
    "    status_breakdown = complete_df.groupBy(\"status\").count().collect()\n",
    "    print(\"\\n    Status Breakdown:\")\n",
    "    for row in status_breakdown:\n",
    "        print(f\"      {row['status']}: {row['count']:,}\")\n",
    "    \n",
    "else:\n",
    "    print(\" No Smart Merge enhanced data found.\")\n",
    "    print(f\"    Checked path: {resolved_output_dir}\")\n",
    "    # Let's also check what files actually exist\n",
    "    try:\n",
    "        import glob\n",
    "        all_csv_files = glob.glob(os.path.join(resolved_output_dir, \"*.csv\"))\n",
    "        print(f\"    Available CSV files: {[os.path.basename(f) for f in all_csv_files]}\")\n",
    "    except Exception as list_error:\n",
    "        print(f\"    Could not list files: {list_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ae581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Validation & Column Check\n",
    "print(\"=\" * 80)\n",
    "print(\" DATA VALIDATION - VERIFYING CSV COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'complete_df' in dir() and complete_df is not None:\n",
    "    total_records = complete_df.count()\n",
    "    print(f\"\\n Total Records Loaded: {total_records:,}\")\n",
    "    \n",
    "    print(f\"\\n Available Columns ({len(complete_df.columns)}):\")\n",
    "    for idx, col_name in enumerate(complete_df.columns, 1):\n",
    "        print(f\"   {idx:2d}. {col_name}\")\n",
    "    \n",
    "    # Verify critical columns exist\n",
    "    critical_columns = ['workspace_id', 'workspace_name', 'item_name', 'item_type', \n",
    "                       'activity_type', 'status', 'start_time', 'end_time', \n",
    "                       'duration_seconds', 'failure_reason', 'error_message']\n",
    "    \n",
    "    print(f\"\\n Critical Column Check:\")\n",
    "    missing_columns = []\n",
    "    for col_name in critical_columns:\n",
    "        if col_name in complete_df.columns:\n",
    "            print(f\"    {col_name}\")\n",
    "        else:\n",
    "            print(f\"    {col_name} - MISSING!\")\n",
    "            missing_columns.append(col_name)\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"\\n  WARNING: {len(missing_columns)} critical columns are missing!\")\n",
    "        print(f\"   Missing: {', '.join(missing_columns)}\")\n",
    "    else:\n",
    "        print(f\"\\n All critical columns present!\")\n",
    "    \n",
    "    # Show status breakdown\n",
    "    print(f\"\\n Status Breakdown:\")\n",
    "    status_summary = complete_df.groupBy(\"status\").count().orderBy(col(\"count\").desc()).collect()\n",
    "    for row in status_summary:\n",
    "        status_name = row['status'] if row['status'] else 'Unknown'\n",
    "        count = row['count']\n",
    "        percentage = (count / total_records * 100) if total_records > 0 else 0\n",
    "        print(f\"   {status_name:15s}: {count:10,} ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Check workspace_name data quality\n",
    "    if 'workspace_name' in complete_df.columns:\n",
    "        null_workspace_names = complete_df.filter(col(\"workspace_name\").isNull() | (col(\"workspace_name\") == \"\")).count()\n",
    "        valid_workspace_names = total_records - null_workspace_names\n",
    "        print(f\"\\n Workspace Name Data Quality:\")\n",
    "        print(f\"   Valid workspace names: {valid_workspace_names:,} ({valid_workspace_names/total_records*100:.1f}%)\")\n",
    "        print(f\"   Null/Empty: {null_workspace_names:,} ({null_workspace_names/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Show sample workspace names\n",
    "        print(f\"\\n Sample Workspace Names:\")\n",
    "        sample_workspaces = complete_df.select(\"workspace_name\", \"status\").limit(10).collect()\n",
    "        for row in sample_workspaces:\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"NULL\"\n",
    "            status = row['status'] if row['status'] else \"Unknown\"\n",
    "            print(f\"   {ws_name:50s} [{status}]\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\" DATA VALIDATION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\" complete_df not loaded!\")\n",
    "    print(\"    Run Cell 11 first to load the CSV data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03157c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Overall Statistics Summary\n",
    "print(\"=\" * 80)\n",
    "print(\" OVERALL ACTIVITY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    # Import required functions explicitly\n",
    "    from pyspark.sql.functions import col, count, countDistinct, avg, sum as spark_sum, desc, when, max as spark_max, min as spark_min\n",
    "    \n",
    "    total_records = complete_df.count()\n",
    "    print(f\"\\n Dataset Overview:\")\n",
    "    print(f\"   Total Activities: {total_records:,}\")\n",
    "    \n",
    "    # Status breakdown with percentages\n",
    "    print(f\"\\n Status Distribution:\")\n",
    "    status_df = complete_df.groupBy(\"status\").agg(count(\"*\").alias(\"count\"))\n",
    "    status_results = status_df.collect()\n",
    "    \n",
    "    for row in status_results:\n",
    "        status_name = row['status'] if row['status'] else 'Unknown'\n",
    "        count_val = row['count']\n",
    "        pct = (count_val / total_records * 100) if total_records > 0 else 0\n",
    "        print(f\"   {status_name:15s}: {count_val:10,} ({pct:5.2f}%)\")\n",
    "    \n",
    "    # Workspace statistics\n",
    "    print(f\"\\n Workspace Statistics:\")\n",
    "    unique_workspaces = complete_df.select(\"workspace_name\").filter(col(\"workspace_name\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Workspaces: {unique_workspaces:,}\")\n",
    "    \n",
    "    # Item statistics\n",
    "    print(f\"\\n Item Statistics:\")\n",
    "    unique_items = complete_df.select(\"item_name\").filter(col(\"item_name\").isNotNull()).distinct().count()\n",
    "    unique_item_types = complete_df.select(\"item_type\").filter(col(\"item_type\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Items: {unique_items:,}\")\n",
    "    print(f\"   Unique Item Types: {unique_item_types:,}\")\n",
    "    \n",
    "    # Activity type statistics\n",
    "    print(f\"\\n  Activity Type Statistics:\")\n",
    "    unique_activity_types = complete_df.select(\"activity_type\").filter(col(\"activity_type\").isNotNull()).distinct().count()\n",
    "    print(f\"   Unique Activity Types: {unique_activity_types:,}\")\n",
    "    \n",
    "    # User statistics\n",
    "    print(f\"\\n User Statistics:\")\n",
    "    unique_users = complete_df.select(\"submitted_by\").filter(\n",
    "        (col(\"submitted_by\").isNotNull()) & \n",
    "        (col(\"submitted_by\") != \"System\") & \n",
    "        (col(\"submitted_by\") != \"\")\n",
    "    ).distinct().count()\n",
    "    print(f\"   Unique Active Users: {unique_users:,}\")\n",
    "    \n",
    "    # Duration statistics\n",
    "    print(f\"\\nâ±  Duration Statistics:\")\n",
    "    duration_df = complete_df.filter(col(\"duration_seconds\").isNotNull() & (col(\"duration_seconds\").cast(\"double\") > 0))\n",
    "    duration_count = duration_df.count()\n",
    "    \n",
    "    if duration_count > 0:\n",
    "        duration_stats = duration_df.agg(\n",
    "            avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\"),\n",
    "            spark_max(col(\"duration_seconds\").cast(\"double\")).alias(\"max_duration\"),\n",
    "            spark_min(col(\"duration_seconds\").cast(\"double\")).alias(\"min_duration\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"   Activities with Duration: {duration_count:,} ({duration_count/total_records*100:.1f}%)\")\n",
    "        print(f\"   Average Duration: {duration_stats['avg_duration']:.1f}s\")\n",
    "        print(f\"   Max Duration: {duration_stats['max_duration']:.1f}s\")\n",
    "        print(f\"   Min Duration: {duration_stats['min_duration']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   No duration data available\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ab253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Workspace Activity Analysis (Using workspace_name from CSV)\n",
    "print(\"=\" * 80)\n",
    "print(\" WORKSPACE ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc\n",
    "    \n",
    "    # Top workspaces by total activity\n",
    "    print(f\"\\n TOP 20 MOST ACTIVE WORKSPACES:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    workspace_activity = (complete_df\n",
    "                         .filter(col(\"workspace_name\").isNotNull())\n",
    "                         .groupBy(\"workspace_name\")\n",
    "                         .agg(\n",
    "                             count(\"*\").alias(\"total_activities\"),\n",
    "                             countDistinct(\"item_name\").alias(\"unique_items\"),\n",
    "                             countDistinct(\"activity_type\").alias(\"activity_types\"),\n",
    "                             countDistinct(\"submitted_by\").alias(\"unique_users\")\n",
    "                         )\n",
    "                         .orderBy(desc(\"total_activities\"))\n",
    "                         .limit(20))\n",
    "    \n",
    "    top_workspaces = workspace_activity.collect()\n",
    "    for idx, row in enumerate(top_workspaces, 1):\n",
    "        ws_name = row['workspace_name']\n",
    "        activities = row['total_activities']\n",
    "        items = row['unique_items']\n",
    "        types = row['activity_types']\n",
    "        users = row['unique_users']\n",
    "        print(f\"  {idx:2d}. {ws_name:45s}  {activities:8,} activities  {items:4,} items  {types:3,} types  {users:4,} users\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Failure Analysis by Workspace (FIXED - Using workspace_name from CSV)\n",
    "print(\"=\" * 80)\n",
    "print(\" WORKSPACE FAILURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc\n",
    "    \n",
    "    # Filter for failures\n",
    "    failures_df = complete_df.filter(col(\"status\") == \"Failed\")\n",
    "    failure_count = failures_df.count()\n",
    "    \n",
    "    print(f\"\\n Total Failures: {failure_count:,}\")\n",
    "    \n",
    "    if failure_count > 0:\n",
    "        # Failures by workspace\n",
    "        print(f\"\\n TOP 20 WORKSPACES WITH FAILURES:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        workspace_failures = (failures_df\n",
    "                             .filter(col(\"workspace_name\").isNotNull())\n",
    "                             .groupBy(\"workspace_name\")\n",
    "                             .agg(\n",
    "                                 count(\"*\").alias(\"failure_count\"),\n",
    "                                 countDistinct(\"item_name\").alias(\"failed_items\"),\n",
    "                                 countDistinct(\"activity_type\").alias(\"failure_types\")\n",
    "                             )\n",
    "                             .orderBy(desc(\"failure_count\"))\n",
    "                             .limit(20))\n",
    "        \n",
    "        top_failure_workspaces = workspace_failures.collect()\n",
    "        \n",
    "        if len(top_failure_workspaces) > 0:\n",
    "            for idx, row in enumerate(top_failure_workspaces, 1):\n",
    "                ws_name = row['workspace_name']\n",
    "                failures = row['failure_count']\n",
    "                items = row['failed_items']\n",
    "                types = row['failure_types']\n",
    "                print(f\"  {idx:2d}. {ws_name:45s}  {failures:6,} failures  {items:4,} items  {types:3,} types\")\n",
    "        else:\n",
    "            print(\"   No failures with workspace names found\")\n",
    "        \n",
    "        # Check for failures without workspace names\n",
    "        failures_no_workspace = failures_df.filter(col(\"workspace_name\").isNull() | (col(\"workspace_name\") == \"\")).count()\n",
    "        \n",
    "        if failures_no_workspace > 0:\n",
    "            print(f\"\\n  Failures without workspace name: {failures_no_workspace:,} ({failures_no_workspace/failure_count*100:.1f}%)\")\n",
    "            print(f\"   These may be system-level or infrastructure failures\")\n",
    "        \n",
    "        # Failure types distribution\n",
    "        print(f\"\\n  FAILURE TYPES DISTRIBUTION:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        failure_types = (failures_df\n",
    "                        .groupBy(\"activity_type\")\n",
    "                        .agg(count(\"*\").alias(\"failure_count\"))\n",
    "                        .orderBy(desc(\"failure_count\"))\n",
    "                        .limit(10))\n",
    "        \n",
    "        failure_type_results = failure_types.collect()\n",
    "        for idx, row in enumerate(failure_type_results, 1):\n",
    "            activity_type = row['activity_type'] if row['activity_type'] else \"Unknown\"\n",
    "            failures = row['failure_count']\n",
    "            pct = (failures / failure_count * 100) if failure_count > 0 else 0\n",
    "            print(f\"  {idx:2d}. {activity_type:35s}  {failures:6,} failures ({pct:5.1f}%)\")\n",
    "        \n",
    "        # Top failing items\n",
    "        print(f\"\\n TOP 15 FAILING ITEMS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        failing_items = (failures_df\n",
    "                        .filter(col(\"item_name\").isNotNull())\n",
    "                        .groupBy(\"workspace_name\", \"item_name\", \"item_type\")\n",
    "                        .agg(count(\"*\").alias(\"failure_count\"))\n",
    "                        .orderBy(desc(\"failure_count\"))\n",
    "                        .limit(15))\n",
    "        \n",
    "        failing_item_results = failing_items.collect()\n",
    "        for idx, row in enumerate(failing_item_results, 1):\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown Workspace\"\n",
    "            item_name = row['item_name']\n",
    "            item_type = row['item_type'] if row['item_type'] else \"Unknown\"\n",
    "            failures = row['failure_count']\n",
    "            print(f\"  {idx:2d}. {item_name:30s} ({item_type:15s})  {ws_name:25s}  {failures:5,} failures\")\n",
    "        \n",
    "    else:\n",
    "        print(\" No failures found in the dataset\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. User Activity & Failure Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\" USER ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, countDistinct, desc, when, sum as spark_sum\n",
    "    \n",
    "    # Based on diagnostic: submitted_by has 95.74% data, created_by is 100% NULL\n",
    "    user_column = 'submitted_by'\n",
    "    print(f\"\\n Using '{user_column}' column for user analysis\")\n",
    "    print(\"   (created_by and last_updated_by are 100% NULL in this dataset)\")\n",
    "    \n",
    "    # Filter out system users and nulls\n",
    "    user_activities = complete_df.filter(\n",
    "        (col(user_column).isNotNull()) & \n",
    "        (col(user_column) != \"System\") & \n",
    "        (col(user_column) != \"\")\n",
    "    )\n",
    "    \n",
    "    total_user_activities = user_activities.count()\n",
    "    unique_users = user_activities.select(user_column).distinct().count()\n",
    "    \n",
    "    print(f\"\\n User Activity Overview:\")\n",
    "    print(f\"   Total User Activities: {total_user_activities:,}\")\n",
    "    print(f\"   Unique Active Users: {unique_users:,}\")\n",
    "    \n",
    "    if total_user_activities > 0:\n",
    "        # Top active users - HANDLES DUPLICATES with groupBy aggregation\n",
    "        print(f\"\\n TOP 20 MOST ACTIVE USERS:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # groupBy automatically handles duplicates by aggregating them\n",
    "        # countDistinct ensures we count unique workspaces/items per user\n",
    "        top_users = (user_activities\n",
    "                    .groupBy(user_column)\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"total_activities\"),  # Total activities (including duplicates if any)\n",
    "                        countDistinct(\"workspace_name\").alias(\"workspaces\"),  # Unique workspaces\n",
    "                        countDistinct(\"item_name\").alias(\"unique_items\"),  # Unique items\n",
    "                        spark_sum(when(col(\"status\") == \"Failed\", 1).otherwise(0)).alias(\"failures\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Succeeded\", 1).otherwise(0)).alias(\"successes\")\n",
    "                    )\n",
    "                    .orderBy(desc(\"total_activities\"))\n",
    "                    .limit(20))\n",
    "        \n",
    "        top_user_results = top_users.collect()\n",
    "        \n",
    "        if len(top_user_results) > 0:\n",
    "            for idx, row in enumerate(top_user_results, 1):\n",
    "                user = row[user_column]\n",
    "                activities = row['total_activities']\n",
    "                workspaces = row['workspaces']\n",
    "                items = row['unique_items']\n",
    "                failures = row['failures']\n",
    "                successes = row['successes']\n",
    "                success_rate = (successes / activities * 100) if activities > 0 else 0\n",
    "                print(f\"  {idx:2d}. {user:40s}  {activities:7,} activities  {workspaces:3,} WS  {items:4,} items  {failures:5,} fails  {success_rate:5.1f}% success\")\n",
    "        else:\n",
    "            print(\"   No user data available\")\n",
    "    else:\n",
    "        print(\"\\n    No user activities found (all records may be System or null)\")\n",
    "    \n",
    "    # Users with most failures\n",
    "    if total_user_activities > 0:\n",
    "        user_failures = user_activities.filter(col(\"status\") == \"Failed\")\n",
    "        user_failure_count = user_failures.count()\n",
    "        \n",
    "        if user_failure_count > 0:\n",
    "            print(f\"\\n TOP 10 USERS WITH MOST FAILURES:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # groupBy with countDistinct handles duplicates properly\n",
    "            users_with_failures = (user_failures\n",
    "                                  .groupBy(user_column)\n",
    "                                  .agg(\n",
    "                                      count(\"*\").alias(\"failure_count\"),  # Total failures per user\n",
    "                                      countDistinct(\"workspace_name\").alias(\"affected_workspaces\"),  # Unique workspaces\n",
    "                                      countDistinct(\"item_name\").alias(\"failed_items\")  # Unique items\n",
    "                                  )\n",
    "                                  .orderBy(desc(\"failure_count\"))\n",
    "                                  .limit(10))\n",
    "            \n",
    "            user_failure_results = users_with_failures.collect()\n",
    "            \n",
    "            if len(user_failure_results) > 0:\n",
    "                for idx, row in enumerate(user_failure_results, 1):\n",
    "                    user = row[user_column]\n",
    "                    failures = row['failure_count']\n",
    "                    workspaces = row['affected_workspaces']\n",
    "                    items = row['failed_items']\n",
    "                    print(f\"  {idx:2d}. {user:40s}  {failures:6,} failures  {workspaces:3,} WS  {items:4,} items\")\n",
    "            else:\n",
    "                print(\"   No failure data available\")\n",
    "        else:\n",
    "            print(f\"\\n No failures found for users\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fa6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Error & Failure Reason Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\" ERROR & FAILURE REASON ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, desc\n",
    "    \n",
    "    failures_df = complete_df.filter(col(\"status\") == \"Failed\")\n",
    "    failure_count = failures_df.count()\n",
    "    \n",
    "    if failure_count > 0:\n",
    "        # Check if error columns exist and have data\n",
    "        has_failure_reason = 'failure_reason' in complete_df.columns\n",
    "        has_error_message = 'error_message' in complete_df.columns\n",
    "        \n",
    "        if has_failure_reason:\n",
    "            # Failure reason distribution\n",
    "            print(f\"\\n FAILURE REASONS:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            failure_reasons = (failures_df\n",
    "                             .filter(col(\"failure_reason\").isNotNull() & (col(\"failure_reason\") != \"\"))\n",
    "                             .groupBy(\"failure_reason\")\n",
    "                             .agg(count(\"*\").alias(\"count\"))\n",
    "                             .orderBy(desc(\"count\"))\n",
    "                             .limit(15))\n",
    "            \n",
    "            reason_results = failure_reasons.collect()\n",
    "            \n",
    "            if len(reason_results) > 0:\n",
    "                for idx, row in enumerate(reason_results, 1):\n",
    "                    reason = row['failure_reason']\n",
    "                    count_val = row['count']\n",
    "                    pct = (count_val / failure_count * 100) if failure_count > 0 else 0\n",
    "                    # Truncate long reasons\n",
    "                    reason_display = reason[:70] + \"...\" if len(reason) > 70 else reason\n",
    "                    print(f\"  {idx:2d}. {reason_display:73s}  {count_val:5,} ({pct:5.1f}%)\")\n",
    "            else:\n",
    "                print(\"   No failure reason data available\")\n",
    "        \n",
    "        if has_error_message:\n",
    "            # Sample error messages for top failures\n",
    "            print(f\"\\n SAMPLE ERROR MESSAGES (Top 10):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            error_samples = (failures_df\n",
    "                           .filter(col(\"error_message\").isNotNull() & (col(\"error_message\") != \"\"))\n",
    "                           .select(\"workspace_name\", \"item_name\", \"error_message\")\n",
    "                           .limit(10))\n",
    "            \n",
    "            error_results = error_samples.collect()\n",
    "            \n",
    "            if len(error_results) > 0:\n",
    "                for idx, row in enumerate(error_results, 1):\n",
    "                    ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown\"\n",
    "                    item = row['item_name'] if row['item_name'] else \"Unknown\"\n",
    "                    error = row['error_message']\n",
    "                    # Truncate long messages\n",
    "                    error_display = error[:100] + \"...\" if len(error) > 100 else error\n",
    "                    print(f\"\\n  {idx:2d}. Workspace: {ws_name}\")\n",
    "                    print(f\"      Item: {item}\")\n",
    "                    print(f\"      Error: {error_display}\")\n",
    "            else:\n",
    "                print(\"   No error message data available\")\n",
    "        \n",
    "        if not has_failure_reason and not has_error_message:\n",
    "            print(\"\\n  No failure_reason or error_message columns found in data\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n No failures to analyze\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Time-Based Analysis (Date & Duration)\n",
    "print(\"=\" * 80)\n",
    "print(\" TIME-BASED ACTIVITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if complete_df:\n",
    "    from pyspark.sql.functions import col, count, desc, avg, sum as spark_sum, when, max as spark_max, min as spark_min\n",
    "    \n",
    "    # Activities by date\n",
    "    print(f\"\\n ACTIVITY DISTRIBUTION BY DATE:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    date_activity = (complete_df\n",
    "                    .filter(col(\"date\").isNotNull())\n",
    "                    .groupBy(\"date\")\n",
    "                    .agg(\n",
    "                        count(\"*\").alias(\"total_activities\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Failed\", 1).otherwise(0)).alias(\"failures\"),\n",
    "                        spark_sum(when(col(\"status\") == \"Succeeded\", 1).otherwise(0)).alias(\"successes\")\n",
    "                    )\n",
    "                    .orderBy(desc(\"date\"))\n",
    "                    .limit(15))\n",
    "    \n",
    "    date_results = date_activity.collect()\n",
    "    \n",
    "    if len(date_results) > 0:\n",
    "        for row in date_results:\n",
    "            date_val = row['date']\n",
    "            total = row['total_activities']\n",
    "            failures = row['failures']\n",
    "            successes = row['successes']\n",
    "            success_rate = (successes / total * 100) if total > 0 else 0\n",
    "            print(f\"  {date_val}  {total:8,} total  {successes:8,} success  {failures:6,} failed  {success_rate:5.1f}% success\")\n",
    "    else:\n",
    "        print(\"   No date information available\")\n",
    "    \n",
    "    # Duration analysis\n",
    "    print(f\"\\nâ±  DURATION ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    duration_df = complete_df.filter(col(\"duration_seconds\").isNotNull() & (col(\"duration_seconds\").cast(\"double\") > 0))\n",
    "    duration_count = duration_df.count()\n",
    "    total_records = complete_df.count()\n",
    "    \n",
    "    if duration_count > 0:\n",
    "        print(f\"  Activities with duration data: {duration_count:,} ({duration_count/total_records*100:.1f}%)\")\n",
    "        \n",
    "        # Overall duration statistics\n",
    "        duration_stats = duration_df.agg(\n",
    "            avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\"),\n",
    "            spark_max(col(\"duration_seconds\").cast(\"double\")).alias(\"max_duration\"),\n",
    "            spark_min(col(\"duration_seconds\").cast(\"double\")).alias(\"min_duration\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\n  Overall Duration Statistics:\")\n",
    "        print(f\"    Average: {duration_stats['avg_duration']:.2f}s ({duration_stats['avg_duration']/60:.2f} minutes)\")\n",
    "        print(f\"    Maximum: {duration_stats['max_duration']:.2f}s ({duration_stats['max_duration']/60:.2f} minutes)\")\n",
    "        print(f\"    Minimum: {duration_stats['min_duration']:.2f}s\")\n",
    "        \n",
    "        # Duration by status\n",
    "        print(f\"\\n  Duration by Status:\")\n",
    "        duration_by_status = (duration_df\n",
    "                             .groupBy(\"status\")\n",
    "                             .agg(\n",
    "                                 count(\"*\").alias(\"count\"),\n",
    "                                 avg(col(\"duration_seconds\").cast(\"double\")).alias(\"avg_duration\")\n",
    "                             )\n",
    "                             .orderBy(desc(\"count\")))\n",
    "        \n",
    "        status_duration_results = duration_by_status.collect()\n",
    "        for row in status_duration_results:\n",
    "            status = row['status'] if row['status'] else \"Unknown\"\n",
    "            count_val = row['count']\n",
    "            avg_dur = row['avg_duration']\n",
    "            print(f\"    {status:15s}: {count_val:8,} activities, avg {avg_dur:.2f}s\")\n",
    "        \n",
    "        # Longest running activities\n",
    "        print(f\"\\n   TOP 10 LONGEST RUNNING ACTIVITIES:\")\n",
    "        longest_activities = (duration_df\n",
    "                             .select(\"workspace_name\", \"item_name\", \"activity_type\", \"status\", \n",
    "                                   col(\"duration_seconds\").cast(\"double\").alias(\"duration\"))\n",
    "                             .orderBy(desc(\"duration\"))\n",
    "                             .limit(10))\n",
    "        \n",
    "        longest_results = longest_activities.collect()\n",
    "        for idx, row in enumerate(longest_results, 1):\n",
    "            ws_name = row['workspace_name'] if row['workspace_name'] else \"Unknown\"\n",
    "            item = row['item_name'] if row['item_name'] else \"Unknown\"\n",
    "            activity = row['activity_type'] if row['activity_type'] else \"Unknown\"\n",
    "            status = row['status']\n",
    "            duration = row['duration']\n",
    "            duration_min = duration / 60\n",
    "            print(f\"    {idx:2d}. {ws_name:30s}  {item:25s}  {duration:.1f}s ({duration_min:.1f}m) [{status}]\")\n",
    "    else:\n",
    "        print(\"   No duration data available\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(\" complete_df not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aced05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the schema definition\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import usf_fabric_monitoring\n",
    "\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.schema import FabricSemanticModel, ALL_DDLS\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Module not found in installed package. Attempting to patch package path...\")\n",
    "    \n",
    "    # 1. Find local src directory\n",
    "    current_dir = Path(os.getcwd())\n",
    "    if current_dir.name == \"notebooks\":\n",
    "        src_path = current_dir.parent / \"src\"\n",
    "    else:\n",
    "        src_path = current_dir / \"src\"\n",
    "        \n",
    "    # 2. Add to sys.path if missing\n",
    "    if src_path.exists() and str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "        print(f\"Added {src_path} to sys.path\")\n",
    "        \n",
    "    # 3. CRITICAL: Patch the already loaded package's __path__\n",
    "    # This tells Python to look for submodules in the local folder too\n",
    "    local_package_path = src_path / \"usf_fabric_monitoring\"\n",
    "    if local_package_path.exists():\n",
    "        # Convert to string for compatibility\n",
    "        local_path_str = str(local_package_path)\n",
    "        if local_path_str not in usf_fabric_monitoring.__path__:\n",
    "            usf_fabric_monitoring.__path__.insert(0, local_path_str)\n",
    "            print(f\"Patched usf_fabric_monitoring.__path__ with: {local_path_str}\")\n",
    "            \n",
    "            # Also need to patch 'core' if it's already loaded\n",
    "            if hasattr(usf_fabric_monitoring, 'core') and hasattr(usf_fabric_monitoring.core, '__path__'):\n",
    "                local_core_path = local_package_path / \"core\"\n",
    "                if str(local_core_path) not in usf_fabric_monitoring.core.__path__:\n",
    "                    usf_fabric_monitoring.core.__path__.insert(0, str(local_core_path))\n",
    "                    print(f\"Patched usf_fabric_monitoring.core.__path__\")\n",
    "\n",
    "    # Retry import\n",
    "    from usf_fabric_monitoring.core.schema import FabricSemanticModel, ALL_DDLS\n",
    "\n",
    "# Initialize the model\n",
    "model = FabricSemanticModel()\n",
    "\n",
    "# Print the Semantic Model Description\n",
    "print(model.describe())\n",
    "\n",
    "# Example: Print DDL for Activities Master\n",
    "print(\"\\nDDL for Activities Master:\")\n",
    "print(ALL_DDLS[\"activities_master\"])"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "e818f402-5fea-490c-9eeb-2e9258522102",
    "workspaceId": "944ae69e-1c99-4ad7-973f-c296c778a5c5"
   },
   "lakehouse": {
    "default_lakehouse": "5cb78b9b-b153-4771-b309-65ec4848433a",
    "default_lakehouse_name": "lh_01_bronze",
    "default_lakehouse_workspace_id": "944ae69e-1c99-4ad7-973f-c296c778a5c5",
    "known_lakehouses": [
     {
      "id": "5cb78b9b-b153-4771-b309-65ec4848433a"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
