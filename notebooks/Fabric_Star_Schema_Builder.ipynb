{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cec7b77",
   "metadata": {},
   "source": [
    "# Star Schema Builder for Fabric Monitoring Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook transforms raw Monitor Hub activity data into a Kimball-style star schema suitable for SQL queries, semantic models, and Power BI reports.\n",
    "\n",
    "It is designed to work in both:\n",
    "- **Microsoft Fabric notebooks** (paths auto-resolve under `/lakehouse/default/Files/`), and\n",
    "- **Local dev** (writes under `exports/` by default).\n",
    "\n",
    "## Star Schema Tables\n",
    "- **Dimensions**: dim_date, dim_time, dim_workspace, dim_item, dim_user, dim_activity_type, dim_status\n",
    "- **Facts**: fact_activity, fact_daily_metrics\n",
    "\n",
    "## Key Features\n",
    "- Incremental loading with high-water mark tracking\n",
    "- SCD Type 2 support for slowly changing dimensions\n",
    "- Automatic surrogate key generation\n",
    "- Pre-aggregated daily metrics for fast dashboards\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package** (first run only): Uncomment and run the pip install cell\n",
    "2. **Configure Paths**: Set INPUT_DIR and OUTPUT_DIR for your environment\n",
    "3. **Run Pipeline**: Execute the build cells\n",
    "4. **Optional**: Convert to Delta tables for SQL Endpoint access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d0ab8",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "<span style=\"color:red\">pip install is only required on first run. Uncomment and run once, then re-comment.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e80275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install /lakehouse/default/Files/usf_fabric_monitoring-0.3.0-py3-none-any.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f6749",
   "metadata": {},
   "source": [
    "## Setup Local Path (For Local Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffb516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/src to sys.path (local development mode)\n"
     ]
    }
   ],
   "source": [
    "# SETUP LOCAL PATH (For Local Development)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to sys.path to allow importing the local package\n",
    "# This is necessary when running locally without installing the package\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# Check if we are in notebooks directory\n",
    "if current_dir.name == \"notebooks\":\n",
    "    src_path = current_dir.parent / \"src\"\n",
    "else:\n",
    "    # Assume we are in project root\n",
    "    src_path = current_dir / \"src\"\n",
    "\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"‚úÖ Added {src_path} to sys.path (local development mode)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Running in Fabric or package already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b118a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules reloaded\n"
     ]
    }
   ],
   "source": [
    "# Force reload of modules to pick up code changes\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb\n",
    "importlib.reload(ssb)\n",
    "print(\"‚úÖ Modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1736ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usf_fabric_monitoring version: 0.3.2\n",
      "Resolved output dir example: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "‚úÖ StarSchemaBuilder module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Package / environment verification (safe: no Azure/API imports)\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "import importlib\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "try:\n",
    "    pkg_version = getattr(usf_fabric_monitoring, \"__version__\", None) or version(\"usf_fabric_monitoring\")\n",
    "except PackageNotFoundError:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "print(f\"usf_fabric_monitoring version: {pkg_version}\")\n",
    "print(f\"Resolved output dir example: {resolve_path('exports/star_schema')}\")\n",
    "\n",
    "# Check star schema builder availability\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.star_schema_builder import StarSchemaBuilder, ALL_STAR_SCHEMA_DDLS\n",
    "    print(\"‚úÖ StarSchemaBuilder module loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import StarSchemaBuilder: {e}\")\n",
    "    print(\"   Make sure you have version 0.3.0+ installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435a0cf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061d87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAR SCHEMA BUILDER CONFIGURATION\n",
      "============================================================\n",
      "Input Directory:       /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis\n",
      "Output Directory:      /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "Mode:                  Incremental\n",
      "Write to Delta Tables: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# ============================================================================\n",
    "\n",
    "# Input: Where Monitor Hub pipeline outputs are stored (CSV files with Smart Merge data)\n",
    "INPUT_DIR = resolve_path(\"exports/monitor_hub_analysis\")\n",
    "\n",
    "# Output: Where star schema tables will be written\n",
    "OUTPUT_DIR = resolve_path(\"exports/star_schema\")\n",
    "\n",
    "# Processing options\n",
    "INCREMENTAL_LOAD = True  # Set to False for full refresh (rebuilds all tables)\n",
    "WRITE_TO_DELTA_TABLES = False  # Set to True in Fabric to create SQL Endpoint tables\n",
    "\n",
    "# ============================================================================\n",
    "# Display configuration\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA BUILDER CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input Directory:       {INPUT_DIR}\")\n",
    "print(f\"Output Directory:      {OUTPUT_DIR}\")\n",
    "print(f\"Mode:                  {'Incremental' if INCREMENTAL_LOAD else 'Full Refresh'}\")\n",
    "print(f\"Write to Delta Tables: {WRITE_TO_DELTA_TABLES}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8267b22",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39150246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available activity files:\n",
      "  1. activities_master_20251204_153124.csv - No failures in sample\n",
      "  2. activities_master_20251203_212443.csv - Has failures\n",
      "  3. activities_master_20251203_082824.csv - No failures in sample\n",
      "  2. activities_master_20251203_212443.csv - Has failures\n",
      "  3. activities_master_20251203_082824.csv - No failures in sample\n",
      "  4. activities_master_20251201_232450.csv - No failures in sample\n",
      "  4. activities_master_20251201_232450.csv - No failures in sample\n",
      "\n",
      "‚úÖ Selected file with Smart Merge failure data: activities_master_20251203_212443.csv\n",
      "\n",
      "‚úÖ Selected file with Smart Merge failure data: activities_master_20251203_212443.csv\n",
      "\n",
      "‚úÖ Loaded 968,766 activity records\n",
      "   Columns: 19 total\n",
      "   Status distribution:\n",
      "     - Succeeded: 961,773\n",
      "     - Completed: 5,747\n",
      "     - Failed: 1,203\n",
      "     - Cancelled: 34\n",
      "     - InProgress: 9\n",
      "   Date range: 2025-11-03 01:00:03.450000 to 2025-12-03 21:00:02.754907300\n",
      "\n",
      "‚úÖ Loaded 968,766 activity records\n",
      "   Columns: 19 total\n",
      "   Status distribution:\n",
      "     - Succeeded: 961,773\n",
      "     - Completed: 5,747\n",
      "     - Failed: 1,203\n",
      "     - Cancelled: 34\n",
      "     - InProgress: 9\n",
      "   Date range: 2025-11-03 01:00:03.450000 to 2025-12-03 21:00:02.754907300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Find activities_master CSV files (contains Smart Merge enriched data with accurate failure status)\n",
    "input_path = Path(INPUT_DIR)\n",
    "activities_files = sorted(input_path.glob(\"activities_master_*.csv\"), reverse=True)\n",
    "\n",
    "if not activities_files:\n",
    "    # Fallback to parquet if no CSV\n",
    "    parquet_dir = input_path / \"parquet\"\n",
    "    parquet_files = sorted(parquet_dir.glob(\"activities_*.parquet\"), reverse=True) if parquet_dir.exists() else []\n",
    "    if parquet_files:\n",
    "        print(f\"‚ö†Ô∏è No CSV files found, falling back to parquet: {parquet_files[0].name}\")\n",
    "        activities_df = pd.read_parquet(parquet_files[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No activities files found in {INPUT_DIR}\\n\"\n",
    "            \"Run the Monitor Hub pipeline first: make monitor-hub\"\n",
    "        )\n",
    "else:\n",
    "    # Show available files\n",
    "    print(\"Available activity files:\")\n",
    "    for i, f in enumerate(activities_files[:5]):\n",
    "        df_check = pd.read_csv(f, usecols=['status'], nrows=100000)\n",
    "        failed_sample = (df_check['status'] == 'Failed').sum()\n",
    "        print(f\"  {i+1}. {f.name} - {'Has failures' if failed_sample > 0 else 'No failures in sample'}\")\n",
    "    \n",
    "    # Try to find a file with failures, otherwise use latest\n",
    "    selected_file = None\n",
    "    for f in activities_files:\n",
    "        df_check = pd.read_csv(f, usecols=['status'], low_memory=False)\n",
    "        if (df_check['status'] == 'Failed').sum() > 0:\n",
    "            selected_file = f\n",
    "            print(f\"\\n‚úÖ Selected file with Smart Merge failure data: {f.name}\")\n",
    "            break\n",
    "    \n",
    "    if selected_file is None:\n",
    "        selected_file = activities_files[0]\n",
    "        print(f\"\\n‚ö†Ô∏è No file with failures found, using latest: {selected_file.name}\")\n",
    "    \n",
    "    activities_df = pd.read_csv(selected_file, low_memory=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(activities_df):,} activity records\")\n",
    "print(f\"   Columns: {len(activities_df.columns)} total\")\n",
    "\n",
    "# Show status distribution (key for Smart Merge validation)\n",
    "if 'status' in activities_df.columns:\n",
    "    status_counts = activities_df['status'].value_counts()\n",
    "    print(f\"   Status distribution:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"     - {status}: {count:,}\")\n",
    "\n",
    "# Show date range\n",
    "time_col = 'start_time' if 'start_time' in activities_df.columns else 'StartTimeUtc'\n",
    "if time_col in activities_df.columns:\n",
    "    activities_df[time_col] = pd.to_datetime(activities_df[time_col], errors='coerce')\n",
    "    print(f\"   Date range: {activities_df[time_col].min()} to {activities_df[time_col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eba7f3",
   "metadata": {},
   "source": [
    "## Build Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6176fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING STAR SCHEMA\n",
      "============================================================\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Starting Star Schema Build\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Mode: Incremental\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Input activities: 968766\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Starting Star Schema Build\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Mode: Incremental\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Input activities: 968766\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 1: Building reference dimensions...\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 1: Building reference dimensions...\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_date with 456 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_date.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_time with 96 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_time.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_activity_type with 61 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_activity_type.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_status with 8 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_status.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 2: Building slowly changing dimensions...\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_date with 456 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_date.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_time with 96 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_time.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_activity_type with 61 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_activity_type.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_status with 8 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_status.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 2: Building slowly changing dimensions...\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_workspace with 159 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_workspace.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_workspace with 159 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_workspace.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_item with 2213 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_item.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_item with 2213 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_item.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_user with 95 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_user.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 3: Building fact activity table...\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Saved dim_user with 95 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_user.parquet\n",
      "2025-12-17 01:09:43 | INFO | star_schema_builder | Step 3: Building fact activity table...\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Saved fact_activity with 1930540 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/fact_activity.parquet\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Step 4: Building aggregate metrics...\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Saved fact_activity with 1930540 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/fact_activity.parquet\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Step 4: Building aggregate metrics...\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Saved fact_daily_metrics with 1573 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/fact_daily_metrics.parquet\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Star Schema Build Complete\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Duration: 95.26 seconds\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Output: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Saved fact_daily_metrics with 1573 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/fact_daily_metrics.parquet\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Star Schema Build Complete\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Duration: 95.26 seconds\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | Output: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "2025-12-17 01:11:18 | INFO | star_schema_builder | ============================================================\n",
      "\n",
      "‚úÖ Star Schema build completed in 100.92 seconds!\n",
      "\n",
      "üìä Dimensions Built:\n",
      "   ‚Ä¢ dim_date: 456 records\n",
      "   ‚Ä¢ dim_time: 96 records\n",
      "   ‚Ä¢ dim_activity_type: 61 records\n",
      "   ‚Ä¢ dim_status: 8 records\n",
      "   ‚Ä¢ dim_workspace: 159 records (+1 new)\n",
      "   ‚Ä¢ dim_item: 2,213 records\n",
      "   ‚Ä¢ dim_user: 95 records\n",
      "\n",
      "üìà Fact Tables Built:\n",
      "   ‚Ä¢ fact_activity: 1,930,540 records (+961774 new)\n",
      "   ‚Ä¢ fact_daily_metrics: 1,573 records\n",
      "\n",
      "üìÅ Output Directory: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "\n",
      "‚úÖ Star Schema build completed in 100.92 seconds!\n",
      "\n",
      "üìä Dimensions Built:\n",
      "   ‚Ä¢ dim_date: 456 records\n",
      "   ‚Ä¢ dim_time: 96 records\n",
      "   ‚Ä¢ dim_activity_type: 61 records\n",
      "   ‚Ä¢ dim_status: 8 records\n",
      "   ‚Ä¢ dim_workspace: 159 records (+1 new)\n",
      "   ‚Ä¢ dim_item: 2,213 records\n",
      "   ‚Ä¢ dim_user: 95 records\n",
      "\n",
      "üìà Fact Tables Built:\n",
      "   ‚Ä¢ fact_activity: 1,930,540 records (+961774 new)\n",
      "   ‚Ä¢ fact_daily_metrics: 1,573 records\n",
      "\n",
      "üìÅ Output Directory: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n"
     ]
    }
   ],
   "source": [
    "# Use reloaded module\n",
    "from usf_fabric_monitoring.core.star_schema_builder import StarSchemaBuilder\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb_module\n",
    "importlib.reload(ssb_module)\n",
    "StarSchemaBuilder = ssb_module.StarSchemaBuilder\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build star schema\n",
    "print(\"=\" * 60)\n",
    "print(\"BUILDING STAR SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "start_time = datetime.now()\n",
    "\n",
    "builder = StarSchemaBuilder(output_directory=OUTPUT_DIR)\n",
    "results = builder.build_complete_schema(\n",
    "    activities=activities_df.to_dict(orient=\"records\"),\n",
    "    incremental=INCREMENTAL_LOAD\n",
    ")\n",
    "\n",
    "duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "if results[\"status\"] == \"success\":\n",
    "    print(f\"\\n‚úÖ Star Schema build completed in {duration:.2f} seconds!\")\n",
    "    \n",
    "    print(f\"\\nüìä Dimensions Built:\")\n",
    "    for dim_name, count in results.get(\"dimensions_built\", {}).items():\n",
    "        if not dim_name.endswith(\"_new\"):\n",
    "            new_count = results.get(\"dimensions_built\", {}).get(f\"{dim_name}_new\", \"\")\n",
    "            new_suffix = f\" (+{new_count} new)\" if new_count else \"\"\n",
    "            print(f\"   ‚Ä¢ {dim_name}: {count:,} records{new_suffix}\")\n",
    "    \n",
    "    print(f\"\\nüìà Fact Tables Built:\")\n",
    "    for fact_name, count in results.get(\"facts_built\", {}).items():\n",
    "        if not fact_name.endswith(\"_new\"):\n",
    "            new_count = results.get(\"facts_built\", {}).get(f\"{fact_name}_new\", \"\")\n",
    "            new_suffix = f\" (+{new_count} new)\" if new_count else \"\"\n",
    "            print(f\"   ‚Ä¢ {fact_name}: {count:,} records{new_suffix}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Output Directory: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Build failed:\")\n",
    "    for error in results.get(\"errors\", []):\n",
    "        print(f\"   {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c04cc",
   "metadata": {},
   "source": [
    "## Convert to Delta Tables (Fabric Only)\n",
    "Run this cell only in Microsoft Fabric to create Delta tables accessible via SQL Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96be1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Delta table creation skipped (WRITE_TO_DELTA_TABLES=False)\n",
      "   Set WRITE_TO_DELTA_TABLES=True in Fabric to enable.\n"
     ]
    }
   ],
   "source": [
    "if WRITE_TO_DELTA_TABLES:\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        print(\"Converting Parquet files to Delta tables...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert each parquet to Delta table\n",
    "        for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "            table_name = parquet_file.stem  # e.g., \"dim_date\", \"fact_activity\"\n",
    "            \n",
    "            try:\n",
    "                df = spark.read.parquet(str(parquet_file))\n",
    "                \n",
    "                # Write as Delta table (overwrite mode)\n",
    "                df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "                \n",
    "                print(f\"   ‚úÖ {table_name}: {df.count():,} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {table_name}: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(\"‚úÖ Delta tables created successfully!\")\n",
    "        print(\"   Tables are now available in the SQL Endpoint.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PySpark not available. Delta table creation skipped.\")\n",
    "        print(\"   This feature is only available in Microsoft Fabric.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Delta table creation skipped (WRITE_TO_DELTA_TABLES=False)\")\n",
    "    print(\"   Set WRITE_TO_DELTA_TABLES=True in Fabric to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6a547",
   "metadata": {},
   "source": [
    "## Validate Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25ce57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAR SCHEMA VALIDATION\n",
      "============================================================\n",
      "dim_activity_type: 61 records\n",
      "dim_user: 95 records\n",
      "dim_item: 2,213 records\n",
      "dim_workspace: 159 records\n",
      "fact_daily_metrics: 1,573 records\n",
      "dim_time: 96 records\n",
      "dim_date: 456 records\n",
      "dim_status: 8 records\n",
      "fact_activity: 1,930,540 records\n",
      "\n",
      "üîó Foreign Key Validation:\n",
      "   workspace_sk: ‚úÖ PASS\n",
      "   item_sk: ‚úÖ PASS\n",
      "   user_sk: ‚úÖ PASS\n",
      "   date_sk: ‚úÖ PASS\n",
      "   time_sk: ‚úÖ PASS\n",
      "   activity_type_sk: ‚úÖ PASS\n",
      "   status_sk: ‚úÖ PASS\n",
      "\n",
      "‚úÖ All FK validations passed!\n",
      "   time_sk: ‚úÖ PASS\n",
      "   activity_type_sk: ‚úÖ PASS\n",
      "   status_sk: ‚úÖ PASS\n",
      "\n",
      "‚úÖ All FK validations passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and validate tables\n",
    "tables = {}\n",
    "for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "    table_name = parquet_file.stem\n",
    "    tables[table_name] = pd.read_parquet(parquet_file)\n",
    "    print(f\"{table_name}: {len(tables[table_name]):,} records\")\n",
    "\n",
    "# Check FK integrity\n",
    "print(\"\\nüîó Foreign Key Validation:\")\n",
    "\n",
    "fk_checks = [\n",
    "    ('fact_activity', 'workspace_sk', 'dim_workspace', 'workspace_sk'),\n",
    "    ('fact_activity', 'item_sk', 'dim_item', 'item_sk'),\n",
    "    ('fact_activity', 'user_sk', 'dim_user', 'user_sk'),\n",
    "    ('fact_activity', 'date_sk', 'dim_date', 'date_sk'),\n",
    "    ('fact_activity', 'time_sk', 'dim_time', 'time_sk'),\n",
    "    ('fact_activity', 'activity_type_sk', 'dim_activity_type', 'activity_type_sk'),\n",
    "    ('fact_activity', 'status_sk', 'dim_status', 'status_sk'),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for fact_table, fact_col, dim_table, dim_col in fk_checks:\n",
    "    if fact_table in tables and dim_table in tables:\n",
    "        fact_vals = set(tables[fact_table][fact_col].dropna().unique())\n",
    "        dim_vals = set(tables[dim_table][dim_col].unique())\n",
    "        orphans = fact_vals - dim_vals\n",
    "        status = '‚úÖ PASS' if len(orphans) == 0 else f'‚ùå {len(orphans)} orphans'\n",
    "        if len(orphans) > 0:\n",
    "            all_passed = False\n",
    "        print(f\"   {fact_col}: {status}\")\n",
    "\n",
    "print(\"\\n\" + (\"‚úÖ All FK validations passed!\" if all_passed else \"‚ö†Ô∏è Some FK validations failed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd3611",
   "metadata": {},
   "source": [
    "## Sample Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e79a632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Top 10 Most Active Workspaces\n",
      "--------------------------------------------------\n",
      "   Unknown                                  |  1,937,534 activities |   95 users\n",
      "   Unknown                                  |  1,937,534 activities |   95 users\n"
     ]
    }
   ],
   "source": [
    "# Top 10 Most Active Workspaces\n",
    "print(\"üìä Top 10 Most Active Workspaces\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    merged = fact.merge(dim_ws[['workspace_sk', 'workspace_name']], on='workspace_sk')\n",
    "    result = merged.groupby('workspace_name').agg(\n",
    "        activity_count=('workspace_sk', 'count'),\n",
    "        unique_users=('user_sk', 'nunique')\n",
    "    ).sort_values('activity_count', ascending=False).head(10)\n",
    "    \n",
    "    for ws_name, row in result.iterrows():\n",
    "        print(f\"   {ws_name[:40]:<40} | {row['activity_count']:>10,} activities | {row['unique_users']:>4} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea0b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Activity Count by Type\n",
      "--------------------------------------------------\n",
      "   Artifact Operations | ReadArtifact                   |    669,120\n",
      "   File Operations | CreateFile                     |    382,302\n",
      "   File Operations | RenameFileOrDirectory          |    176,752\n",
      "   Compute         | RunArtifact                    |    170,752\n",
      "   File Operations | CreateDirectory                |    136,566\n",
      "   Spark           | ViewSparkAppLog                |    107,356\n",
      "   File Operations | DeleteFileOrBlob               |     60,954\n",
      "   Artifact Operations | UpdateArtifact                 |     53,972\n",
      "   Query           | ConnectWarehouseAndSqlAnalyticsEndpointLakehouseFromExternalApp |     39,874\n",
      "   Spark           | MountStorageByMssparkutils     |     27,654\n",
      "   Artifact Operations | ReadArtifact                   |    669,120\n",
      "   File Operations | CreateFile                     |    382,302\n",
      "   File Operations | RenameFileOrDirectory          |    176,752\n",
      "   Compute         | RunArtifact                    |    170,752\n",
      "   File Operations | CreateDirectory                |    136,566\n",
      "   Spark           | ViewSparkAppLog                |    107,356\n",
      "   File Operations | DeleteFileOrBlob               |     60,954\n",
      "   Artifact Operations | UpdateArtifact                 |     53,972\n",
      "   Query           | ConnectWarehouseAndSqlAnalyticsEndpointLakehouseFromExternalApp |     39,874\n",
      "   Spark           | MountStorageByMssparkutils     |     27,654\n"
     ]
    }
   ],
   "source": [
    "# Activity by Type\n",
    "print(\"üìä Activity Count by Type\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_activity_type' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_type = tables['dim_activity_type']\n",
    "    \n",
    "    merged = fact.merge(dim_type[['activity_type_sk', 'activity_type', 'activity_category']], on='activity_type_sk')\n",
    "    result = merged.groupby(['activity_category', 'activity_type']).size().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    for (cat, act_type), count in result.items():\n",
    "        print(f\"   {cat:<15} | {act_type:<30} | {count:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82c4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Daily Activity Trend (Last 14 Days)\n",
      "--------------------------------------------------\n",
      "   2025-12-03 (Wed) |      761 activities |    1 users |  95 failed\n",
      "   2025-12-02 (Tue) |   60,809 activities |  283 users |  41 failed\n",
      "   2025-12-01 (Mon) |   47,268 activities |  124 users | 106 failed\n",
      "   2025-11-30 (Sun) |   20,184 activities |   56 users |  30 failed\n",
      "   2025-11-29 (Sat) |   15,260 activities |   60 users |  17 failed\n",
      "   2025-11-28 (Fri) |   32,843 activities |  107 users |  23 failed\n",
      "   2025-11-27 (Thu) |   27,540 activities |  113 users |  34 failed\n",
      "   2025-11-26 (Wed) |   36,159 activities |  115 users |  33 failed\n",
      "   2025-11-25 (Tue) |   29,762 activities |  241 users |  30 failed\n",
      "   2025-11-24 (Mon) |   32,881 activities |  121 users |  28 failed\n",
      "   2025-11-23 (Sun) |   18,854 activities |   52 users |  22 failed\n",
      "   2025-11-22 (Sat) |   15,373 activities |   52 users |  19 failed\n",
      "   2025-11-21 (Fri) |   24,490 activities |  103 users |  42 failed\n",
      "   2025-11-20 (Thu) |   52,738 activities |  107 users |  35 failed\n"
     ]
    }
   ],
   "source": [
    "# Daily Activity Trend\n",
    "print(\"üìä Daily Activity Trend (Last 14 Days)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_daily_metrics' in tables and 'dim_date' in tables:\n",
    "    fact_daily = tables['fact_daily_metrics']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Aggregate across workspaces\n",
    "    daily_totals = fact_daily.groupby('date_sk').agg({\n",
    "        'total_activities': 'sum',\n",
    "        'unique_users': 'sum',\n",
    "        'failed_activities': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Join with date dimension\n",
    "    daily_totals = daily_totals.merge(\n",
    "        dim_date[['date_sk', 'full_date', 'day_of_week_name']], \n",
    "        on='date_sk'\n",
    "    ).sort_values('full_date', ascending=False).head(14)\n",
    "    \n",
    "    for _, row in daily_totals.iterrows():\n",
    "        day = row['day_of_week_name'][:3]\n",
    "        print(f\"   {row['full_date']} ({day}) | {int(row['total_activities']):>8,} activities | {int(row['unique_users']):>4} users | {int(row['failed_activities']):>3} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c3b9f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Failure Analysis\n",
      "--------------------------------------------------\n",
      "   Total Activities:    1,930,540\n",
      "   Failed Activities:   1,238\n",
      "   Success Rate:        99.94%\n",
      "\n",
      "   Failures by Status:\n",
      "     - Failed: 1,204\n",
      "     - Cancelled: 34\n",
      "\n",
      "   Top Failed Activity Types:\n",
      "     - Unknown: 1,238\n"
     ]
    }
   ],
   "source": [
    "# Failure Analysis (Smart Merge Data)\n",
    "print(\"üìä Failure Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_status' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_status = tables['dim_status']\n",
    "    \n",
    "    # Overall failure stats\n",
    "    total = len(fact)\n",
    "    failed = (fact['is_failed'] == 1).sum()\n",
    "    success_rate = ((total - failed) / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    print(f\"   Total Activities:    {total:,}\")\n",
    "    print(f\"   Failed Activities:   {failed:,}\")\n",
    "    print(f\"   Success Rate:        {success_rate:.2f}%\")\n",
    "    \n",
    "    # Failures by status\n",
    "    print(f\"\\n   Failures by Status:\")\n",
    "    merged = fact[fact['is_failed'] == 1].merge(\n",
    "        dim_status[['status_sk', 'status_code']], on='status_sk'\n",
    "    )\n",
    "    if len(merged) > 0:\n",
    "        for status, count in merged['status_code'].value_counts().items():\n",
    "            print(f\"     - {status}: {count:,}\")\n",
    "    \n",
    "    # Failures by activity type (if available)\n",
    "    if 'dim_activity_type' in tables:\n",
    "        dim_type = tables['dim_activity_type']\n",
    "        merged = fact[fact['is_failed'] == 1].merge(\n",
    "            dim_type[['activity_type_sk', 'activity_type']], on='activity_type_sk'\n",
    "        )\n",
    "        if len(merged) > 0:\n",
    "            print(f\"\\n   Top Failed Activity Types:\")\n",
    "            for act_type, count in merged['activity_type'].value_counts().head(5).items():\n",
    "                print(f\"     - {act_type}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22a685",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The star schema has been built and is ready for:\n",
    "\n",
    "1. **SQL Queries** - Query the parquet files directly or use Delta tables via SQL Endpoint\n",
    "2. **Semantic Model** - Create a Direct Lake model pointing to these tables\n",
    "3. **Power BI Reports** - Build monitoring dashboards using the semantic model\n",
    "\n",
    "### Scheduled Refresh\n",
    "To automate the star schema build:\n",
    "1. Schedule this notebook to run daily after the Monitor Hub pipeline\n",
    "2. Or use a Fabric Data Pipeline to orchestrate both steps\n",
    "\n",
    "### Table Relationships (for Semantic Model)\n",
    "```\n",
    "fact_activity[date_sk] ‚Üí dim_date[date_sk]\n",
    "fact_activity[time_sk] ‚Üí dim_time[time_sk]\n",
    "fact_activity[workspace_sk] ‚Üí dim_workspace[workspace_sk]\n",
    "fact_activity[item_sk] ‚Üí dim_item[item_sk]\n",
    "fact_activity[user_sk] ‚Üí dim_user[user_sk]\n",
    "fact_activity[activity_type_sk] ‚Üí dim_activity_type[activity_type_sk]\n",
    "fact_activity[status_sk] ‚Üí dim_status[status_sk]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
