{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cec7b77",
   "metadata": {},
   "source": [
    "# Star Schema Builder for Fabric Monitoring Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook transforms raw Monitor Hub activity data into a Kimball-style star schema suitable for SQL queries, semantic models, and Power BI reports.\n",
    "\n",
    "It is designed to work in both:\n",
    "- **Microsoft Fabric notebooks** (paths auto-resolve under `/lakehouse/default/Files/`), and\n",
    "- **Local dev** (writes under `exports/` by default).\n",
    "\n",
    "## Star Schema Tables\n",
    "- **Dimensions**: dim_date, dim_time, dim_workspace, dim_item, dim_user, dim_activity_type, dim_status\n",
    "- **Facts**: fact_activity, fact_daily_metrics\n",
    "\n",
    "## Key Features\n",
    "- Incremental loading with high-water mark tracking\n",
    "- SCD Type 2 support for slowly changing dimensions\n",
    "- Automatic surrogate key generation\n",
    "- Pre-aggregated daily metrics for fast dashboards\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package** (first run only): Uncomment and run the pip install cell\n",
    "2. **Configure Paths**: Set INPUT_DIR and OUTPUT_DIR for your environment\n",
    "3. **Run Pipeline**: Execute the build cells\n",
    "4. **Optional**: Convert to Delta tables for SQL Endpoint access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d0ab8",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "<span style=\"color:red\">pip install is only required on first run. Uncomment and run once, then re-comment.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e80275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install /lakehouse/default/Files/usf_fabric_monitoring-0.3.0-py3-none-any.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f6749",
   "metadata": {},
   "source": [
    "## Setup Local Path (For Local Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffb516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Running in Fabric or package already installed\n"
     ]
    }
   ],
   "source": [
    "# SETUP LOCAL PATH (For Local Development)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to sys.path to allow importing the local package\n",
    "# This is necessary when running locally without installing the package\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# Check if we are in notebooks directory\n",
    "if current_dir.name == \"notebooks\":\n",
    "    src_path = current_dir.parent / \"src\"\n",
    "else:\n",
    "    # Assume we are in project root\n",
    "    src_path = current_dir / \"src\"\n",
    "\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"‚úÖ Added {src_path} to sys.path (local development mode)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Running in Fabric or package already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b118a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modules reloaded\n"
     ]
    }
   ],
   "source": [
    "# Force reload of modules to pick up code changes\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb\n",
    "importlib.reload(ssb)\n",
    "print(\"‚úÖ Modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1736ec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usf_fabric_monitoring version: 0.3.7\n",
      "Resolved output dir example: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "‚úÖ StarSchemaBuilder module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Package / environment verification (safe: no Azure/API imports)\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "import importlib\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "try:\n",
    "    pkg_version = getattr(usf_fabric_monitoring, \"__version__\", None) or version(\"usf_fabric_monitoring\")\n",
    "except PackageNotFoundError:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "print(f\"usf_fabric_monitoring version: {pkg_version}\")\n",
    "print(f\"Resolved output dir example: {resolve_path('exports/star_schema')}\")\n",
    "\n",
    "# Check star schema builder availability\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.star_schema_builder import StarSchemaBuilder, ALL_STAR_SCHEMA_DDLS\n",
    "    print(\"‚úÖ StarSchemaBuilder module loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import StarSchemaBuilder: {e}\")\n",
    "    print(\"   Make sure you have version 0.3.0+ installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435a0cf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061d87ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAR SCHEMA BUILDER CONFIGURATION\n",
      "============================================================\n",
      "Input Directory:       /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis\n",
      "Output Directory:      /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema\n",
      "Mode:                  Incremental\n",
      "Write to Delta Tables: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# ============================================================================\n",
    "\n",
    "# Input: Where Monitor Hub pipeline outputs are stored (CSV files with Smart Merge data)\n",
    "INPUT_DIR = resolve_path(\"exports/monitor_hub_analysis\")\n",
    "\n",
    "# Output: Where star schema tables will be written\n",
    "OUTPUT_DIR = resolve_path(\"exports/star_schema\")\n",
    "\n",
    "# Processing options\n",
    "INCREMENTAL_LOAD = True  # Set to False for full refresh (rebuilds all tables)\n",
    "WRITE_TO_DELTA_TABLES = False  # Set to True in Fabric to create SQL Endpoint tables\n",
    "\n",
    "# ============================================================================\n",
    "# Display configuration\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA BUILDER CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input Directory:       {INPUT_DIR}\")\n",
    "print(f\"Output Directory:      {OUTPUT_DIR}\")\n",
    "print(f\"Mode:                  {'Incremental' if INCREMENTAL_LOAD else 'Full Refresh'}\")\n",
    "print(f\"Write to Delta Tables: {WRITE_TO_DELTA_TABLES}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8267b22",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39150246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found parquet source: activities_20251217_143508.parquet\n",
      "   Columns: 39 (includes end_time for failure tracking)\n",
      "   Records: 1,286,374\n",
      "   Status distribution:\n",
      "     - Succeeded: 1,280,156\n",
      "     - Failed: 6,218\n",
      "   Columns: 39 (includes end_time for failure tracking)\n",
      "   Records: 1,286,374\n",
      "   Status distribution:\n",
      "     - Succeeded: 1,280,156\n",
      "     - Failed: 6,218\n"
     ]
    }
   ],
   "source": [
    "# Preview source data (optional - for verification only)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "input_path = Path(INPUT_DIR)\n",
    "\n",
    "# Check for parquet files first (preferred - has complete data including end_time for failures)\n",
    "parquet_dir = input_path / \"parquet\"\n",
    "parquet_files = sorted(parquet_dir.glob(\"activities_*.parquet\"), reverse=True) if parquet_dir.exists() else []\n",
    "\n",
    "if parquet_files:\n",
    "    print(f\"‚úÖ Found parquet source: {parquet_files[0].name}\")\n",
    "    preview_df = pd.read_parquet(parquet_files[0])\n",
    "    print(f\"   Columns: {len(preview_df.columns)} (includes end_time for failure tracking)\")\n",
    "else:\n",
    "    # Fallback to CSV\n",
    "    csv_files = sorted(input_path.glob(\"activities_master_*.csv\"), reverse=True)\n",
    "    if csv_files:\n",
    "        print(f\"‚ö†Ô∏è Using CSV source: {csv_files[0].name}\")\n",
    "        preview_df = pd.read_csv(csv_files[0], low_memory=False)\n",
    "        print(f\"   Columns: {len(preview_df.columns)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No activities files found in {INPUT_DIR}\")\n",
    "\n",
    "print(f\"   Records: {len(preview_df):,}\")\n",
    "\n",
    "# Show status distribution\n",
    "if 'status' in preview_df.columns:\n",
    "    print(f\"   Status distribution:\")\n",
    "    for status, count in preview_df['status'].value_counts().items():\n",
    "        print(f\"     - {status}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eba7f3",
   "metadata": {},
   "source": [
    "## Build Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING STAR SCHEMA\n",
      "============================================================\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Starting Star Schema Build\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Mode: Full Refresh\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Input activities: 1286374\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Pre-processing: Enriching activities with workspace names...\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Starting Star Schema Build\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Mode: Full Refresh\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Input activities: 1286374\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | ============================================================\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Pre-processing: Enriching activities with workspace names...\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Loaded 512 workspace names from /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis/parquet/workspaces_20251217_143508.parquet\n",
      "2025-12-17 14:50:56 | INFO | star_schema_builder | Loaded 512 workspace names from /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis/parquet/workspaces_20251217_143508.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Enriched 1286374 of 1286374 activities with workspace names\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 1: Building reference dimensions...\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_date with 456 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_date.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Enriched 1286374 of 1286374 activities with workspace names\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 1: Building reference dimensions...\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_date with 456 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_date.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_time with 96 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_time.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_activity_type with 70 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_activity_type.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_status with 8 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_status.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 2: Building slowly changing dimensions...\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_time with 96 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_time.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_activity_type with 70 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_activity_type.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_status with 8 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_status.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 2: Building slowly changing dimensions...\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_workspace with 512 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_workspace.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_workspace with 512 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_workspace.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_item with 3741 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_item.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_item with 3741 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_item.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_user with 1505 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_user.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 3: Building fact activity table...\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Saved dim_user with 1505 records to /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/star_schema/dim_user.parquet\n",
      "2025-12-17 14:50:57 | INFO | star_schema_builder | Step 3: Building fact activity table...\n"
     ]
    }
   ],
   "source": [
    "# Build Star Schema using the recommended function\n",
    "# This function:\n",
    "# 1. Loads from parquet source (complete data with 28 columns including end_time)\n",
    "# 2. Uses end_time fallback for failed records that have NULL start_time\n",
    "# 3. Enriches with workspace names automatically\n",
    "# 4. Handles all dimension lookups correctly\n",
    "\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb_module\n",
    "importlib.reload(ssb_module)\n",
    "build_star_schema_from_pipeline_output = ssb_module.build_star_schema_from_pipeline_output\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build star schema\n",
    "print(\"=\" * 60)\n",
    "print(\"BUILDING STAR SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "start_time = datetime.now()\n",
    "\n",
    "# NOTE: Using incremental=False to ensure clean rebuild\n",
    "# Set incremental=True only for production daily loads with new data\n",
    "tables = build_star_schema_from_pipeline_output(\n",
    "    pipeline_output_dir=str(INPUT_DIR),\n",
    "    output_directory=str(OUTPUT_DIR),\n",
    "    incremental=False  # Full rebuild - prevents duplicate data issues\n",
    ")\n",
    "\n",
    "duration = (datetime.now() - start_time).total_seconds()\n",
    "print(f\"\\n‚úÖ Star Schema build completed in {duration:.2f} seconds!\")\n",
    "print(f\"üìÅ Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c04cc",
   "metadata": {},
   "source": [
    "## Convert to Delta Tables (Fabric Only)\n",
    "Run this cell only in Microsoft Fabric to create Delta tables accessible via SQL Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_TO_DELTA_TABLES:\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        print(\"Converting Parquet files to Delta tables...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert each parquet to Delta table\n",
    "        for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "            table_name = parquet_file.stem  # e.g., \"dim_date\", \"fact_activity\"\n",
    "            \n",
    "            try:\n",
    "                df = spark.read.parquet(str(parquet_file))\n",
    "                \n",
    "                # Write as Delta table (overwrite mode)\n",
    "                df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "                \n",
    "                print(f\"   ‚úÖ {table_name}: {df.count():,} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {table_name}: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(\"‚úÖ Delta tables created successfully!\")\n",
    "        print(\"   Tables are now available in the SQL Endpoint.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PySpark not available. Delta table creation skipped.\")\n",
    "        print(\"   This feature is only available in Microsoft Fabric.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Delta table creation skipped (WRITE_TO_DELTA_TABLES=False)\")\n",
    "    print(\"   Set WRITE_TO_DELTA_TABLES=True in Fabric to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6a547",
   "metadata": {},
   "source": [
    "## Validate Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load tables from parquet files (ensures we have fresh data)\n",
    "tables = {}\n",
    "for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "    table_name = parquet_file.stem\n",
    "    tables[table_name] = pd.read_parquet(parquet_file)\n",
    "    print(f\"{table_name}: {len(tables[table_name]):,} records\")\n",
    "\n",
    "# Quick data quality check\n",
    "fact = tables.get('fact_activity')\n",
    "if fact is not None:\n",
    "    print(f\"\\nüìä Fact Activity Summary:\")\n",
    "    print(f\"   Total records: {len(fact):,}\")\n",
    "    print(f\"   Total activities (record_count sum): {fact['record_count'].sum():,}\")\n",
    "    print(f\"   Failed activities: {fact['is_failed'].sum():,}\")\n",
    "    print(f\"   Records with date_sk: {fact['date_sk'].notna().sum():,}\")\n",
    "\n",
    "# Check FK integrity\n",
    "print(\"\\nüîó Foreign Key Validation:\")\n",
    "\n",
    "fk_checks = [\n",
    "    ('fact_activity', 'workspace_sk', 'dim_workspace', 'workspace_sk'),\n",
    "    ('fact_activity', 'item_sk', 'dim_item', 'item_sk'),\n",
    "    ('fact_activity', 'user_sk', 'dim_user', 'user_sk'),\n",
    "    ('fact_activity', 'date_sk', 'dim_date', 'date_sk'),\n",
    "    ('fact_activity', 'time_sk', 'dim_time', 'time_sk'),\n",
    "    ('fact_activity', 'activity_type_sk', 'dim_activity_type', 'activity_type_sk'),\n",
    "    ('fact_activity', 'status_sk', 'dim_status', 'status_sk'),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for fact_table, fact_col, dim_table, dim_col in fk_checks:\n",
    "    if fact_table in tables and dim_table in tables:\n",
    "        fact_vals = set(tables[fact_table][fact_col].dropna().unique())\n",
    "        dim_vals = set(tables[dim_table][dim_col].unique())\n",
    "        orphans = fact_vals - dim_vals\n",
    "        status = '‚úÖ PASS' if len(orphans) == 0 else f'‚ùå {len(orphans)} orphans'\n",
    "        if len(orphans) > 0:\n",
    "            all_passed = False\n",
    "        print(f\"   {fact_col}: {status}\")\n",
    "\n",
    "print(\"\\n\" + (\"‚úÖ All FK validations passed!\" if all_passed else \"‚ö†Ô∏è Some FK validations failed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd3611",
   "metadata": {},
   "source": [
    "## Sample Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Active Workspaces\n",
    "# NOTE: Use record_count (not activity_id) as many activities have NULL activity_id\n",
    "print(\"üìä Top 10 Most Active Workspaces\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Aggregate by workspace_sk - use record_count for accurate totals\n",
    "    ws_stats = fact.groupby('workspace_sk').agg(\n",
    "        activity_count=('record_count', 'sum'),  # Sum record_count, not count activity_id\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Join with dimension for names\n",
    "    ws_stats = ws_stats.merge(\n",
    "        dim_ws[['workspace_sk', 'workspace_name', 'environment']], \n",
    "        on='workspace_sk', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Sort and display top 10\n",
    "    ws_stats = ws_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    ws_stats['duration_hrs'] = round(ws_stats['total_duration'] / 3600, 2)\n",
    "    ws_stats['failure_rate'] = round(100 * ws_stats['failed'] / ws_stats['activity_count'], 2)\n",
    "    \n",
    "    print(f\"{'Workspace':<45} | {'Env':<7} | {'Activities':>12} | {'Hours':>10} | {'Failed':>8} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in ws_stats.iterrows():\n",
    "        ws_name = str(row['workspace_name'])[:44] if row['workspace_name'] else 'Unknown'\n",
    "        env = str(row['environment'])[:7] if row['environment'] else 'N/A'\n",
    "        print(f\"{ws_name:<45} | {env:<7} | {int(row['activity_count']):>12,} | {row['duration_hrs']:>10.2f} | {int(row['failed']):>8,} | {row['failure_rate']:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity by Type - use record_count for accurate totals\n",
    "# Shows BOTH top 15 by volume AND all activity types with failures\n",
    "print(\"üìä Activity Count by Type (Top 15 by Volume)\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_activity_type' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_type = tables['dim_activity_type']\n",
    "    \n",
    "    # Aggregate by activity_type_sk first\n",
    "    type_stats = fact.groupby('activity_type_sk').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Join with dimension for type details\n",
    "    type_stats = type_stats.merge(\n",
    "        dim_type[['activity_type_sk', 'activity_type', 'activity_category']], \n",
    "        on='activity_type_sk'\n",
    "    )\n",
    "    type_stats['success_rate'] = round(100 * (1 - type_stats['failed'] / type_stats['activity_count']), 2)\n",
    "    \n",
    "    # Top 15 by volume\n",
    "    top_15 = type_stats.sort_values('activity_count', ascending=False).head(15)\n",
    "    \n",
    "    print(f\"{'Category':<18} | {'Activity Type':<30} | {'Count':>12} | {'Failed':>8} | {'Success %':>9}\")\n",
    "    print(\"-\" * 85)\n",
    "    for _, row in top_15.iterrows():\n",
    "        print(f\"{row['activity_category']:<18} | {row['activity_type']:<30} | {int(row['activity_count']):>12,} | {int(row['failed']):>8,} | {row['success_rate']:>8.2f}%\")\n",
    "    \n",
    "    # Show ALL activity types with failures (important for job history activities)\n",
    "    types_with_failures = type_stats[type_stats['failed'] > 0].sort_values('failed', ascending=False)\n",
    "    if len(types_with_failures) > 0:\n",
    "        print()\n",
    "        print(\"=\" * 85)\n",
    "        print(\"üìä Activity Types with Failures (All)\")\n",
    "        print(\"=\" * 85)\n",
    "        print(f\"{'Category':<18} | {'Activity Type':<30} | {'Count':>12} | {'Failed':>8} | {'Success %':>9}\")\n",
    "        print(\"-\" * 85)\n",
    "        for _, row in types_with_failures.iterrows():\n",
    "            print(f\"{row['activity_category']:<18} | {row['activity_type']:<30} | {int(row['activity_count']):>12,} | {int(row['failed']):>8,} | {row['success_rate']:>8.2f}%\")\n",
    "        print(\"-\" * 85)\n",
    "        print(f\"{'TOTAL':<18} | {'':<30} | {int(types_with_failures['activity_count'].sum()):>12,} | {int(types_with_failures['failed'].sum()):>8,} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Activity Trend\n",
    "print(\"üìä Daily Activity Trend (Last 14 Days)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_daily_metrics' in tables and 'dim_date' in tables:\n",
    "    fact_daily = tables['fact_daily_metrics']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Aggregate across workspaces\n",
    "    daily_totals = fact_daily.groupby('date_sk').agg({\n",
    "        'total_activities': 'sum',\n",
    "        'unique_users': 'sum',\n",
    "        'failed_activities': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Join with date dimension\n",
    "    daily_totals = daily_totals.merge(\n",
    "        dim_date[['date_sk', 'full_date', 'day_of_week_name']], \n",
    "        on='date_sk'\n",
    "    ).sort_values('full_date', ascending=False).head(14)\n",
    "    \n",
    "    for _, row in daily_totals.iterrows():\n",
    "        day = row['day_of_week_name'][:3]\n",
    "        print(f\"   {row['full_date']} ({day}) | {int(row['total_activities']):>8,} activities | {int(row['unique_users']):>4} users | {int(row['failed_activities']):>3} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Analysis (Smart Merge Data)\n",
    "print(\"üìä Failure Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_status' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_status = tables['dim_status']\n",
    "    \n",
    "    # Overall failure stats\n",
    "    total = len(fact)\n",
    "    failed = (fact['is_failed'] == 1).sum()\n",
    "    success_rate = ((total - failed) / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    print(f\"   Total Activities:    {total:,}\")\n",
    "    print(f\"   Failed Activities:   {failed:,}\")\n",
    "    print(f\"   Success Rate:        {success_rate:.2f}%\")\n",
    "    \n",
    "    # Failures by status\n",
    "    print(f\"\\n   Failures by Status:\")\n",
    "    merged = fact[fact['is_failed'] == 1].merge(\n",
    "        dim_status[['status_sk', 'status_code']], on='status_sk'\n",
    "    )\n",
    "    if len(merged) > 0:\n",
    "        for status, count in merged['status_code'].value_counts().items():\n",
    "            print(f\"     - {status}: {count:,}\")\n",
    "    \n",
    "    # Failures by activity type (if available)\n",
    "    if 'dim_activity_type' in tables:\n",
    "        dim_type = tables['dim_activity_type']\n",
    "        merged = fact[fact['is_failed'] == 1].merge(\n",
    "            dim_type[['activity_type_sk', 'activity_type']], on='activity_type_sk'\n",
    "        )\n",
    "        if len(merged) > 0:\n",
    "            print(f\"\\n   Top Failed Activity Types:\")\n",
    "            for act_type, count in merged['activity_type'].value_counts().head(5).items():\n",
    "                print(f\"     - {act_type}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a6602",
   "metadata": {},
   "source": [
    "## Additional Sample Analytical Queries\n",
    "\n",
    "The following cells demonstrate various analytical capabilities of the star schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f15176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Frequently Used Items\n",
    "print(\"üìä Top 10 Most Frequently Used Items\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    \n",
    "    # Use record_count for accurate totals\n",
    "    item_stats = fact.groupby('item_sk').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    item_stats = item_stats.merge(\n",
    "        dim_item[['item_sk', 'item_name', 'item_type', 'item_category']], \n",
    "        on='item_sk'\n",
    "    )\n",
    "    \n",
    "    top_items = item_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'Item Name':<40} | {'Type':<18} | {'Category':<12} | {'Activities':>12} | {'Users':>6}\")\n",
    "    print(\"-\" * 110)\n",
    "    for _, row in top_items.iterrows():\n",
    "        item_name = str(row['item_name'])[:39] if row['item_name'] else 'Unknown'\n",
    "        item_type = str(row['item_type'])[:17] if row['item_type'] else 'N/A'\n",
    "        category = str(row['item_category'])[:11] if row['item_category'] else 'N/A'\n",
    "        print(f\"{item_name:<40} | {item_type:<18} | {category:<12} | {int(row['activity_count']):>12,} | {int(row['unique_users']):>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df35c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity by Hour of Day\n",
    "# Shows when activities occur during the day\n",
    "print(\"üìä Activity by Hour of Day\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_time' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_time = tables['dim_time']\n",
    "    \n",
    "    # Get hourly stats from fact table (use record_count for accurate totals)\n",
    "    hourly_stats = fact.groupby('time_sk').agg(\n",
    "        activity_count=('record_count', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Join with time dimension (using correct column names: hour_24, time_period)\n",
    "    hourly_stats = hourly_stats.merge(\n",
    "        dim_time[['time_sk', 'hour_24', 'time_period', 'is_business_hours']], \n",
    "        on='time_sk'\n",
    "    )\n",
    "    \n",
    "    # Aggregate by hour\n",
    "    hourly_summary = hourly_stats.groupby(['hour_24', 'time_period']).agg(\n",
    "        total_activities=('activity_count', 'sum')\n",
    "    ).reset_index().sort_values('hour_24')\n",
    "    \n",
    "    print(f\"{'Hour':<8} | {'Period':<12} | {'Activities':>14} | {'Chart'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in hourly_summary.iterrows():\n",
    "        bar = \"‚ñà\" * int(row['total_activities'] / hourly_summary['total_activities'].max() * 30)\n",
    "        print(f\"{row['hour_24']:02d}:00    | {row['time_period']:<12} | {int(row['total_activities']):>14,} | {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Active Users\n",
    "print(\"üìä Top 10 Most Active Users\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_user' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_user = tables['dim_user']\n",
    "    \n",
    "    # Use record_count for accurate totals\n",
    "    user_stats = fact.groupby('user_sk').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_workspaces=('workspace_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Join with user dimension (correct column names: user_principal_name, domain)\n",
    "    user_stats = user_stats.merge(\n",
    "        dim_user[['user_sk', 'user_principal_name', 'domain', 'user_type']], \n",
    "        on='user_sk'\n",
    "    )\n",
    "    \n",
    "    top_users = user_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'User':<35} | {'Domain':<15} | {'Type':<8} | {'Activities':>12} | {'Items':>6} | {'WS':>4}\")\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in top_users.iterrows():\n",
    "        user = str(row['user_principal_name'])[:34] if row['user_principal_name'] else 'Unknown'\n",
    "        domain = str(row['domain'])[:14] if row['domain'] else 'N/A'\n",
    "        user_type = str(row['user_type'])[:7] if row['user_type'] else 'N/A'\n",
    "        print(f\"{user:<35} | {domain:<15} | {user_type:<8} | {int(row['activity_count']):>12,} | {int(row['unique_items']):>6} | {int(row['unique_workspaces']):>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30558aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity Volume by Day of Week\n",
    "print(\"üìä Activity Volume by Day of Week\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_date' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Use record_count for accurate totals\n",
    "    daily_stats = fact.groupby('date_sk').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    daily_stats = daily_stats.merge(\n",
    "        dim_date[['date_sk', 'day_of_week_name', 'day_of_week', 'is_weekend']], \n",
    "        on='date_sk'\n",
    "    )\n",
    "    \n",
    "    dow_stats = daily_stats.groupby(['day_of_week', 'day_of_week_name', 'is_weekend']).agg({\n",
    "        'activity_count': 'sum',\n",
    "        'unique_users': 'mean',\n",
    "        'failed': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dow_stats = dow_stats.sort_values('day_of_week')\n",
    "    \n",
    "    print(f\"{'Day':<12} | {'Weekend':>8} | {'Activities':>14} | {'Avg Users':>10} | {'Failed':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in dow_stats.iterrows():\n",
    "        day_name = str(row['day_of_week_name'])\n",
    "        weekend = \"Yes\" if row['is_weekend'] else \"No\"\n",
    "        print(f\"{day_name:<12} | {weekend:>8} | {int(row['activity_count']):>14,} | {row['unique_users']:>10.1f} | {int(row['failed']):>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef725f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Comparison (DEV vs TEST vs UAT vs PRD)\n",
    "print(\"üìä Environment Comparison\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Get workspace environments\n",
    "    ws_env = dim_ws[['workspace_sk', 'environment']].drop_duplicates()\n",
    "    fact_with_env = fact.merge(ws_env, on='workspace_sk', how='left')\n",
    "    \n",
    "    # Use record_count for accurate totals\n",
    "    env_stats = fact_with_env.groupby('environment').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_workspaces=('workspace_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    env_stats['duration_hrs'] = round(env_stats['total_duration'] / 3600, 2)\n",
    "    env_stats['failure_rate'] = round(100 * env_stats['failed'] / env_stats['activity_count'], 2)\n",
    "    env_stats = env_stats.sort_values('activity_count', ascending=False)\n",
    "    \n",
    "    print(f\"{'Environment':<12} | {'Activities':>14} | {'Users':>6} | {'Items':>6} | {'WS':>4} | {'Hours':>10} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 95)\n",
    "    for _, row in env_stats.iterrows():\n",
    "        env = str(row['environment']) if row['environment'] else 'Unknown'\n",
    "        print(f\"{env:<12} | {int(row['activity_count']):>14,} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['unique_workspaces']):>4} | {row['duration_hrs']:>10.2f} | {row['failure_rate']:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity by Item Category\n",
    "print(\"üìä Activity by Item Category\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    \n",
    "    item_cat = dim_item[['item_sk', 'item_category']].drop_duplicates()\n",
    "    fact_with_cat = fact.merge(item_cat, on='item_sk', how='left')\n",
    "    \n",
    "    # Use record_count for accurate totals\n",
    "    cat_stats = fact_with_cat.groupby('item_category').agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    cat_stats['failure_rate'] = round(100 * cat_stats['failed'] / cat_stats['activity_count'], 2)\n",
    "    cat_stats = cat_stats.sort_values('activity_count', ascending=False)\n",
    "    \n",
    "    print(f\"{'Category':<18} | {'Activities':>14} | {'Items':>6} | {'Users':>6} | {'Failed':>8} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in cat_stats.iterrows():\n",
    "        cat = str(row['item_category'])[:17] if row['item_category'] else 'Unknown'\n",
    "        print(f\"{cat:<18} | {int(row['activity_count']):>14,} | {int(row['unique_items']):>6} | {int(row['unique_users']):>6} | {int(row['failed']):>8,} | {row['failure_rate']:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-Running Activities Analysis\n",
    "print(\"üìä Long-Running Activities Analysis (>1 hour)\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Filter for long-running activities (>1 hour = 3600 seconds)\n",
    "    long_running = fact[fact['is_long_running'] == 1].copy() if 'is_long_running' in fact.columns else fact[fact['duration_seconds'] > 3600].copy()\n",
    "    \n",
    "    if len(long_running) > 0:\n",
    "        print(f\"Total long-running activities (>1 hr): {len(long_running):,}\")\n",
    "        print()\n",
    "        \n",
    "        # Enrich with names\n",
    "        long_running = long_running.merge(\n",
    "            dim_item[['item_sk', 'item_name', 'item_type']], on='item_sk', how='left'\n",
    "        ).merge(\n",
    "            dim_ws[['workspace_sk', 'workspace_name']], on='workspace_sk', how='left'\n",
    "        )\n",
    "        \n",
    "        # Top 10 longest\n",
    "        top_longest = long_running.nlargest(10, 'duration_seconds')\n",
    "        \n",
    "        print(f\"{'Item Name':<35} | {'Type':<15} | {'Workspace':<25} | {'Duration':>12}\")\n",
    "        print(\"-\" * 100)\n",
    "        for _, row in top_longest.iterrows():\n",
    "            item_name = str(row['item_name'])[:34] if row['item_name'] else 'Unknown'\n",
    "            item_type = str(row['item_type'])[:14] if row['item_type'] else 'N/A'\n",
    "            ws_name = str(row['workspace_name'])[:24] if row['workspace_name'] else 'Unknown'\n",
    "            duration_hrs = round(row['duration_seconds'] / 3600, 2)\n",
    "            print(f\"{item_name:<35} | {item_type:<15} | {ws_name:<25} | {duration_hrs:>10} hrs\")\n",
    "    else:\n",
    "        print(\"No long-running activities found (>1 hour)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Workspace Usage Patterns (Workspaces shared by multiple users)\n",
    "print(\"üìä Cross-Workspace Usage Patterns\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Calculate collaboration metrics per workspace\n",
    "    # Use record_count sum instead of activity_id count (activity_id is NULL for granular operations)\n",
    "    ws_collab = fact.groupby('workspace_sk').agg(\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        activity_count=('record_count', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    ws_collab = ws_collab.merge(\n",
    "        dim_ws[['workspace_sk', 'workspace_name', 'environment']], on='workspace_sk'\n",
    "    )\n",
    "    \n",
    "    # Sort by user count (collaboration indicator)\n",
    "    ws_collab = ws_collab.sort_values('unique_users', ascending=False).head(10)\n",
    "    ws_collab['activities_per_user'] = round(ws_collab['activity_count'] / ws_collab['unique_users'], 1)\n",
    "    \n",
    "    print(f\"{'Workspace':<40} | {'Env':<7} | {'Users':>6} | {'Items':>6} | {'Activities':>10} | {'Act/User':>8}\")\n",
    "    print(\"-\" * 95)\n",
    "    for _, row in ws_collab.iterrows():\n",
    "        ws_name = str(row['workspace_name'])[:39] if row['workspace_name'] else 'Unknown'\n",
    "        env = str(row['environment'])[:6] if row['environment'] else 'N/A'\n",
    "        print(f\"{ws_name:<40} | {env:<7} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['activity_count']):>10,} | {row['activities_per_user']:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Trend Analysis\n",
    "print(\"üìä Monthly Activity Trend\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_date' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Join with date dimension (use month_number not month)\n",
    "    fact_dated = fact.merge(\n",
    "        dim_date[['date_sk', 'year', 'month_number', 'month_name']], on='date_sk'\n",
    "    )\n",
    "    \n",
    "    # Use record_count sum instead of activity_id count\n",
    "    monthly_stats = fact_dated.groupby(['year', 'month_number', 'month_name']).agg(\n",
    "        activity_count=('record_count', 'sum'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    monthly_stats = monthly_stats.sort_values(['year', 'month_number'], ascending=False).head(12)\n",
    "    monthly_stats['failure_rate'] = round(100 * monthly_stats['failed'] / monthly_stats['activity_count'], 2)\n",
    "    \n",
    "    print(f\"{'Month':<15} | {'Activities':>12} | {'Users':>6} | {'Items':>6} | {'Failed':>8} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in monthly_stats.iterrows():\n",
    "        month_label = f\"{row['month_name'][:3]} {int(row['year'])}\"\n",
    "        print(f\"{month_label:<15} | {int(row['activity_count']):>12,} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['failed']):>8,} | {row['failure_rate']:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22a685",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The star schema has been built and is ready for:\n",
    "\n",
    "1. **SQL Queries** - Query the parquet files directly or use Delta tables via SQL Endpoint\n",
    "2. **Semantic Model** - Create a Direct Lake model pointing to these tables\n",
    "3. **Power BI Reports** - Build monitoring dashboards using the semantic model\n",
    "\n",
    "### Scheduled Refresh\n",
    "To automate the star schema build:\n",
    "1. Schedule this notebook to run daily after the Monitor Hub pipeline\n",
    "2. Or use a Fabric Data Pipeline to orchestrate both steps\n",
    "\n",
    "### Table Relationships (for Semantic Model)\n",
    "```\n",
    "fact_activity[date_sk] ‚Üí dim_date[date_sk]\n",
    "fact_activity[time_sk] ‚Üí dim_time[time_sk]\n",
    "fact_activity[workspace_sk] ‚Üí dim_workspace[workspace_sk]\n",
    "fact_activity[item_sk] ‚Üí dim_item[item_sk]\n",
    "fact_activity[user_sk] ‚Üí dim_user[user_sk]\n",
    "fact_activity[activity_type_sk] ‚Üí dim_activity_type[activity_type_sk]\n",
    "fact_activity[status_sk] ‚Üí dim_status[status_sk]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
