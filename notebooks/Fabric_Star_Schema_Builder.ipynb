{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cec7b77",
   "metadata": {},
   "source": [
    "# Star Schema Builder for Fabric Monitoring Analytics\n",
    "\n",
    "## Overview\n",
    "This notebook transforms raw Monitor Hub activity data into a Kimball-style star schema suitable for SQL queries, semantic models, and Power BI reports.\n",
    "\n",
    "It is designed to work in both:\n",
    "- **Microsoft Fabric notebooks** (paths auto-resolve under `/lakehouse/default/Files/`), and\n",
    "- **Local dev** (writes under `exports/` by default).\n",
    "\n",
    "## Star Schema Tables\n",
    "- **Dimensions**: dim_date, dim_time, dim_workspace, dim_item, dim_user, dim_activity_type, dim_status\n",
    "- **Facts**: fact_activity, fact_daily_metrics\n",
    "\n",
    "## Key Features\n",
    "- Incremental loading with high-water mark tracking\n",
    "- SCD Type 2 support for slowly changing dimensions\n",
    "- Automatic surrogate key generation\n",
    "- Pre-aggregated daily metrics for fast dashboards\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package** (first run only): Uncomment and run the pip install cell\n",
    "2. **Configure Paths**: Set INPUT_DIR and OUTPUT_DIR for your environment\n",
    "3. **Run Pipeline**: Execute the build cells\n",
    "4. **Optional**: Convert to Delta tables for SQL Endpoint access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d0ab8",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "<span style=\"color:red\">pip install is only required on first run. Uncomment and run once, then re-comment.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e80275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install /lakehouse/default/Files/usf_fabric_monitoring-0.3.0-py3-none-any.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f6749",
   "metadata": {},
   "source": [
    "## Setup Local Path (For Local Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP LOCAL PATH (For Local Development)\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to sys.path to allow importing the local package\n",
    "# This is necessary when running locally without installing the package\n",
    "current_dir = Path(os.getcwd())\n",
    "\n",
    "# Check if we are in notebooks directory\n",
    "if current_dir.name == \"notebooks\":\n",
    "    src_path = current_dir.parent / \"src\"\n",
    "else:\n",
    "    # Assume we are in project root\n",
    "    src_path = current_dir / \"src\"\n",
    "\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"‚úÖ Added {src_path} to sys.path (local development mode)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Running in Fabric or package already installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b118a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload of modules to pick up code changes\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb\n",
    "importlib.reload(ssb)\n",
    "print(\"‚úÖ Modules reloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package / environment verification (safe: no Azure/API imports)\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "import importlib\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "try:\n",
    "    pkg_version = getattr(usf_fabric_monitoring, \"__version__\", None) or version(\"usf_fabric_monitoring\")\n",
    "except PackageNotFoundError:\n",
    "    pkg_version = \"unknown\"\n",
    "\n",
    "print(f\"usf_fabric_monitoring version: {pkg_version}\")\n",
    "print(f\"Resolved output dir example: {resolve_path('exports/star_schema')}\")\n",
    "\n",
    "# Check star schema builder availability\n",
    "try:\n",
    "    from usf_fabric_monitoring.core.star_schema_builder import StarSchemaBuilder, ALL_STAR_SCHEMA_DDLS\n",
    "    print(\"‚úÖ StarSchemaBuilder module loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import StarSchemaBuilder: {e}\")\n",
    "    print(\"   Make sure you have version 0.3.0+ installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435a0cf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Update these values for your environment\n",
    "# ============================================================================\n",
    "\n",
    "# Input: Where Monitor Hub pipeline outputs are stored (CSV files with Smart Merge data)\n",
    "INPUT_DIR = resolve_path(\"exports/monitor_hub_analysis\")\n",
    "\n",
    "# Output: Where star schema tables will be written\n",
    "OUTPUT_DIR = resolve_path(\"exports/star_schema\")\n",
    "\n",
    "# Processing options\n",
    "INCREMENTAL_LOAD = True  # Set to False for full refresh (rebuilds all tables)\n",
    "WRITE_TO_DELTA_TABLES = False  # Set to True in Fabric to create SQL Endpoint tables\n",
    "\n",
    "# ============================================================================\n",
    "# Display configuration\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA BUILDER CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input Directory:       {INPUT_DIR}\")\n",
    "print(f\"Output Directory:      {OUTPUT_DIR}\")\n",
    "print(f\"Mode:                  {'Incremental' if INCREMENTAL_LOAD else 'Full Refresh'}\")\n",
    "print(f\"Write to Delta Tables: {WRITE_TO_DELTA_TABLES}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8267b22",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39150246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Find activities_master CSV files (contains Smart Merge enriched data with accurate failure status)\n",
    "input_path = Path(INPUT_DIR)\n",
    "activities_files = sorted(input_path.glob(\"activities_master_*.csv\"), reverse=True)\n",
    "\n",
    "if not activities_files:\n",
    "    # Fallback to parquet if no CSV\n",
    "    parquet_dir = input_path / \"parquet\"\n",
    "    parquet_files = sorted(parquet_dir.glob(\"activities_*.parquet\"), reverse=True) if parquet_dir.exists() else []\n",
    "    if parquet_files:\n",
    "        print(f\"‚ö†Ô∏è No CSV files found, falling back to parquet: {parquet_files[0].name}\")\n",
    "        activities_df = pd.read_parquet(parquet_files[0])\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No activities files found in {INPUT_DIR}\\n\"\n",
    "            \"Run the Monitor Hub pipeline first: make monitor-hub\"\n",
    "        )\n",
    "else:\n",
    "    # Show available files\n",
    "    print(\"Available activity files:\")\n",
    "    for i, f in enumerate(activities_files[:5]):\n",
    "        df_check = pd.read_csv(f, usecols=['status'], nrows=100000)\n",
    "        failed_sample = (df_check['status'] == 'Failed').sum()\n",
    "        print(f\"  {i+1}. {f.name} - {'Has failures' if failed_sample > 0 else 'No failures in sample'}\")\n",
    "    \n",
    "    # Try to find a file with failures, otherwise use latest\n",
    "    selected_file = None\n",
    "    for f in activities_files:\n",
    "        df_check = pd.read_csv(f, usecols=['status'], low_memory=False)\n",
    "        if (df_check['status'] == 'Failed').sum() > 0:\n",
    "            selected_file = f\n",
    "            print(f\"\\n‚úÖ Selected file with Smart Merge failure data: {f.name}\")\n",
    "            break\n",
    "    \n",
    "    if selected_file is None:\n",
    "        selected_file = activities_files[0]\n",
    "        print(f\"\\n‚ö†Ô∏è No file with failures found, using latest: {selected_file.name}\")\n",
    "    \n",
    "    activities_df = pd.read_csv(selected_file, low_memory=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(activities_df):,} activity records\")\n",
    "print(f\"   Columns: {len(activities_df.columns)} total\")\n",
    "\n",
    "# Show status distribution (key for Smart Merge validation)\n",
    "if 'status' in activities_df.columns:\n",
    "    status_counts = activities_df['status'].value_counts()\n",
    "    print(f\"   Status distribution:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"     - {status}: {count:,}\")\n",
    "\n",
    "# Show date range\n",
    "time_col = 'start_time' if 'start_time' in activities_df.columns else 'StartTimeUtc'\n",
    "if time_col in activities_df.columns:\n",
    "    activities_df[time_col] = pd.to_datetime(activities_df[time_col], errors='coerce')\n",
    "    print(f\"   Date range: {activities_df[time_col].min()} to {activities_df[time_col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eba7f3",
   "metadata": {},
   "source": [
    "## Build Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reloaded module\n",
    "from usf_fabric_monitoring.core.star_schema_builder import StarSchemaBuilder\n",
    "import importlib\n",
    "import usf_fabric_monitoring.core.star_schema_builder as ssb_module\n",
    "importlib.reload(ssb_module)\n",
    "StarSchemaBuilder = ssb_module.StarSchemaBuilder\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build star schema\n",
    "print(\"=\" * 60)\n",
    "print(\"BUILDING STAR SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "start_time = datetime.now()\n",
    "\n",
    "builder = StarSchemaBuilder(output_directory=OUTPUT_DIR)\n",
    "results = builder.build_complete_schema(\n",
    "    activities=activities_df.to_dict(orient=\"records\"),\n",
    "    incremental=INCREMENTAL_LOAD\n",
    ")\n",
    "\n",
    "duration = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "if results[\"status\"] == \"success\":\n",
    "    print(f\"\\n‚úÖ Star Schema build completed in {duration:.2f} seconds!\")\n",
    "    \n",
    "    print(f\"\\nüìä Dimensions Built:\")\n",
    "    for dim_name, count in results.get(\"dimensions_built\", {}).items():\n",
    "        if not dim_name.endswith(\"_new\"):\n",
    "            new_count = results.get(\"dimensions_built\", {}).get(f\"{dim_name}_new\", \"\")\n",
    "            new_suffix = f\" (+{new_count} new)\" if new_count else \"\"\n",
    "            print(f\"   ‚Ä¢ {dim_name}: {count:,} records{new_suffix}\")\n",
    "    \n",
    "    print(f\"\\nüìà Fact Tables Built:\")\n",
    "    for fact_name, count in results.get(\"facts_built\", {}).items():\n",
    "        if not fact_name.endswith(\"_new\"):\n",
    "            new_count = results.get(\"facts_built\", {}).get(f\"{fact_name}_new\", \"\")\n",
    "            new_suffix = f\" (+{new_count} new)\" if new_count else \"\"\n",
    "            print(f\"   ‚Ä¢ {fact_name}: {count:,} records{new_suffix}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Output Directory: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Build failed:\")\n",
    "    for error in results.get(\"errors\", []):\n",
    "        print(f\"   {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c04cc",
   "metadata": {},
   "source": [
    "## Convert to Delta Tables (Fabric Only)\n",
    "Run this cell only in Microsoft Fabric to create Delta tables accessible via SQL Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_TO_DELTA_TABLES:\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        \n",
    "        print(\"Converting Parquet files to Delta tables...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert each parquet to Delta table\n",
    "        for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "            table_name = parquet_file.stem  # e.g., \"dim_date\", \"fact_activity\"\n",
    "            \n",
    "            try:\n",
    "                df = spark.read.parquet(str(parquet_file))\n",
    "                \n",
    "                # Write as Delta table (overwrite mode)\n",
    "                df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_name)\n",
    "                \n",
    "                print(f\"   ‚úÖ {table_name}: {df.count():,} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {table_name}: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(\"‚úÖ Delta tables created successfully!\")\n",
    "        print(\"   Tables are now available in the SQL Endpoint.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PySpark not available. Delta table creation skipped.\")\n",
    "        print(\"   This feature is only available in Microsoft Fabric.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Delta table creation skipped (WRITE_TO_DELTA_TABLES=False)\")\n",
    "    print(\"   Set WRITE_TO_DELTA_TABLES=True in Fabric to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6a547",
   "metadata": {},
   "source": [
    "## Validate Star Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAR SCHEMA VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load and validate tables\n",
    "tables = {}\n",
    "for parquet_file in Path(OUTPUT_DIR).glob(\"*.parquet\"):\n",
    "    table_name = parquet_file.stem\n",
    "    tables[table_name] = pd.read_parquet(parquet_file)\n",
    "    print(f\"{table_name}: {len(tables[table_name]):,} records\")\n",
    "\n",
    "# Check FK integrity\n",
    "print(\"\\nüîó Foreign Key Validation:\")\n",
    "\n",
    "fk_checks = [\n",
    "    ('fact_activity', 'workspace_sk', 'dim_workspace', 'workspace_sk'),\n",
    "    ('fact_activity', 'item_sk', 'dim_item', 'item_sk'),\n",
    "    ('fact_activity', 'user_sk', 'dim_user', 'user_sk'),\n",
    "    ('fact_activity', 'date_sk', 'dim_date', 'date_sk'),\n",
    "    ('fact_activity', 'time_sk', 'dim_time', 'time_sk'),\n",
    "    ('fact_activity', 'activity_type_sk', 'dim_activity_type', 'activity_type_sk'),\n",
    "    ('fact_activity', 'status_sk', 'dim_status', 'status_sk'),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for fact_table, fact_col, dim_table, dim_col in fk_checks:\n",
    "    if fact_table in tables and dim_table in tables:\n",
    "        fact_vals = set(tables[fact_table][fact_col].dropna().unique())\n",
    "        dim_vals = set(tables[dim_table][dim_col].unique())\n",
    "        orphans = fact_vals - dim_vals\n",
    "        status = '‚úÖ PASS' if len(orphans) == 0 else f'‚ùå {len(orphans)} orphans'\n",
    "        if len(orphans) > 0:\n",
    "            all_passed = False\n",
    "        print(f\"   {fact_col}: {status}\")\n",
    "\n",
    "print(\"\\n\" + (\"‚úÖ All FK validations passed!\" if all_passed else \"‚ö†Ô∏è Some FK validations failed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd3611",
   "metadata": {},
   "source": [
    "## Sample Analytical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Active Workspaces (FIXED: Aggregate by SK first, then join for names)\n",
    "print(\"üìä Top 10 Most Active Workspaces\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Aggregate by workspace_sk first (avoids duplicate rows from merge)\n",
    "    ws_stats = fact.groupby('workspace_sk').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Join with dimension for names\n",
    "    ws_stats = ws_stats.merge(\n",
    "        dim_ws[['workspace_sk', 'workspace_name', 'environment']], \n",
    "        on='workspace_sk', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Sort and display top 10\n",
    "    ws_stats = ws_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    ws_stats['duration_hrs'] = round(ws_stats['total_duration'] / 3600, 2)\n",
    "    ws_stats['failure_rate'] = round(100 * ws_stats['failed'] / ws_stats['activity_count'], 2)\n",
    "    \n",
    "    print(f\"{'Workspace':<45} | {'Env':<7} | {'Activities':>10} | {'Hours':>8} | {'Failed':>6} | {'Fail %':>6}\")\n",
    "    print(\"-\" * 105)\n",
    "    for _, row in ws_stats.iterrows():\n",
    "        ws_name = str(row['workspace_name'])[:44] if row['workspace_name'] else 'Unknown'\n",
    "        env = str(row['environment'])[:7] if row['environment'] else 'N/A'\n",
    "        print(f\"{ws_name:<45} | {env:<7} | {int(row['activity_count']):>10,} | {row['duration_hrs']:>8} | {int(row['failed']):>6} | {row['failure_rate']:>5}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity by Type\n",
    "print(\"üìä Activity Count by Type\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_activity_type' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_type = tables['dim_activity_type']\n",
    "    \n",
    "    merged = fact.merge(dim_type[['activity_type_sk', 'activity_type', 'activity_category']], on='activity_type_sk')\n",
    "    result = merged.groupby(['activity_category', 'activity_type']).size().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    for (cat, act_type), count in result.items():\n",
    "        print(f\"   {cat:<15} | {act_type:<30} | {count:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Activity Trend\n",
    "print(\"üìä Daily Activity Trend (Last 14 Days)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_daily_metrics' in tables and 'dim_date' in tables:\n",
    "    fact_daily = tables['fact_daily_metrics']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Aggregate across workspaces\n",
    "    daily_totals = fact_daily.groupby('date_sk').agg({\n",
    "        'total_activities': 'sum',\n",
    "        'unique_users': 'sum',\n",
    "        'failed_activities': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Join with date dimension\n",
    "    daily_totals = daily_totals.merge(\n",
    "        dim_date[['date_sk', 'full_date', 'day_of_week_name']], \n",
    "        on='date_sk'\n",
    "    ).sort_values('full_date', ascending=False).head(14)\n",
    "    \n",
    "    for _, row in daily_totals.iterrows():\n",
    "        day = row['day_of_week_name'][:3]\n",
    "        print(f\"   {row['full_date']} ({day}) | {int(row['total_activities']):>8,} activities | {int(row['unique_users']):>4} users | {int(row['failed_activities']):>3} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Analysis (Smart Merge Data)\n",
    "print(\"üìä Failure Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_status' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_status = tables['dim_status']\n",
    "    \n",
    "    # Overall failure stats\n",
    "    total = len(fact)\n",
    "    failed = (fact['is_failed'] == 1).sum()\n",
    "    success_rate = ((total - failed) / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    print(f\"   Total Activities:    {total:,}\")\n",
    "    print(f\"   Failed Activities:   {failed:,}\")\n",
    "    print(f\"   Success Rate:        {success_rate:.2f}%\")\n",
    "    \n",
    "    # Failures by status\n",
    "    print(f\"\\n   Failures by Status:\")\n",
    "    merged = fact[fact['is_failed'] == 1].merge(\n",
    "        dim_status[['status_sk', 'status_code']], on='status_sk'\n",
    "    )\n",
    "    if len(merged) > 0:\n",
    "        for status, count in merged['status_code'].value_counts().items():\n",
    "            print(f\"     - {status}: {count:,}\")\n",
    "    \n",
    "    # Failures by activity type (if available)\n",
    "    if 'dim_activity_type' in tables:\n",
    "        dim_type = tables['dim_activity_type']\n",
    "        merged = fact[fact['is_failed'] == 1].merge(\n",
    "            dim_type[['activity_type_sk', 'activity_type']], on='activity_type_sk'\n",
    "        )\n",
    "        if len(merged) > 0:\n",
    "            print(f\"\\n   Top Failed Activity Types:\")\n",
    "            for act_type, count in merged['activity_type'].value_counts().head(5).items():\n",
    "                print(f\"     - {act_type}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a6602",
   "metadata": {},
   "source": [
    "## Additional Sample Analytical Queries\n",
    "\n",
    "The following cells demonstrate various analytical capabilities of the star schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f15176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Frequently Used Items\n",
    "print(\"üìä Top 10 Most Frequently Used Items\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    \n",
    "    item_stats = fact.groupby('item_sk').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    item_stats = item_stats.merge(\n",
    "        dim_item[['item_sk', 'item_name', 'item_type', 'item_category']], \n",
    "        on='item_sk'\n",
    "    )\n",
    "    \n",
    "    top_items = item_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'Item Name':<40} | {'Type':<18} | {'Category':<12} | {'Activities':>10} | {'Users':>6}\")\n",
    "    print(\"-\" * 100)\n",
    "    for _, row in top_items.iterrows():\n",
    "        item_name = str(row['item_name'])[:39] if row['item_name'] else 'Unknown'\n",
    "        item_type = str(row['item_type'])[:17] if row['item_type'] else 'N/A'\n",
    "        category = str(row['item_category'])[:11] if row['item_category'] else 'N/A'\n",
    "        print(f\"{item_name:<40} | {item_type:<18} | {category:<12} | {int(row['activity_count']):>10,} | {int(row['unique_users']):>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df35c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity by Hour of Day (Peak Usage Analysis)\n",
    "print(\"üìä Activity by Hour of Day (Peak Usage Analysis)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_time' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_time = tables['dim_time']\n",
    "    \n",
    "    hourly_stats = fact.groupby('time_sk').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    hourly_stats = hourly_stats.merge(\n",
    "        dim_time[['time_sk', 'hour', 'period_of_day', 'is_business_hours']], \n",
    "        on='time_sk'\n",
    "    )\n",
    "    \n",
    "    hourly_stats = hourly_stats.sort_values('hour')\n",
    "    \n",
    "    print(f\"{'Hour':<6} | {'Period':<12} | {'Business':>8} | {'Activities':>12} | {'Users':>6}\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in hourly_stats.iterrows():\n",
    "        hour_str = f\"{int(row['hour']):02d}:00\"\n",
    "        period = str(row['period_of_day'])[:11]\n",
    "        biz = \"Yes\" if row['is_business_hours'] else \"No\"\n",
    "        print(f\"{hour_str:<6} | {period:<12} | {biz:>8} | {int(row['activity_count']):>12,} | {int(row['unique_users']):>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d4f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Most Active Users\n",
    "print(\"üìä Top 10 Most Active Users\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_user' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_user = tables['dim_user']\n",
    "    \n",
    "    user_stats = fact.groupby('user_sk').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_workspaces=('workspace_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    user_stats = user_stats.merge(\n",
    "        dim_user[['user_sk', 'user_key', 'user_domain', 'user_type']], \n",
    "        on='user_sk'\n",
    "    )\n",
    "    \n",
    "    top_users = user_stats.sort_values('activity_count', ascending=False).head(10)\n",
    "    \n",
    "    print(f\"{'User':<35} | {'Domain':<15} | {'Type':<8} | {'Activities':>10} | {'Items':>6} | {'WS':>4}\")\n",
    "    print(\"-\" * 90)\n",
    "    for _, row in top_users.iterrows():\n",
    "        user_key = str(row['user_key'])[:34] if row['user_key'] else 'Unknown'\n",
    "        domain = str(row['user_domain'])[:14] if row['user_domain'] else 'N/A'\n",
    "        user_type = str(row['user_type'])[:7] if row['user_type'] else 'N/A'\n",
    "        print(f\"{user_key:<35} | {domain:<15} | {user_type:<8} | {int(row['activity_count']):>10,} | {int(row['unique_items']):>6} | {int(row['unique_workspaces']):>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30558aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity Volume by Day of Week\n",
    "print(\"üìä Activity Volume by Day of Week\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_date' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    daily_stats = fact.groupby('date_sk').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    daily_stats = daily_stats.merge(\n",
    "        dim_date[['date_sk', 'day_of_week_name', 'day_of_week', 'is_weekend']], \n",
    "        on='date_sk'\n",
    "    )\n",
    "    \n",
    "    dow_stats = daily_stats.groupby(['day_of_week', 'day_of_week_name', 'is_weekend']).agg({\n",
    "        'activity_count': 'sum',\n",
    "        'unique_users': 'mean',\n",
    "        'failed': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dow_stats = dow_stats.sort_values('day_of_week')\n",
    "    \n",
    "    print(f\"{'Day':<12} | {'Weekend':>8} | {'Activities':>12} | {'Avg Users':>10} | {'Failed':>8}\")\n",
    "    print(\"-\" * 60)\n",
    "    for _, row in dow_stats.iterrows():\n",
    "        day_name = str(row['day_of_week_name'])\n",
    "        weekend = \"Yes\" if row['is_weekend'] else \"No\"\n",
    "        print(f\"{day_name:<12} | {weekend:>8} | {int(row['activity_count']):>12,} | {row['unique_users']:>10.1f} | {int(row['failed']):>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef725f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Comparison (DEV vs TEST vs UAT vs PRD)\n",
    "print(\"üìä Environment Comparison\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Get workspace environments\n",
    "    ws_env = dim_ws[['workspace_sk', 'environment']].drop_duplicates()\n",
    "    fact_with_env = fact.merge(ws_env, on='workspace_sk', how='left')\n",
    "    \n",
    "    env_stats = fact_with_env.groupby('environment').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_workspaces=('workspace_sk', 'nunique'),\n",
    "        total_duration=('duration_seconds', 'sum'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    env_stats['duration_hrs'] = round(env_stats['total_duration'] / 3600, 2)\n",
    "    env_stats['failure_rate'] = round(100 * env_stats['failed'] / env_stats['activity_count'], 2)\n",
    "    env_stats = env_stats.sort_values('activity_count', ascending=False)\n",
    "    \n",
    "    print(f\"{'Environment':<12} | {'Activities':>12} | {'Users':>6} | {'Items':>6} | {'WS':>4} | {'Hours':>10} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in env_stats.iterrows():\n",
    "        env = str(row['environment']) if row['environment'] else 'Unknown'\n",
    "        print(f\"{env:<12} | {int(row['activity_count']):>12,} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['unique_workspaces']):>4} | {row['duration_hrs']:>10} | {row['failure_rate']:>6}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item Category Distribution\n",
    "print(\"üìä Activity by Item Category\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    \n",
    "    item_cat = dim_item[['item_sk', 'item_category']].drop_duplicates()\n",
    "    fact_with_cat = fact.merge(item_cat, on='item_sk', how='left')\n",
    "    \n",
    "    cat_stats = fact_with_cat.groupby('item_category').agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    cat_stats['failure_rate'] = round(100 * cat_stats['failed'] / cat_stats['activity_count'], 2)\n",
    "    cat_stats = cat_stats.sort_values('activity_count', ascending=False)\n",
    "    \n",
    "    print(f\"{'Category':<18} | {'Activities':>12} | {'Items':>6} | {'Users':>6} | {'Failed':>8} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in cat_stats.iterrows():\n",
    "        cat = str(row['item_category'])[:17] if row['item_category'] else 'Unknown'\n",
    "        print(f\"{cat:<18} | {int(row['activity_count']):>12,} | {int(row['unique_items']):>6} | {int(row['unique_users']):>6} | {int(row['failed']):>8} | {row['failure_rate']:>6}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-Running Activities Analysis\n",
    "print(\"üìä Long-Running Activities Analysis (>1 hour)\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_item' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_item = tables['dim_item']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Filter for long-running activities (>1 hour = 3600 seconds)\n",
    "    long_running = fact[fact['is_long_running'] == 1].copy() if 'is_long_running' in fact.columns else fact[fact['duration_seconds'] > 3600].copy()\n",
    "    \n",
    "    if len(long_running) > 0:\n",
    "        print(f\"Total long-running activities (>1 hr): {len(long_running):,}\")\n",
    "        print()\n",
    "        \n",
    "        # Enrich with names\n",
    "        long_running = long_running.merge(\n",
    "            dim_item[['item_sk', 'item_name', 'item_type']], on='item_sk', how='left'\n",
    "        ).merge(\n",
    "            dim_ws[['workspace_sk', 'workspace_name']], on='workspace_sk', how='left'\n",
    "        )\n",
    "        \n",
    "        # Top 10 longest\n",
    "        top_longest = long_running.nlargest(10, 'duration_seconds')\n",
    "        \n",
    "        print(f\"{'Item Name':<35} | {'Type':<15} | {'Workspace':<25} | {'Duration':>12}\")\n",
    "        print(\"-\" * 100)\n",
    "        for _, row in top_longest.iterrows():\n",
    "            item_name = str(row['item_name'])[:34] if row['item_name'] else 'Unknown'\n",
    "            item_type = str(row['item_type'])[:14] if row['item_type'] else 'N/A'\n",
    "            ws_name = str(row['workspace_name'])[:24] if row['workspace_name'] else 'Unknown'\n",
    "            duration_hrs = round(row['duration_seconds'] / 3600, 2)\n",
    "            print(f\"{item_name:<35} | {item_type:<15} | {ws_name:<25} | {duration_hrs:>10} hrs\")\n",
    "    else:\n",
    "        print(\"No long-running activities found (>1 hour)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Workspace Usage Patterns (Workspaces shared by multiple users)\n",
    "print(\"üìä Cross-Workspace Usage Patterns\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_workspace' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_ws = tables['dim_workspace']\n",
    "    \n",
    "    # Calculate collaboration metrics per workspace\n",
    "    ws_collab = fact.groupby('workspace_sk').agg(\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        activity_count=('activity_id', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    ws_collab = ws_collab.merge(\n",
    "        dim_ws[['workspace_sk', 'workspace_name', 'environment']], on='workspace_sk'\n",
    "    )\n",
    "    \n",
    "    # Sort by user count (collaboration indicator)\n",
    "    ws_collab = ws_collab.sort_values('unique_users', ascending=False).head(10)\n",
    "    ws_collab['activities_per_user'] = round(ws_collab['activity_count'] / ws_collab['unique_users'], 1)\n",
    "    \n",
    "    print(f\"{'Workspace':<40} | {'Env':<7} | {'Users':>6} | {'Items':>6} | {'Activities':>10} | {'Act/User':>8}\")\n",
    "    print(\"-\" * 95)\n",
    "    for _, row in ws_collab.iterrows():\n",
    "        ws_name = str(row['workspace_name'])[:39] if row['workspace_name'] else 'Unknown'\n",
    "        env = str(row['environment'])[:6] if row['environment'] else 'N/A'\n",
    "        print(f\"{ws_name:<40} | {env:<7} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['activity_count']):>10,} | {row['activities_per_user']:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Trend Analysis\n",
    "print(\"üìä Monthly Activity Trend\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if 'fact_activity' in tables and 'dim_date' in tables:\n",
    "    fact = tables['fact_activity']\n",
    "    dim_date = tables['dim_date']\n",
    "    \n",
    "    # Join with date dimension\n",
    "    fact_dated = fact.merge(\n",
    "        dim_date[['date_sk', 'year', 'month', 'month_name']], on='date_sk'\n",
    "    )\n",
    "    \n",
    "    monthly_stats = fact_dated.groupby(['year', 'month', 'month_name']).agg(\n",
    "        activity_count=('activity_id', 'count'),\n",
    "        unique_users=('user_sk', 'nunique'),\n",
    "        unique_items=('item_sk', 'nunique'),\n",
    "        failed=('is_failed', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    monthly_stats = monthly_stats.sort_values(['year', 'month'], ascending=False).head(12)\n",
    "    monthly_stats['failure_rate'] = round(100 * monthly_stats['failed'] / monthly_stats['activity_count'], 2)\n",
    "    \n",
    "    print(f\"{'Month':<15} | {'Activities':>12} | {'Users':>6} | {'Items':>6} | {'Failed':>8} | {'Fail %':>7}\")\n",
    "    print(\"-\" * 70)\n",
    "    for _, row in monthly_stats.iterrows():\n",
    "        month_label = f\"{row['month_name'][:3]} {int(row['year'])}\"\n",
    "        print(f\"{month_label:<15} | {int(row['activity_count']):>12,} | {int(row['unique_users']):>6} | {int(row['unique_items']):>6} | {int(row['failed']):>8} | {row['failure_rate']:>6}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22a685",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The star schema has been built and is ready for:\n",
    "\n",
    "1. **SQL Queries** - Query the parquet files directly or use Delta tables via SQL Endpoint\n",
    "2. **Semantic Model** - Create a Direct Lake model pointing to these tables\n",
    "3. **Power BI Reports** - Build monitoring dashboards using the semantic model\n",
    "\n",
    "### Scheduled Refresh\n",
    "To automate the star schema build:\n",
    "1. Schedule this notebook to run daily after the Monitor Hub pipeline\n",
    "2. Or use a Fabric Data Pipeline to orchestrate both steps\n",
    "\n",
    "### Table Relationships (for Semantic Model)\n",
    "```\n",
    "fact_activity[date_sk] ‚Üí dim_date[date_sk]\n",
    "fact_activity[time_sk] ‚Üí dim_time[time_sk]\n",
    "fact_activity[workspace_sk] ‚Üí dim_workspace[workspace_sk]\n",
    "fact_activity[item_sk] ‚Üí dim_item[item_sk]\n",
    "fact_activity[user_sk] ‚Üí dim_user[user_sk]\n",
    "fact_activity[activity_type_sk] ‚Üí dim_activity_type[activity_type_sk]\n",
    "fact_activity[status_sk] ‚Üí dim_status[status_sk]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
