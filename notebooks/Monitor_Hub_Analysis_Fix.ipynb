{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a409903",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis (Fix)\n",
    "\n",
    "This notebook performs the analysis using the raw downloaded data directly, bypassing the potentially incomplete CSV reports.\n",
    "\n",
    "## Fixes Implemented:\n",
    "1.  **Workspace & Error Messages**: Merges detailed job history to populate missing fields.\n",
    "2.  **User ID Recovery (Smart Merge)**: Correlates detailed jobs with base activity logs (by Item ID & Time) to preserve the original `User ID` instead of defaulting to \"System\".\n",
    "3.  **Non-Destructive**: Runs entirely within this notebook, leaving the core library untouched to prevent breaking changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "from usf_fabric_monitoring.core.data_loader import load_activities_from_directory\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = \"monitor_hub_analysis\" \n",
    "\n",
    "# Initialize Pipeline (to access helper methods)\n",
    "pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "\n",
    "print(f\"üìÇ Output Directory: {pipeline.output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Raw Data (Skip API Extraction)\n",
    "\n",
    "# A. Load Base Activities from 'raw_data/daily'\n",
    "extraction_dir = pipeline._prepare_extraction_directory()\n",
    "print(f\"Loading raw activities from: {extraction_dir}\")\n",
    "activities = load_activities_from_directory(str(extraction_dir))\n",
    "print(f\"‚úÖ Loaded {len(activities)} base activities.\")\n",
    "\n",
    "# B. Load Detailed Jobs from 'fabric_item_details'\n",
    "print(\"Loading detailed job history...\")\n",
    "detailed_jobs = pipeline._load_detailed_jobs()\n",
    "print(f\"‚úÖ Loaded {len(detailed_jobs)} detailed job records.\")\n",
    "\n",
    "# C. Optimized Smart Merge (Pandas)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîÑ Starting Optimized Smart Merge (Pandas)...\")\n",
    "\n",
    "# 1. Convert to DataFrames\n",
    "df_activities = pd.DataFrame(activities)\n",
    "df_jobs = pd.DataFrame(detailed_jobs)\n",
    "\n",
    "# 2. Pre-process for Merge\n",
    "# Ensure timestamps are datetime and UTC\n",
    "def to_utc(df, col):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "    return df\n",
    "\n",
    "df_activities = to_utc(df_activities, \"start_time\")\n",
    "df_jobs = to_utc(df_jobs, \"startTimeUtc\")\n",
    "\n",
    "# Filter out jobs without start time or item id\n",
    "df_jobs = df_jobs.dropna(subset=[\"startTimeUtc\", \"itemId\"])\n",
    "\n",
    "# Rename job columns for merge preparation\n",
    "# We map 'itemId' to 'item_id' for the join key\n",
    "df_jobs = df_jobs.rename(columns={\n",
    "    \"startTimeUtc\": \"job_start_time\",\n",
    "    \"itemId\": \"item_id\", \n",
    "    \"status\": \"job_status\",\n",
    "    \"failureReason\": \"job_failure_reason\"\n",
    "})\n",
    "\n",
    "# Sort for merge_asof (required)\n",
    "df_activities = df_activities.sort_values(\"start_time\")\n",
    "df_jobs = df_jobs.sort_values(\"job_start_time\")\n",
    "\n",
    "# 3. Merge Asof\n",
    "# Find the nearest job for each activity to enrich it\n",
    "# Tolerance: 5 minutes (API logs vs Job History can drift)\n",
    "merged_df = pd.merge_asof(\n",
    "    df_activities,\n",
    "    df_jobs,\n",
    "    left_on=\"start_time\",\n",
    "    right_on=\"job_start_time\",\n",
    "    by=\"item_id\",\n",
    "    tolerance=pd.Timedelta(\"5min\"),\n",
    "    direction=\"nearest\"\n",
    ")\n",
    "\n",
    "print(f\"   - Merged {len(merged_df)} records.\")\n",
    "\n",
    "# 4. Enrich Data\n",
    "# Extract error message from the job's failure details\n",
    "def extract_error_msg(val):\n",
    "    if pd.isna(val): return None\n",
    "    if isinstance(val, dict): return val.get(\"message\")\n",
    "    return str(val)\n",
    "\n",
    "def extract_error_code(val):\n",
    "    if pd.isna(val): return None\n",
    "    if isinstance(val, dict): return val.get(\"errorCode\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Ensure target columns exist before filling\n",
    "for col_name in [\"failure_reason\", \"error_message\", \"error_code\"]:\n",
    "    if col_name not in merged_df.columns:\n",
    "        merged_df[col_name] = None\n",
    "\n",
    "# Apply extraction if job data was found\n",
    "if \"job_failure_reason\" in merged_df.columns:\n",
    "    merged_df[\"job_error_message\"] = merged_df[\"job_failure_reason\"].apply(extract_error_msg)\n",
    "    merged_df[\"job_error_code\"] = merged_df[\"job_failure_reason\"].apply(extract_error_code)\n",
    "    \n",
    "    # Coalesce with existing columns\n",
    "    # If activity has no error info, take it from the job\n",
    "    merged_df[\"failure_reason\"] = merged_df[\"failure_reason\"].fillna(merged_df[\"job_failure_reason\"].astype(str))\n",
    "    merged_df[\"error_message\"] = merged_df[\"error_message\"].fillna(merged_df[\"job_error_message\"])\n",
    "    merged_df[\"error_code\"] = merged_df[\"error_code\"].fillna(merged_df[\"job_error_code\"])\n",
    "    \n",
    "    # Enrich other metadata\n",
    "    if \"_workspace_name\" in merged_df.columns:\n",
    "        merged_df[\"workspace_name\"] = merged_df[\"workspace_name\"].fillna(merged_df[\"_workspace_name\"])\n",
    "    if \"_item_name\" in merged_df.columns:\n",
    "        merged_df[\"item_name\"] = merged_df[\"item_name\"].fillna(merged_df[\"_item_name\"])\n",
    "    if \"_item_type\" in merged_df.columns:\n",
    "        merged_df[\"item_type\"] = merged_df[\"item_type\"].fillna(merged_df[\"_item_type\"])\n",
    "        \n",
    "    # Update status: If job failed, the activity failed (even if API said InProgress)\n",
    "    merged_df.loc[merged_df[\"job_status\"] == \"Failed\", \"status\"] = \"Failed\"\n",
    "\n",
    "# 5. Convert back to list of dicts for compatibility\n",
    "merged_activities = merged_df.to_dict(orient=\"records\")\n",
    "\n",
    "print(f\"‚úÖ Smart Merge Complete.\")\n",
    "print(f\"   - Total Activities: {len(merged_activities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9516246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare DataFrame for Analysis (Pandas Fallback)\n",
    "\n",
    "# Note: We are using Pandas directly because the local Spark environment \n",
    "# is experiencing connection issues. The data volume is small enough for Pandas.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîÑ Preparing Analysis DataFrame (Pandas)...\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_pd = pd.DataFrame(merged_activities)\n",
    "\n",
    "# Ensure critical columns exist\n",
    "expected_cols = [\"workspace_name\", \"failure_reason\", \"error_message\", \"error_code\", \"submitted_by\", \"item_name\", \"item_type\"]\n",
    "for c in expected_cols:\n",
    "    if c not in df_pd.columns:\n",
    "        df_pd[c] = None\n",
    "\n",
    "# Filter for Failures\n",
    "final_df = df_pd[df_pd[\"status\"] == \"Failed\"].copy()\n",
    "\n",
    "count = len(final_df)\n",
    "print(f\"‚úÖ Filtered to {count} failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare Analysis DataFrame (Pandas)\n",
    "\n",
    "# Helper for Coalesce\n",
    "def coalesce_series(*series):\n",
    "    result = series[0].copy()\n",
    "    for s in series[1:]:\n",
    "        result = result.fillna(s)\n",
    "    return result\n",
    "\n",
    "# Helper for User Name Extraction\n",
    "def extract_user_name(user_id):\n",
    "    if pd.isna(user_id) or not isinstance(user_id, str):\n",
    "        return user_id\n",
    "    try:\n",
    "        # Extract part before @ and replace . with space\n",
    "        name_part = user_id.split('@')[0]\n",
    "        return name_part.replace('.', ' ').title()\n",
    "    except:\n",
    "        return user_id\n",
    "\n",
    "# Select and Rename columns\n",
    "analysis_df = pd.DataFrame()\n",
    "\n",
    "# Workspace\n",
    "analysis_df[\"Workspace\"] = coalesce_series(\n",
    "    final_df[\"workspace_name\"], \n",
    "    final_df[\"workspace_id\"]\n",
    ").fillna(\"Unknown\")\n",
    "\n",
    "# Item Name\n",
    "analysis_df[\"Item Name\"] = final_df[\"item_name\"].fillna(\"Unknown\")\n",
    "\n",
    "# Item Type\n",
    "analysis_df[\"Item Type\"] = final_df[\"item_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Invoke Type\n",
    "analysis_df[\"Invoke Type\"] = final_df[\"activity_type\"]\n",
    "\n",
    "# Time & Duration\n",
    "analysis_df[\"Start Time\"] = final_df[\"start_time\"]\n",
    "analysis_df[\"End Time\"] = final_df[\"end_time\"]\n",
    "analysis_df[\"Duration (s)\"] = final_df[\"duration_seconds\"]\n",
    "\n",
    "# User ID\n",
    "analysis_df[\"User ID\"] = final_df[\"submitted_by\"]\n",
    "\n",
    "# User Name\n",
    "analysis_df[\"User Name\"] = final_df[\"submitted_by\"].apply(extract_user_name)\n",
    "# Fallback to User ID if extraction failed or was null\n",
    "analysis_df[\"User Name\"] = analysis_df[\"User Name\"].fillna(analysis_df[\"User ID\"])\n",
    "\n",
    "# Error Details\n",
    "analysis_df[\"Error Message\"] = coalesce_series(\n",
    "    final_df[\"failure_reason\"], \n",
    "    final_df[\"error_message\"], \n",
    "    final_df[\"error_code\"]\n",
    ").fillna(\"Unknown Error\")\n",
    "\n",
    "analysis_df[\"Error Code\"] = final_df[\"error_code\"]\n",
    "\n",
    "print(\"‚úÖ Analysis DataFrame Prepared.\")\n",
    "print(analysis_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f01f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Execute Analysis (Pandas)\n",
    "\n",
    "if not analysis_df.empty:\n",
    "    # --- 1. Summary Statistics ---\n",
    "    total_failures = len(analysis_df)\n",
    "    unique_workspaces = analysis_df[\"Workspace\"].nunique()\n",
    "    unique_items = analysis_df[\"Item Name\"].nunique()\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY STATISTICS\")\n",
    "    print(f\"Total Failures: {total_failures}\")\n",
    "    print(f\"Affected Workspaces: {unique_workspaces}\")\n",
    "    print(f\"Affected Items: {unique_items}\")\n",
    "\n",
    "    # --- 2. Top 10 Failing Items ---\n",
    "    print(\"\\nüèÜ TOP 10 FAILING ITEMS\")\n",
    "    top_items = analysis_df.groupby([\"Workspace\", \"Item Name\", \"Item Type\"]) \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False) \\\n",
    "        .head(10)\n",
    "    print(top_items.to_string(index=False))\n",
    "\n",
    "    # --- 3. Failures by User ---\n",
    "    print(\"\\nüë§ FAILURES BY USER\")\n",
    "    user_stats = analysis_df.groupby(\"User Name\") \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    print(user_stats.to_string(index=False))\n",
    "\n",
    "    # --- 4. Error Message Distribution ---\n",
    "    print(\"\\n‚ö†Ô∏è ERROR MESSAGE DISTRIBUTION\")\n",
    "    error_stats = analysis_df.groupby(\"Error Message\") \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    print(error_stats.to_string(index=False))\n",
    "\n",
    "    # --- 5. Recent Failures (Last 20) ---\n",
    "    print(\"\\nüïí MOST RECENT FAILURES\")\n",
    "    recent_failures = analysis_df[[\"Start Time\", \"Workspace\", \"Item Name\", \"User Name\", \"Error Message\"]] \\\n",
    "        .sort_values(\"Start Time\", ascending=False) \\\n",
    "        .head(20)\n",
    "    \n",
    "    # Truncate long error messages for display\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    print(recent_failures.to_string(index=False))\n",
    "else:\n",
    "    print(\"No failure data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b07744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Investigate Missing End Times and Duration Issues\n",
    "\n",
    "print(\"üîç INVESTIGATING DURATION AND END TIME ISSUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check the original raw data structure\n",
    "print(\"üìã SAMPLE RAW ACTIVITY STRUCTURE:\")\n",
    "if activities:\n",
    "    sample_activity = activities[0]\n",
    "    for key, value in sample_activity.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä RAW ACTIVITIES DATA ANALYSIS:\")\n",
    "print(f\"Total raw activities: {len(activities)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "raw_df = pd.DataFrame(activities)\n",
    "\n",
    "# Check end_time availability in raw data\n",
    "if 'end_time' in raw_df.columns:\n",
    "    end_time_missing = raw_df['end_time'].isna().sum()\n",
    "    end_time_total = len(raw_df)\n",
    "    print(f\"Missing end_time in raw data: {end_time_missing}/{end_time_total} ({end_time_missing/end_time_total*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚ùå 'end_time' column not found in raw activities\")\n",
    "    print(\"Available columns:\", list(raw_df.columns))\n",
    "\n",
    "# Check detailed jobs data for duration info\n",
    "print(f\"\\nüìä DETAILED JOBS DATA ANALYSIS:\")\n",
    "jobs_df = pd.DataFrame(detailed_jobs)\n",
    "print(f\"Total detailed jobs: {len(detailed_jobs)}\")\n",
    "\n",
    "if detailed_jobs:\n",
    "    sample_job = detailed_jobs[0]\n",
    "    print(\"Sample job structure:\")\n",
    "    for key, value in sample_job.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check for duration-related fields in jobs\n",
    "duration_fields = ['duration', 'durationSeconds', 'endTime', 'endTimeUtc', 'startTime', 'startTimeUtc']\n",
    "available_duration_fields = [field for field in duration_fields if field in jobs_df.columns]\n",
    "print(f\"\\nAvailable duration-related fields in jobs: {available_duration_fields}\")\n",
    "\n",
    "for field in available_duration_fields:\n",
    "    if field in jobs_df.columns:\n",
    "        missing_count = jobs_df[field].isna().sum()\n",
    "        total_count = len(jobs_df)\n",
    "        print(f\"  {field}: {missing_count}/{total_count} missing ({missing_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c52379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fix Duration Calculation Using Job Data\n",
    "\n",
    "print(\"üîß IMPLEMENTING DURATION FIX\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a copy of merged_df to avoid modifying the original\n",
    "fixed_df = merged_df.copy()\n",
    "\n",
    "# Convert job times to datetime if they aren't already\n",
    "if 'job_start_time' in fixed_df.columns:\n",
    "    fixed_df['job_start_time'] = pd.to_datetime(fixed_df['job_start_time'], utc=True, errors='coerce')\n",
    "\n",
    "if 'endTimeUtc' in fixed_df.columns:\n",
    "    fixed_df['job_end_time'] = pd.to_datetime(fixed_df['endTimeUtc'], utc=True, errors='coerce')\n",
    "else:\n",
    "    # Create job_end_time from endTimeUtc if it exists in the merge\n",
    "    job_columns = [col for col in fixed_df.columns if 'endTime' in col]\n",
    "    print(f\"Available end time columns: {job_columns}\")\n",
    "    \n",
    "    if job_columns:\n",
    "        end_time_col = job_columns[0]  # Use the first available end time column\n",
    "        fixed_df['job_end_time'] = pd.to_datetime(fixed_df[end_time_col], utc=True, errors='coerce')\n",
    "\n",
    "# Fix end_time: Use job end time when activity end time is missing\n",
    "print(\"Fixing end_time...\")\n",
    "original_missing_end_time = fixed_df['end_time'].isna().sum()\n",
    "print(f\"  Activities missing end_time: {original_missing_end_time}\")\n",
    "\n",
    "if 'job_end_time' in fixed_df.columns:\n",
    "    # Fill missing end_time with job_end_time\n",
    "    fixed_df['end_time'] = fixed_df['end_time'].fillna(fixed_df['job_end_time'])\n",
    "    \n",
    "    after_fix_missing_end_time = fixed_df['end_time'].isna().sum()\n",
    "    fixed_count = original_missing_end_time - after_fix_missing_end_time\n",
    "    print(f\"  Fixed {fixed_count} missing end times using job data\")\n",
    "    print(f\"  Remaining missing end_time: {after_fix_missing_end_time}\")\n",
    "\n",
    "# Recalculate duration_seconds\n",
    "print(\"Recalculating duration...\")\n",
    "def calculate_duration(start_time, end_time):\n",
    "    if pd.isna(start_time) or pd.isna(end_time):\n",
    "        return 0.0\n",
    "    try:\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        return max(0.0, duration)  # Ensure non-negative duration\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "fixed_df['duration_seconds'] = fixed_df.apply(\n",
    "    lambda row: calculate_duration(row['start_time'], row['end_time']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Update the merged_activities list with fixed data\n",
    "fixed_activities = fixed_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Show improvement statistics\n",
    "original_zero_duration = (merged_df['duration_seconds'] == 0.0).sum()\n",
    "fixed_zero_duration = (fixed_df['duration_seconds'] == 0.0).sum()\n",
    "improvement = original_zero_duration - fixed_zero_duration\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT STATISTICS:\")\n",
    "print(f\"  Original zero duration records: {original_zero_duration}\")\n",
    "print(f\"  Fixed zero duration records: {fixed_zero_duration}\")  \n",
    "print(f\"  Records with duration restored: {improvement}\")\n",
    "\n",
    "# Update the global variables for downstream analysis\n",
    "merged_activities = fixed_activities\n",
    "merged_df = fixed_df\n",
    "\n",
    "print(\"‚úÖ Duration fix applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Regenerate Analysis with Fixed Duration Data\n",
    "\n",
    "print(\"üîÑ REGENERATING ANALYSIS WITH FIXED DURATION DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Re-prepare DataFrame for Analysis with fixed data\n",
    "df_pd_fixed = pd.DataFrame(merged_activities)\n",
    "\n",
    "# Ensure critical columns exist\n",
    "for c in expected_cols:\n",
    "    if c not in df_pd_fixed.columns:\n",
    "        df_pd_fixed[c] = None\n",
    "\n",
    "# Filter for Failures\n",
    "final_df_fixed = df_pd_fixed[df_pd_fixed[\"status\"] == \"Failed\"].copy()\n",
    "\n",
    "# Regenerate Analysis DataFrame with fixed duration\n",
    "analysis_df_fixed = pd.DataFrame()\n",
    "\n",
    "# Workspace  \n",
    "analysis_df_fixed[\"Workspace\"] = coalesce_series(\n",
    "    final_df_fixed[\"workspace_name\"], \n",
    "    final_df_fixed[\"workspace_id\"]\n",
    ").fillna(\"Unknown\")\n",
    "\n",
    "# Item Name\n",
    "analysis_df_fixed[\"Item Name\"] = final_df_fixed[\"item_name\"].fillna(\"Unknown\")\n",
    "\n",
    "# Item Type\n",
    "analysis_df_fixed[\"Item Type\"] = final_df_fixed[\"item_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Invoke Type\n",
    "analysis_df_fixed[\"Invoke Type\"] = final_df_fixed[\"activity_type\"]\n",
    "\n",
    "# Time & Duration (FIXED)\n",
    "analysis_df_fixed[\"Start Time\"] = final_df_fixed[\"start_time\"]\n",
    "analysis_df_fixed[\"End Time\"] = final_df_fixed[\"end_time\"]\n",
    "analysis_df_fixed[\"Duration (s)\"] = final_df_fixed[\"duration_seconds\"]\n",
    "\n",
    "# User ID\n",
    "analysis_df_fixed[\"User ID\"] = final_df_fixed[\"submitted_by\"]\n",
    "\n",
    "# User Name\n",
    "analysis_df_fixed[\"User Name\"] = final_df_fixed[\"submitted_by\"].apply(extract_user_name)\n",
    "analysis_df_fixed[\"User Name\"] = analysis_df_fixed[\"User Name\"].fillna(analysis_df_fixed[\"User ID\"])\n",
    "\n",
    "# Error Details\n",
    "analysis_df_fixed[\"Error Message\"] = coalesce_series(\n",
    "    final_df_fixed[\"failure_reason\"], \n",
    "    final_df_fixed[\"error_message\"], \n",
    "    final_df_fixed[\"error_code\"]\n",
    ").fillna(\"Unknown Error\")\n",
    "\n",
    "analysis_df_fixed[\"Error Code\"] = final_df_fixed[\"error_code\"]\n",
    "\n",
    "# Show duration improvement\n",
    "zero_duration_original = (analysis_df[\"Duration (s)\"] == 0.0).sum()\n",
    "zero_duration_fixed = (analysis_df_fixed[\"Duration (s)\"] == 0.0).sum()\n",
    "non_zero_duration_fixed = (analysis_df_fixed[\"Duration (s)\"] > 0.0).sum()\n",
    "\n",
    "print(f\"üìä DURATION ANALYSIS IMPROVEMENT:\")\n",
    "print(f\"  Original analysis - Zero duration failures: {zero_duration_original}\")\n",
    "print(f\"  Fixed analysis - Zero duration failures: {zero_duration_fixed}\")\n",
    "print(f\"  Fixed analysis - Non-zero duration failures: {non_zero_duration_fixed}\")\n",
    "print(f\"  Improvement: {zero_duration_original - zero_duration_fixed} failures now have duration data\")\n",
    "\n",
    "# Show sample of fixed data\n",
    "print(f\"\\n‚úÖ SAMPLE OF FIXED ANALYSIS DATA:\")\n",
    "print(analysis_df_fixed[analysis_df_fixed[\"Duration (s)\"] > 0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38599c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Enhanced Duration-Based Analysis\n",
    "\n",
    "print(\"‚è±Ô∏è ENHANCED DURATION-BASED ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Filter for failures with valid durations\n",
    "valid_duration_failures = analysis_df_fixed[analysis_df_fixed[\"Duration (s)\"] > 0].copy()\n",
    "\n",
    "if len(valid_duration_failures) > 0:\n",
    "    print(f\"üìä DURATION STATISTICS:\")\n",
    "    print(f\"  Failures with duration data: {len(valid_duration_failures)}\")\n",
    "    print(f\"  Average failure duration: {valid_duration_failures['Duration (s)'].mean():.2f} seconds\")\n",
    "    print(f\"  Median failure duration: {valid_duration_failures['Duration (s)'].median():.2f} seconds\") \n",
    "    print(f\"  Max failure duration: {valid_duration_failures['Duration (s)'].max():.2f} seconds\")\n",
    "    print(f\"  Min failure duration: {valid_duration_failures['Duration (s)'].min():.2f} seconds\")\n",
    "    \n",
    "    # Duration percentiles\n",
    "    percentiles = [25, 50, 75, 90, 95, 99]\n",
    "    print(f\"\\n  Duration Percentiles:\")\n",
    "    for p in percentiles:\n",
    "        value = valid_duration_failures['Duration (s)'].quantile(p/100)\n",
    "        print(f\"    {p}th percentile: {value:.2f}s\")\n",
    "    \n",
    "    # Longest running failures\n",
    "    print(f\"\\nüêå TOP 10 LONGEST RUNNING FAILURES:\")\n",
    "    longest_failures = valid_duration_failures.nlargest(10, \"Duration (s)\")\n",
    "    for idx, row in longest_failures.iterrows():\n",
    "        print(f\"  {row['Duration (s)']:.1f}s - {row['Workspace']} / {row['Item Name']} ({row['Item Type']})\")\n",
    "    \n",
    "    # Quick vs Long failures\n",
    "    quick_threshold = 30  # 30 seconds\n",
    "    long_threshold = 300  # 5 minutes\n",
    "    \n",
    "    quick_failures = len(valid_duration_failures[valid_duration_failures[\"Duration (s)\"] <= quick_threshold])\n",
    "    long_failures = len(valid_duration_failures[valid_duration_failures[\"Duration (s)\"] >= long_threshold])\n",
    "    \n",
    "    print(f\"\\n‚ö° FAILURE CATEGORIES BY DURATION:\")\n",
    "    print(f\"  Quick failures (‚â§{quick_threshold}s): {quick_failures}\")\n",
    "    print(f\"  Long failures (‚â•{long_threshold}s): {long_failures}\")\n",
    "    print(f\"  Medium failures: {len(valid_duration_failures) - quick_failures - long_failures}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No failures with valid duration data found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Duration analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the fixed analysis DataFrame\n",
    "analysis_df_fixed.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
