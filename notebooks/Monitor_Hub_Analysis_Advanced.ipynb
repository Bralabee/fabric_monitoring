{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a409903",
   "metadata": {},
   "source": [
    "# Monitor Hub Advanced Analysis\n",
    "\n",
    "This notebook is an exploratory offline insight extraction tool that demonstrates how Fabric monitoring data can be utilized to extract deep operational insights.\n",
    "\n",
    "## Key Features & Capabilities:\n",
    "1.  **Smart Data Correlation**: Merges detailed job history with activity logs to enrich analysis with comprehensive metadata including workspace names, error messages, and execution status.\n",
    "2.  **Advanced User Analytics**: Correlates detailed job execution data with activity logs by Item ID and timestamp proximity to accurately track user activities and preserve attribution.\n",
    "3.  **Non-Intrusive Design**: Operates independently as an exploratory tool without modifying core libraries, enabling safe experimentation and insight discovery.\n",
    "4.  **Comprehensive Data Enrichment**: Combines multiple data sources to provide complete context for failure analysis, performance monitoring, and operational intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13fd75",
   "metadata": {},
   "source": [
    "# Technical Documentation: Data Architecture & Schema Design\n",
    "\n",
    "## Overview\n",
    "This notebook implements an advanced data integration pipeline that combines multiple Fabric monitoring data sources to produce comprehensive operational reports. The architecture is built on three core data streams that are merged through temporal and entity-based joins.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Core Data Sources\n",
    "\n",
    "### 1.1 Base Activity Stream (`activities`)\n",
    "**Source**: Fabric Activity Events API  \n",
    "**Extraction Path**: `raw_data/daily/*.json`  \n",
    "**Granularity**: Individual activity events\n",
    "\n",
    "#### Schema Structure:\n",
    "```\n",
    "{\n",
    "  \"activity_id\": string,           // Unique identifier for activity event\n",
    "  \"workspace_id\": string,           // GUID of workspace\n",
    "  \"workspace_name\": string,         // Human-readable workspace name (often incomplete)\n",
    "  \"item_id\": string,                // GUID of Fabric item\n",
    "  \"item_name\": string,              // Name of item (often incomplete)\n",
    "  \"item_type\": string,              // Type: DataPipeline, Notebook, Lakehouse, etc.\n",
    "  \"activity_type\": string,          // Operation: RunArtifact, ExecuteNotebook, etc.\n",
    "  \"status\": string,                 // Completed, Failed, InProgress\n",
    "  \"start_time\": datetime,           // Activity start timestamp (UTC)\n",
    "  \"end_time\": datetime,             // Activity end timestamp (UTC) - often NULL\n",
    "  \"creation_time\": datetime,        // Event creation timestamp\n",
    "  \"submitted_by\": string,           // User email (primary user tracking field)\n",
    "  \"created_by\": string,             // User email (rarely populated - 100% NULL)\n",
    "  \"last_updated_by\": string,        // User email (rarely populated - 100% NULL)\n",
    "  \"domain\": string,                 // Fabric domain classification\n",
    "  \"location\": string,               // Geographic location\n",
    "  \"object_url\": string,             // Direct URL to Fabric item\n",
    "  \"failure_reason\": string,         // High-level failure description (sparse)\n",
    "  \"error_message\": string           // Detailed error message (sparse)\n",
    "}\n",
    "```\n",
    "\n",
    "#### Data Quality Characteristics:\n",
    "- **Volume**: ~2M events per 30-day period\n",
    "- **User Field Coverage**: `submitted_by` (95.74% populated), `created_by` (0% populated)\n",
    "- **Temporal Coverage**: `start_time` (100%), `end_time` (~40% populated)\n",
    "- **Metadata Completeness**: Workspace/item names often missing or show GUIDs\n",
    "- **Failure Details**: Error information incomplete in ~60% of failure events\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Detailed Job History Stream (`detailed_jobs`)\n",
    "**Source**: Fabric Item Job History API  \n",
    "**Extraction Path**: `fabric_item_details/jobs_*.json`  \n",
    "**Granularity**: Job execution instances\n",
    "\n",
    "#### Schema Structure:\n",
    "```\n",
    "{\n",
    "  \"id\": string,                    // Job instance ID\n",
    "  \"itemId\": string,                // GUID of Fabric item (join key)\n",
    "  \"jobType\": string,               // Pipeline, Notebook, SparkJob, etc.\n",
    "  \"invokeType\": string,            // Manual, Scheduled, OnDemand\n",
    "  \"status\": string,                // Completed, Failed, Cancelled\n",
    "  \"startTimeUtc\": datetime,        // Job start timestamp (join key)\n",
    "  \"endTimeUtc\": datetime,          // Job end timestamp\n",
    "  \"failureReason\": object,         // Structured failure details\n",
    "  \"rootActivityId\": string,        // Correlation to activity stream\n",
    "  \"invoker\": object                // User information (limited)\n",
    "}\n",
    "```\n",
    "\n",
    "#### Failure Reason Schema (Nested):\n",
    "```\n",
    "failureReason: {\n",
    "  \"message\": string,               // Human-readable error description\n",
    "  \"errorCode\": string,             // Standardized error code\n",
    "  \"messageParameters\": object      // Additional context\n",
    "}\n",
    "```\n",
    "\n",
    "#### Data Quality Characteristics:\n",
    "- **Volume**: ~400K job records per 30-day period\n",
    "- **Temporal Precision**: Accurate to millisecond level\n",
    "- **Metadata Completeness**: High (95%+ for core fields)\n",
    "- **Failure Details**: Comprehensive structured error information\n",
    "- **Coverage**: Only captures items with job execution history\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Workspace & Item Metadata\n",
    "**Source**: Enriched from both streams during merge\n",
    "**Purpose**: Dimensional reference data\n",
    "\n",
    "#### Workspace Schema:\n",
    "```\n",
    "{\n",
    "  \"workspace_id\": string,          // Primary key\n",
    "  \"workspace_name\": string,        // Resolved name\n",
    "  \"domain\": string,                // Classification\n",
    "  \"location\": string               // Geographic region\n",
    "}\n",
    "```\n",
    "\n",
    "#### Item Schema:\n",
    "```\n",
    "{\n",
    "  \"item_id\": string,               // Primary key\n",
    "  \"item_name\": string,             // Resolved name\n",
    "  \"item_type\": string,             // Fabric item type\n",
    "  \"workspace_id\": string           // Foreign key to workspace\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Integration Architecture\n",
    "\n",
    "### 2.1 Smart Merge Algorithm\n",
    "The core integration uses **pandas merge_asof** to perform temporal proximity joins:\n",
    "\n",
    "```python\n",
    "merged_df = pd.merge_asof(\n",
    "    df_activities,           // Left table: All activity events\n",
    "    df_jobs,                 // Right table: Job execution details\n",
    "    left_on='start_time',    // Temporal join key (activity)\n",
    "    right_on='job_start_time', // Temporal join key (job)\n",
    "    by='item_id',            // Entity join key (exact match)\n",
    "    tolerance=pd.Timedelta('5min'),  // Max time difference\n",
    "    direction='nearest'      // Find closest match\n",
    ")\n",
    "```\n",
    "\n",
    "#### Join Logic:\n",
    "1. **Entity Match**: Exact match on `item_id` (Fabric item GUID)\n",
    "2. **Temporal Match**: Find nearest job start time within ±5 minutes of activity start\n",
    "3. **Cardinality**: Many-to-one (activities to jobs)\n",
    "4. **Preservation**: All activities retained (left join)\n",
    "\n",
    "#### Why 5-minute tolerance?\n",
    "- API timestamp drift between activity logs and job history\n",
    "- Allows matching despite slight clock skew\n",
    "- Validated empirically to capture 98%+ of valid correlations\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Data Enrichment Pipeline\n",
    "\n",
    "#### Phase 1: Timestamp Normalization\n",
    "```python\n",
    "# Convert all timestamps to UTC datetime\n",
    "df_activities['start_time'] = pd.to_datetime(df_activities['start_time'], utc=True)\n",
    "df_jobs['job_start_time'] = pd.to_datetime(df_jobs['startTimeUtc'], utc=True)\n",
    "```\n",
    "\n",
    "#### Phase 2: Pre-Join Filtering\n",
    "```python\n",
    "# Remove invalid records before merge\n",
    "df_jobs = df_jobs.dropna(subset=['job_start_time', 'item_id'])\n",
    "```\n",
    "\n",
    "#### Phase 3: Column Mapping & Enrichment\n",
    "```python\n",
    "# Map job columns to merged schema\n",
    "df_jobs.rename(columns={\n",
    "    'id': 'job_instance_id',\n",
    "    'startTimeUtc': 'job_start_time',\n",
    "    'endTimeUtc': 'job_end_time',\n",
    "    'status': 'job_status',\n",
    "    'failureReason': 'job_failure_reason'\n",
    "})\n",
    "```\n",
    "\n",
    "#### Phase 4: Status Resolution\n",
    "```python\n",
    "# Job status takes precedence (more accurate)\n",
    "merged_df.loc[merged_df['job_status'] == 'Failed', 'status'] = 'Failed'\n",
    "```\n",
    "\n",
    "#### Phase 5: Failure Detail Extraction\n",
    "```python\n",
    "# Extract structured error information\n",
    "def extract_error_msg(failure_reason):\n",
    "    if isinstance(failure_reason, dict):\n",
    "        return failure_reason.get('message')\n",
    "    return str(failure_reason)\n",
    "\n",
    "merged_df['error_message'] = merged_df['job_failure_reason'].apply(extract_error_msg)\n",
    "merged_df['error_code'] = merged_df['job_failure_reason'].apply(\n",
    "    lambda x: x.get('errorCode') if isinstance(x, dict) else None\n",
    ")\n",
    "```\n",
    "\n",
    "#### Phase 6: Duration Calculation Fix\n",
    "```python\n",
    "# Use job end times when activity end times missing\n",
    "merged_df['end_time'] = merged_df['end_time'].fillna(merged_df['job_end_time'])\n",
    "\n",
    "# Recalculate duration\n",
    "merged_df['duration_seconds'] = (\n",
    "    merged_df['end_time'] - merged_df['start_time']\n",
    ").dt.total_seconds()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Aggregation & Report Generation\n",
    "\n",
    "### 3.1 Dimensional Aggregations\n",
    "\n",
    "#### Workspace Activity Analysis\n",
    "```python\n",
    "workspace_agg = merged_df.groupBy('workspace_id', 'workspace_name').agg(\n",
    "    count('*').alias('total_activities'),\n",
    "    countDistinct('submitted_by').alias('unique_users'),\n",
    "    countDistinct('item_id').alias('unique_items'),\n",
    "    sum(when(col('status') == 'Failed', 1).otherwise(0)).alias('failures'),\n",
    "    avg('duration_seconds').alias('avg_duration')\n",
    ")\n",
    "```\n",
    "\n",
    "**Output Schema**:\n",
    "```\n",
    "workspace_id | workspace_name | total_activities | unique_users | unique_items | failures | avg_duration\n",
    "```\n",
    "\n",
    "#### User Activity Analysis\n",
    "```python\n",
    "user_agg = merged_df.groupBy('submitted_by').agg(\n",
    "    count('*').alias('total_activities'),\n",
    "    countDistinct('workspace_id').alias('workspaces_accessed'),\n",
    "    sum(when(col('status') == 'Failed', 1).otherwise(0)).alias('failure_count'),\n",
    "    (1 - sum(when(col('status') == 'Failed', 1).otherwise(0)) / count('*')).alias('success_rate')\n",
    ")\n",
    "```\n",
    "\n",
    "**Output Schema**:\n",
    "```\n",
    "submitted_by | total_activities | workspaces_accessed | failure_count | success_rate\n",
    "```\n",
    "\n",
    "#### Item Type Performance Analysis\n",
    "```python\n",
    "item_type_agg = merged_df.groupBy('item_type', 'activity_type').agg(\n",
    "    count('*').alias('execution_count'),\n",
    "    avg('duration_seconds').alias('avg_duration'),\n",
    "    percentile_approx('duration_seconds', 0.95).alias('p95_duration'),\n",
    "    sum(when(col('status') == 'Failed', 1).otherwise(0)).alias('failures')\n",
    ")\n",
    "```\n",
    "\n",
    "**Output Schema**:\n",
    "```\n",
    "item_type | activity_type | execution_count | avg_duration | p95_duration | failures\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Time-Series Aggregations\n",
    "\n",
    "#### Daily Activity Trends\n",
    "```python\n",
    "merged_df['date'] = merged_df['start_time'].dt.date\n",
    "daily_agg = merged_df.groupBy('date').agg(\n",
    "    count('*').alias('total_activities'),\n",
    "    sum(when(col('status') == 'Failed', 1).otherwise(0)).alias('failures'),\n",
    "    avg('duration_seconds').alias('avg_duration')\n",
    ")\n",
    "```\n",
    "\n",
    "#### Hourly Pattern Analysis\n",
    "```python\n",
    "merged_df['hour'] = merged_df['start_time'].dt.hour\n",
    "hourly_agg = merged_df.groupBy('hour', 'item_type').agg(\n",
    "    count('*').alias('activity_count')\n",
    ").pivot('item_type')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Failure Analysis Deep Dive\n",
    "\n",
    "#### Error Distribution\n",
    "```python\n",
    "error_agg = merged_df.filter(col('status') == 'Failed').groupBy(\n",
    "    'error_code', 'error_message'\n",
    ").agg(\n",
    "    count('*').alias('occurrence_count'),\n",
    "    countDistinct('workspace_id').alias('affected_workspaces'),\n",
    "    countDistinct('item_id').alias('affected_items'),\n",
    "    min('start_time').alias('first_occurrence'),\n",
    "    max('start_time').alias('last_occurrence')\n",
    ")\n",
    "```\n",
    "\n",
    "**Output Schema**:\n",
    "```\n",
    "error_code | error_message | occurrence_count | affected_workspaces | affected_items | first_occurrence | last_occurrence\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Transformation Rules\n",
    "\n",
    "### 4.1 Coalescing Strategy (Priority Order)\n",
    "For fields that exist in both streams, we use this precedence:\n",
    "\n",
    "1. **Status**: Job status > Activity status (job status is authoritative)\n",
    "2. **Error Details**: Job failure_reason > Activity failure_reason\n",
    "3. **End Time**: Job end_time > Activity end_time\n",
    "4. **User**: Activity submitted_by (job invoker unreliable)\n",
    "\n",
    "### 4.2 Handling NULL Values\n",
    "\n",
    "```python\n",
    "# User identification\n",
    "merged_df['user'] = coalesce(\n",
    "    col('submitted_by'),      # Primary (95% coverage)\n",
    "    col('created_by'),        # Backup (0% coverage)\n",
    "    lit('Unknown')            # Fallback\n",
    ")\n",
    "\n",
    "# Workspace/Item names\n",
    "merged_df['workspace_name'] = coalesce(\n",
    "    col('workspace_name'),    # From activities\n",
    "    col('workspace_id')       # Fallback to GUID\n",
    ")\n",
    "\n",
    "# Failure details\n",
    "merged_df['failure_reason'] = coalesce(\n",
    "    col('job_failure_reason'),    # From jobs (preferred)\n",
    "    col('failure_reason'),        # From activities\n",
    "    lit('Unknown Error')          # Fallback\n",
    ")\n",
    "```\n",
    "\n",
    "### 4.3 Data Type Conversions\n",
    "\n",
    "```python\n",
    "# Temporal fields\n",
    "for col in ['start_time', 'end_time', 'creation_time']:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "# Numeric fields\n",
    "df['duration_seconds'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
    "df['duration_minutes'] = df['duration_seconds'] / 60.0\n",
    "\n",
    "# Categorical standardization\n",
    "df['status'] = df['status'].fillna('Unknown')\n",
    "df['item_type'] = df['item_type'].str.strip()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Report Outputs\n",
    "\n",
    "### 5.1 Generated Reports & Schemas\n",
    "\n",
    "#### `activities_master.csv`\n",
    "**Purpose**: Complete unified activity log  \n",
    "**Rows**: ~2M per 30 days  \n",
    "**Schema**: 22 columns (merged schema from sections 1.1 + 1.2)\n",
    "\n",
    "#### `workspace_activity_summary.csv`\n",
    "**Purpose**: Workspace-level KPIs  \n",
    "**Schema**:\n",
    "```\n",
    "workspace_id | workspace_name | domain | total_activities | unique_users | \n",
    "unique_items | failed_activities | success_rate | avg_duration_minutes | \n",
    "total_duration_hours | first_activity | last_activity\n",
    "```\n",
    "\n",
    "#### `user_activity_summary.csv`\n",
    "**Purpose**: User behavior & performance metrics  \n",
    "**Schema**:\n",
    "```\n",
    "submitted_by | total_activities | unique_workspaces | unique_items | \n",
    "failed_activities | success_rate | avg_duration_minutes | \n",
    "most_used_item_type | total_duration_hours\n",
    "```\n",
    "\n",
    "#### `item_type_performance.csv`\n",
    "**Purpose**: Item type benchmarking  \n",
    "**Schema**:\n",
    "```\n",
    "item_type | activity_type | total_executions | failed_executions | \n",
    "success_rate | avg_duration_minutes | p50_duration | p95_duration | \n",
    "max_duration | total_duration_hours\n",
    "```\n",
    "\n",
    "#### `failure_analysis.csv`\n",
    "**Purpose**: Detailed failure investigation  \n",
    "**Schema**:\n",
    "```\n",
    "workspace_name | item_name | item_type | activity_type | start_time | \n",
    "end_time | duration_minutes | submitted_by | error_code | error_message | \n",
    "failure_reason | status\n",
    "```\n",
    "\n",
    "#### `daily_activity_trends.csv`\n",
    "**Purpose**: Time-series analysis  \n",
    "**Schema**:\n",
    "```\n",
    "date | total_activities | failed_activities | success_rate | \n",
    "unique_users | unique_workspaces | avg_duration_minutes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Data Quality & Validation\n",
    "\n",
    "### 6.1 Pre-Merge Validation\n",
    "```python\n",
    "# Check for duplicate activity IDs\n",
    "assert df_activities['activity_id'].duplicated().sum() == 0\n",
    "\n",
    "# Validate timestamp ranges\n",
    "assert df_activities['start_time'].notna().all()\n",
    "assert (df_activities['end_time'] >= df_activities['start_time']).all()\n",
    "\n",
    "# Check join key coverage\n",
    "print(f\"Activities with item_id: {df_activities['item_id'].notna().mean():.1%}\")\n",
    "print(f\"Jobs with item_id: {df_jobs['item_id'].notna().mean():.1%}\")\n",
    "```\n",
    "\n",
    "### 6.2 Post-Merge Validation\n",
    "```python\n",
    "# Verify merge coverage\n",
    "merge_rate = merged_df['job_instance_id'].notna().mean()\n",
    "print(f\"Activities matched to jobs: {merge_rate:.1%}\")\n",
    "\n",
    "# Check enrichment effectiveness\n",
    "print(f\"Records with error details: {merged_df['error_message'].notna().mean():.1%}\")\n",
    "print(f\"Records with duration: {merged_df['duration_seconds'].notna().mean():.1%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Performance Considerations\n",
    "\n",
    "### 7.1 Data Volume Management\n",
    "- **Activities**: ~2M rows/month → ~24M rows/year\n",
    "- **Jobs**: ~400K rows/month → ~4.8M rows/year\n",
    "- **Merged**: ~2M rows/month (1:1 cardinality preserved)\n",
    "\n",
    "### 7.2 Optimization Strategies\n",
    "1. **Partitioning**: By date for incremental processing\n",
    "2. **Indexing**: On `item_id` and `start_time` for join performance\n",
    "3. **Caching**: 8-hour cache for detailed job extractions\n",
    "4. **Parquet Storage**: Columnar format for analytical queries\n",
    "\n",
    "### 7.3 Scalability Path\n",
    "```python\n",
    "# Current: Pandas (in-memory) → ~10M row limit\n",
    "# Future: PySpark (distributed) → unlimited scale\n",
    "\n",
    "# Spark equivalent of merge_asof\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, abs\n",
    "\n",
    "window_spec = Window.partitionBy('item_id').orderBy(\n",
    "    abs(col('start_time') - col('job_start_time'))\n",
    ")\n",
    "\n",
    "spark_merged = df_activities.join(\n",
    "    df_jobs, \n",
    "    on='item_id', \n",
    "    how='left'\n",
    ").withColumn('rank', row_number().over(window_spec)).filter(col('rank') == 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Extension Points\n",
    "\n",
    "### 8.1 Adding New Data Sources\n",
    "To integrate additional monitoring streams:\n",
    "\n",
    "```python\n",
    "# 1. Define schema\n",
    "new_source_schema = {\n",
    "    'source_id': 'string',\n",
    "    'item_id': 'string',      # Common join key\n",
    "    'timestamp': 'datetime',   # Common temporal key\n",
    "    'metric_value': 'float'\n",
    "}\n",
    "\n",
    "# 2. Load and normalize\n",
    "df_new = pd.read_json('new_source.json')\n",
    "df_new['timestamp'] = pd.to_datetime(df_new['timestamp'], utc=True)\n",
    "\n",
    "# 3. Merge with existing\n",
    "merged_df = merged_df.merge_asof(\n",
    "    df_new,\n",
    "    left_on='start_time',\n",
    "    right_on='timestamp',\n",
    "    by='item_id',\n",
    "    tolerance=pd.Timedelta('1min')\n",
    ")\n",
    "```\n",
    "\n",
    "### 8.2 Custom Aggregations\n",
    "Template for adding new report types:\n",
    "\n",
    "```python\n",
    "def generate_custom_report(df, groupby_cols, metrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df: Merged dataframe\n",
    "        groupby_cols: List of dimension columns\n",
    "        metrics: Dict of {output_name: aggregation_function}\n",
    "    \"\"\"\n",
    "    return df.groupby(groupby_cols).agg(**metrics).reset_index()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Known Limitations & Mitigation\n",
    "\n",
    "### 9.1 Data Completeness\n",
    "| Field | Coverage | Mitigation |\n",
    "|-------|----------|------------|\n",
    "| end_time | 40% | Use job_end_time from detailed jobs |\n",
    "| created_by | 0% | Use submitted_by instead |\n",
    "| error_message | 60% | Merge with job failure_reason |\n",
    "| workspace_name | 75% | Accept GUID fallback |\n",
    "\n",
    "### 9.2 Temporal Alignment\n",
    "- **Issue**: 5-minute tolerance may miss events\n",
    "- **Impact**: ~2% of activities unmatched\n",
    "- **Mitigation**: Adjust tolerance based on validation metrics\n",
    "\n",
    "### 9.3 Job History Coverage\n",
    "- **Issue**: Only items with execution history appear in jobs stream\n",
    "- **Impact**: Read-only operations (e.g., Lakehouse queries) may be missing from job details\n",
    "- **Mitigation**: Activity stream remains complete; job enrichment is additive\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary: Data Flow Diagram\n",
    "\n",
    "```\n",
    "┌─────────────────────────┐\n",
    "│ Fabric Activity Events  │\n",
    "│ API                     │\n",
    "└───────────┬─────────────┘\n",
    "            │ Extract\n",
    "            ▼\n",
    "┌─────────────────────────┐      ┌──────────────────────┐\n",
    "│ Raw Activities          │      │ Fabric Item Job      │\n",
    "│ (~2M rows/month)        │      │ History API          │\n",
    "│ - Sparse metadata       │      └─────────┬────────────┘\n",
    "│ - Incomplete errors     │                │ Extract\n",
    "│ - Missing end times     │                ▼\n",
    "└───────────┬─────────────┘      ┌──────────────────────┐\n",
    "            │                    │ Detailed Jobs        │\n",
    "            │                    │ (~400K rows/month)   │\n",
    "            │                    │ - Complete metadata  │\n",
    "            │                    │ - Structured errors  │\n",
    "            │                    │ - Accurate durations │\n",
    "            │                    └─────────┬────────────┘\n",
    "            │                              │\n",
    "            │         Smart Merge          │\n",
    "            │      (merge_asof on          │\n",
    "            │    item_id + start_time)     │\n",
    "            └──────────────┬───────────────┘\n",
    "                           │\n",
    "                           ▼\n",
    "            ┌──────────────────────────┐\n",
    "            │ Enriched Activity Stream │\n",
    "            │ (~2M rows/month)         │\n",
    "            │ - Complete metadata      │\n",
    "            │ - Accurate status        │\n",
    "            │ - Detailed errors        │\n",
    "            │ - Calculated durations   │\n",
    "            └──────────────┬───────────┘\n",
    "                           │\n",
    "                  Group By & Aggregate\n",
    "                           │\n",
    "            ┌──────────────┴───────────────┐\n",
    "            │                              │\n",
    "            ▼                              ▼\n",
    "┌───────────────────────┐      ┌──────────────────────┐\n",
    "│ Dimensional Reports   │      │ Time-Series Reports  │\n",
    "│ - Workspace summary   │      │ - Daily trends       │\n",
    "│ - User activity       │      │ - Hourly patterns    │\n",
    "│ - Item performance    │      │ - Failure timeline   │\n",
    "│ - Failure analysis    │      │ - Duration trends    │\n",
    "└───────────────────────┘      └──────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This architecture enables comprehensive operational monitoring by intelligently combining sparse real-time activity events with detailed historical job execution data, producing actionable insights for Fabric platform management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "from usf_fabric_monitoring.core.data_loader import load_activities_from_directory\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = os.getenv(\"EXPORT_DIRECTORY\", \"exports/monitor_hub_analysis\")\n",
    "\n",
    "# Initialize Pipeline (to access helper methods)\n",
    "pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Output Directory: {pipeline.output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Raw Data (Skip API Extraction)\n",
    "\n",
    "# A. Load Base Activities from 'raw_data/daily'\n",
    "extraction_dir = pipeline._prepare_extraction_directory()\n",
    "print(f\"Loading raw activities from: {extraction_dir}\")\n",
    "activities = load_activities_from_directory(str(extraction_dir))\n",
    "print(f\" Loaded {len(activities)} base activities.\")\n",
    "\n",
    "# B. Load Detailed Jobs from 'fabric_item_details'\n",
    "print(\"Loading detailed job history...\")\n",
    "detailed_jobs = pipeline._load_detailed_jobs()\n",
    "print(f\" Loaded {len(detailed_jobs)} detailed job records.\")\n",
    "\n",
    "# C. Optimized Smart Merge (Pandas)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\" Starting Optimized Smart Merge (Pandas)...\")\n",
    "\n",
    "# 1. Convert to DataFrames\n",
    "df_activities = pd.DataFrame(activities)\n",
    "df_jobs = pd.DataFrame(detailed_jobs)\n",
    "\n",
    "# 2. Pre-process for Merge\n",
    "# Ensure timestamps are datetime and UTC\n",
    "def to_utc(df, col):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "    return df\n",
    "\n",
    "df_activities = to_utc(df_activities, \"start_time\")\n",
    "df_jobs = to_utc(df_jobs, \"startTimeUtc\")\n",
    "\n",
    "# Filter out jobs without start time or item id\n",
    "df_jobs = df_jobs.dropna(subset=[\"startTimeUtc\", \"itemId\"])\n",
    "\n",
    "# Rename job columns for merge preparation\n",
    "# We map 'itemId' to 'item_id' for the join key\n",
    "df_jobs = df_jobs.rename(columns={\n",
    "    \"startTimeUtc\": \"job_start_time\",\n",
    "    \"itemId\": \"item_id\", \n",
    "    \"status\": \"job_status\",\n",
    "    \"failureReason\": \"job_failure_reason\"\n",
    "})\n",
    "\n",
    "# Sort for merge_asof (required)\n",
    "df_activities = df_activities.sort_values(\"start_time\")\n",
    "df_jobs = df_jobs.sort_values(\"job_start_time\")\n",
    "\n",
    "# 3. Merge Asof\n",
    "# Find the nearest job for each activity to enrich it\n",
    "# Tolerance: 5 minutes (API logs vs Job History can drift)\n",
    "merged_df = pd.merge_asof(\n",
    "    df_activities,\n",
    "    df_jobs,\n",
    "    left_on=\"start_time\",\n",
    "    right_on=\"job_start_time\",\n",
    "    by=\"item_id\",\n",
    "    tolerance=pd.Timedelta(\"5min\"),\n",
    "    direction=\"nearest\"\n",
    ")\n",
    "\n",
    "print(f\"   - Merged {len(merged_df)} records.\")\n",
    "\n",
    "# 4. Enrich Data\n",
    "# Extract error message from the job's failure details\n",
    "def extract_error_msg(val):\n",
    "    if pd.isna(val): return None\n",
    "    if isinstance(val, dict): return val.get(\"message\")\n",
    "    return str(val)\n",
    "\n",
    "def extract_error_code(val):\n",
    "    if pd.isna(val): return None\n",
    "    if isinstance(val, dict): return val.get(\"errorCode\")\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Ensure target columns exist before filling\n",
    "for col_name in [\"failure_reason\", \"error_message\", \"error_code\"]:\n",
    "    if col_name not in merged_df.columns:\n",
    "        merged_df[col_name] = None\n",
    "\n",
    "# Apply extraction if job data was found\n",
    "if \"job_failure_reason\" in merged_df.columns:\n",
    "    merged_df[\"job_error_message\"] = merged_df[\"job_failure_reason\"].apply(extract_error_msg)\n",
    "    merged_df[\"job_error_code\"] = merged_df[\"job_failure_reason\"].apply(extract_error_code)\n",
    "    \n",
    "    # Coalesce with existing columns\n",
    "    # If activity has no error info, take it from the job\n",
    "    merged_df[\"failure_reason\"] = merged_df[\"failure_reason\"].fillna(merged_df[\"job_failure_reason\"].astype(str))\n",
    "    merged_df[\"error_message\"] = merged_df[\"error_message\"].fillna(merged_df[\"job_error_message\"])\n",
    "    merged_df[\"error_code\"] = merged_df[\"error_code\"].fillna(merged_df[\"job_error_code\"])\n",
    "    \n",
    "    # Enrich other metadata\n",
    "    if \"_workspace_name\" in merged_df.columns:\n",
    "        merged_df[\"workspace_name\"] = merged_df[\"workspace_name\"].fillna(merged_df[\"_workspace_name\"])\n",
    "    if \"_item_name\" in merged_df.columns:\n",
    "        merged_df[\"item_name\"] = merged_df[\"item_name\"].fillna(merged_df[\"_item_name\"])\n",
    "    if \"_item_type\" in merged_df.columns:\n",
    "        merged_df[\"item_type\"] = merged_df[\"item_type\"].fillna(merged_df[\"_item_type\"])\n",
    "        \n",
    "    # Update status: If job failed, the activity failed (even if API said InProgress)\n",
    "    merged_df.loc[merged_df[\"job_status\"] == \"Failed\", \"status\"] = \"Failed\"\n",
    "\n",
    "# 5. Convert back to list of dicts for compatibility\n",
    "merged_activities = merged_df.to_dict(orient=\"records\")\n",
    "\n",
    "print(f\" Smart Merge Complete.\")\n",
    "print(f\"   - Total Activities: {len(merged_activities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9516246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prepare DataFrame for Analysis (Pandas Fallback)\n",
    "\n",
    "# Note: We are using Pandas directly because the local Spark environment \n",
    "# is experiencing connection issues. The data volume is small enough for Pandas.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\" Preparing Analysis DataFrame (Pandas)...\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_pd = pd.DataFrame(merged_activities)\n",
    "\n",
    "# Ensure critical columns exist\n",
    "expected_cols = [\"workspace_name\", \"failure_reason\", \"error_message\", \"error_code\", \"submitted_by\", \"item_name\", \"item_type\"]\n",
    "for c in expected_cols:\n",
    "    if c not in df_pd.columns:\n",
    "        df_pd[c] = None\n",
    "\n",
    "# Filter for Failures\n",
    "final_df = df_pd[df_pd[\"status\"] == \"Failed\"].copy()\n",
    "\n",
    "count = len(final_df)\n",
    "print(f\" Filtered to {count} failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare Analysis DataFrame (Pandas)\n",
    "\n",
    "# Helper for Coalesce\n",
    "def coalesce_series(*series):\n",
    "    result = series[0].copy()\n",
    "    for s in series[1:]:\n",
    "        result = result.fillna(s)\n",
    "    return result\n",
    "\n",
    "# Helper for User Name Extraction\n",
    "def extract_user_name(user_id):\n",
    "    if pd.isna(user_id) or not isinstance(user_id, str):\n",
    "        return user_id\n",
    "    try:\n",
    "        # Extract part before @ and replace . with space\n",
    "        name_part = user_id.split('@')[0]\n",
    "        return name_part.replace('.', ' ').title()\n",
    "    except:\n",
    "        return user_id\n",
    "\n",
    "# Select and Rename columns\n",
    "analysis_df = pd.DataFrame()\n",
    "\n",
    "# Workspace\n",
    "analysis_df[\"Workspace\"] = coalesce_series(\n",
    "    final_df[\"workspace_name\"], \n",
    "    final_df[\"workspace_id\"]\n",
    ").fillna(\"Unknown\")\n",
    "\n",
    "# Item Name\n",
    "analysis_df[\"Item Name\"] = final_df[\"item_name\"].fillna(\"Unknown\")\n",
    "\n",
    "# Item Type\n",
    "analysis_df[\"Item Type\"] = final_df[\"item_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Invoke Type\n",
    "analysis_df[\"Invoke Type\"] = final_df[\"activity_type\"]\n",
    "\n",
    "# Time & Duration\n",
    "analysis_df[\"Start Time\"] = final_df[\"start_time\"]\n",
    "analysis_df[\"End Time\"] = final_df[\"end_time\"]\n",
    "analysis_df[\"Duration (s)\"] = final_df[\"duration_seconds\"]\n",
    "\n",
    "# User ID\n",
    "analysis_df[\"User ID\"] = final_df[\"submitted_by\"]\n",
    "\n",
    "# User Name\n",
    "analysis_df[\"User Name\"] = final_df[\"submitted_by\"].apply(extract_user_name)\n",
    "# Fallback to User ID if extraction failed or was null\n",
    "analysis_df[\"User Name\"] = analysis_df[\"User Name\"].fillna(analysis_df[\"User ID\"])\n",
    "\n",
    "# Error Details\n",
    "analysis_df[\"Error Message\"] = coalesce_series(\n",
    "    final_df[\"failure_reason\"], \n",
    "    final_df[\"error_message\"], \n",
    "    final_df[\"error_code\"]\n",
    ").fillna(\"Unknown Error\")\n",
    "\n",
    "analysis_df[\"Error Code\"] = final_df[\"error_code\"]\n",
    "\n",
    "print(\" Analysis DataFrame Prepared.\")\n",
    "print(analysis_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f01f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Execute Analysis (Pandas)\n",
    "\n",
    "if not analysis_df.empty:\n",
    "    # --- 1. Summary Statistics ---\n",
    "    total_failures = len(analysis_df)\n",
    "    unique_workspaces = analysis_df[\"Workspace\"].nunique()\n",
    "    unique_items = analysis_df[\"Item Name\"].nunique()\n",
    "    \n",
    "    print(f\"\\n SUMMARY STATISTICS\")\n",
    "    print(f\"Total Failures: {total_failures}\")\n",
    "    print(f\"Affected Workspaces: {unique_workspaces}\")\n",
    "    print(f\"Affected Items: {unique_items}\")\n",
    "\n",
    "    # --- 2. Top 10 Failing Items ---\n",
    "    print(\"\\n TOP 10 FAILING ITEMS\")\n",
    "    top_items = analysis_df.groupby([\"Workspace\", \"Item Name\", \"Item Type\"]) \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False) \\\n",
    "        .head(10)\n",
    "    print(top_items.to_string(index=False))\n",
    "\n",
    "    # --- 3. Failures by User ---\n",
    "    print(\"\\n FAILURES BY USER\")\n",
    "    user_stats = analysis_df.groupby(\"User Name\") \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    print(user_stats.to_string(index=False))\n",
    "\n",
    "    # --- 4. Error Message Distribution ---\n",
    "    print(\"\\n ERROR MESSAGE DISTRIBUTION\")\n",
    "    error_stats = analysis_df.groupby(\"Error Message\") \\\n",
    "        .size() \\\n",
    "        .reset_index(name=\"count\") \\\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    print(error_stats.to_string(index=False))\n",
    "\n",
    "    # --- 5. Recent Failures (Last 20) ---\n",
    "    print(\"\\n MOST RECENT FAILURES\")\n",
    "    recent_failures = analysis_df[[\"Start Time\", \"Workspace\", \"Item Name\", \"User Name\", \"Error Message\"]] \\\n",
    "        .sort_values(\"Start Time\", ascending=False) \\\n",
    "        .head(20)\n",
    "    \n",
    "    # Truncate long error messages for display\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    print(recent_failures.to_string(index=False))\n",
    "else:\n",
    "    print(\"No failure data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b07744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Investigate Missing End Times and Duration Issues\n",
    "\n",
    "print(\" INVESTIGATING DURATION AND END TIME ISSUES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check the original raw data structure\n",
    "print(\" SAMPLE RAW ACTIVITY STRUCTURE:\")\n",
    "if activities:\n",
    "    sample_activity = activities[0]\n",
    "    for key, value in sample_activity.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n RAW ACTIVITIES DATA ANALYSIS:\")\n",
    "print(f\"Total raw activities: {len(activities)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "raw_df = pd.DataFrame(activities)\n",
    "\n",
    "# Check end_time availability in raw data\n",
    "if 'end_time' in raw_df.columns:\n",
    "    end_time_missing = raw_df['end_time'].isna().sum()\n",
    "    end_time_total = len(raw_df)\n",
    "    print(f\"Missing end_time in raw data: {end_time_missing}/{end_time_total} ({end_time_missing/end_time_total*100:.1f}%)\")\n",
    "else:\n",
    "    print(\" 'end_time' column not found in raw activities\")\n",
    "    print(\"Available columns:\", list(raw_df.columns))\n",
    "\n",
    "# Check detailed jobs data for duration info\n",
    "print(f\"\\n DETAILED JOBS DATA ANALYSIS:\")\n",
    "jobs_df = pd.DataFrame(detailed_jobs)\n",
    "print(f\"Total detailed jobs: {len(detailed_jobs)}\")\n",
    "\n",
    "if detailed_jobs:\n",
    "    sample_job = detailed_jobs[0]\n",
    "    print(\"Sample job structure:\")\n",
    "    for key, value in sample_job.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check for duration-related fields in jobs\n",
    "duration_fields = ['duration', 'durationSeconds', 'endTime', 'endTimeUtc', 'startTime', 'startTimeUtc']\n",
    "available_duration_fields = [field for field in duration_fields if field in jobs_df.columns]\n",
    "print(f\"\\nAvailable duration-related fields in jobs: {available_duration_fields}\")\n",
    "\n",
    "for field in available_duration_fields:\n",
    "    if field in jobs_df.columns:\n",
    "        missing_count = jobs_df[field].isna().sum()\n",
    "        total_count = len(jobs_df)\n",
    "        print(f\"  {field}: {missing_count}/{total_count} missing ({missing_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c52379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fix Duration Calculation Using Job Data\n",
    "\n",
    "print(\" IMPLEMENTING DURATION FIX\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a copy of merged_df to avoid modifying the original\n",
    "fixed_df = merged_df.copy()\n",
    "\n",
    "# Convert job times to datetime if they aren't already\n",
    "if 'job_start_time' in fixed_df.columns:\n",
    "    fixed_df['job_start_time'] = pd.to_datetime(fixed_df['job_start_time'], utc=True, errors='coerce')\n",
    "\n",
    "if 'endTimeUtc' in fixed_df.columns:\n",
    "    fixed_df['job_end_time'] = pd.to_datetime(fixed_df['endTimeUtc'], utc=True, errors='coerce')\n",
    "else:\n",
    "    # Create job_end_time from endTimeUtc if it exists in the merge\n",
    "    job_columns = [col for col in fixed_df.columns if 'endTime' in col]\n",
    "    print(f\"Available end time columns: {job_columns}\")\n",
    "    \n",
    "    if job_columns:\n",
    "        end_time_col = job_columns[0]  # Use the first available end time column\n",
    "        fixed_df['job_end_time'] = pd.to_datetime(fixed_df[end_time_col], utc=True, errors='coerce')\n",
    "\n",
    "# Fix end_time: Use job end time when activity end time is missing\n",
    "print(\"Fixing end_time...\")\n",
    "original_missing_end_time = fixed_df['end_time'].isna().sum()\n",
    "print(f\"  Activities missing end_time: {original_missing_end_time}\")\n",
    "\n",
    "if 'job_end_time' in fixed_df.columns:\n",
    "    # Fill missing end_time with job_end_time\n",
    "    fixed_df['end_time'] = fixed_df['end_time'].fillna(fixed_df['job_end_time'])\n",
    "    \n",
    "    after_fix_missing_end_time = fixed_df['end_time'].isna().sum()\n",
    "    fixed_count = original_missing_end_time - after_fix_missing_end_time\n",
    "    print(f\"  Fixed {fixed_count} missing end times using job data\")\n",
    "    print(f\"  Remaining missing end_time: {after_fix_missing_end_time}\")\n",
    "\n",
    "# Recalculate duration_seconds\n",
    "print(\"Recalculating duration...\")\n",
    "def calculate_duration(start_time, end_time):\n",
    "    if pd.isna(start_time) or pd.isna(end_time):\n",
    "        return 0.0\n",
    "    try:\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        return max(0.0, duration)  # Ensure non-negative duration\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "fixed_df['duration_seconds'] = fixed_df.apply(\n",
    "    lambda row: calculate_duration(row['start_time'], row['end_time']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Update the merged_activities list with fixed data\n",
    "fixed_activities = fixed_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Show improvement statistics\n",
    "original_zero_duration = (merged_df['duration_seconds'] == 0.0).sum()\n",
    "fixed_zero_duration = (fixed_df['duration_seconds'] == 0.0).sum()\n",
    "improvement = original_zero_duration - fixed_zero_duration\n",
    "\n",
    "print(f\"\\n IMPROVEMENT STATISTICS:\")\n",
    "print(f\"  Original zero duration records: {original_zero_duration}\")\n",
    "print(f\"  Fixed zero duration records: {fixed_zero_duration}\")  \n",
    "print(f\"  Records with duration restored: {improvement}\")\n",
    "\n",
    "# Update the global variables for downstream analysis\n",
    "merged_activities = fixed_activities\n",
    "merged_df = fixed_df\n",
    "\n",
    "print(\" Duration fix applied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Regenerate Analysis with Fixed Duration Data\n",
    "\n",
    "print(\" REGENERATING ANALYSIS WITH FIXED DURATION DATA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Re-prepare DataFrame for Analysis with fixed data\n",
    "df_pd_fixed = pd.DataFrame(merged_activities)\n",
    "\n",
    "# Ensure critical columns exist\n",
    "for c in expected_cols:\n",
    "    if c not in df_pd_fixed.columns:\n",
    "        df_pd_fixed[c] = None\n",
    "\n",
    "# Filter for Failures\n",
    "final_df_fixed = df_pd_fixed[df_pd_fixed[\"status\"] == \"Failed\"].copy()\n",
    "\n",
    "# Regenerate Analysis DataFrame with fixed duration\n",
    "analysis_df_fixed = pd.DataFrame()\n",
    "\n",
    "# Workspace  \n",
    "analysis_df_fixed[\"Workspace\"] = coalesce_series(\n",
    "    final_df_fixed[\"workspace_name\"], \n",
    "    final_df_fixed[\"workspace_id\"]\n",
    ").fillna(\"Unknown\")\n",
    "\n",
    "# Item Name\n",
    "analysis_df_fixed[\"Item Name\"] = final_df_fixed[\"item_name\"].fillna(\"Unknown\")\n",
    "\n",
    "# Item Type\n",
    "analysis_df_fixed[\"Item Type\"] = final_df_fixed[\"item_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Invoke Type\n",
    "analysis_df_fixed[\"Invoke Type\"] = final_df_fixed[\"activity_type\"]\n",
    "\n",
    "# Time & Duration (FIXED)\n",
    "analysis_df_fixed[\"Start Time\"] = final_df_fixed[\"start_time\"]\n",
    "analysis_df_fixed[\"End Time\"] = final_df_fixed[\"end_time\"]\n",
    "analysis_df_fixed[\"Duration (s)\"] = final_df_fixed[\"duration_seconds\"]\n",
    "\n",
    "# User ID\n",
    "analysis_df_fixed[\"User ID\"] = final_df_fixed[\"submitted_by\"]\n",
    "\n",
    "# User Name\n",
    "analysis_df_fixed[\"User Name\"] = final_df_fixed[\"submitted_by\"].apply(extract_user_name)\n",
    "analysis_df_fixed[\"User Name\"] = analysis_df_fixed[\"User Name\"].fillna(analysis_df_fixed[\"User ID\"])\n",
    "\n",
    "# Error Details\n",
    "analysis_df_fixed[\"Error Message\"] = coalesce_series(\n",
    "    final_df_fixed[\"failure_reason\"], \n",
    "    final_df_fixed[\"error_message\"], \n",
    "    final_df_fixed[\"error_code\"]\n",
    ").fillna(\"Unknown Error\")\n",
    "\n",
    "analysis_df_fixed[\"Error Code\"] = final_df_fixed[\"error_code\"]\n",
    "\n",
    "# Show duration improvement\n",
    "zero_duration_original = (analysis_df[\"Duration (s)\"] == 0.0).sum()\n",
    "zero_duration_fixed = (analysis_df_fixed[\"Duration (s)\"] == 0.0).sum()\n",
    "non_zero_duration_fixed = (analysis_df_fixed[\"Duration (s)\"] > 0.0).sum()\n",
    "\n",
    "print(f\" DURATION ANALYSIS IMPROVEMENT:\")\n",
    "print(f\"  Original analysis - Zero duration failures: {zero_duration_original}\")\n",
    "print(f\"  Fixed analysis - Zero duration failures: {zero_duration_fixed}\")\n",
    "print(f\"  Fixed analysis - Non-zero duration failures: {non_zero_duration_fixed}\")\n",
    "print(f\"  Improvement: {zero_duration_original - zero_duration_fixed} failures now have duration data\")\n",
    "\n",
    "# Show sample of fixed data\n",
    "print(f\"\\n SAMPLE OF FIXED ANALYSIS DATA:\")\n",
    "print(analysis_df_fixed[analysis_df_fixed[\"Duration (s)\"] > 0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38599c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Enhanced Duration-Based Analysis\n",
    "\n",
    "print(\" ENHANCED DURATION-BASED ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Filter for failures with valid durations\n",
    "valid_duration_failures = analysis_df_fixed[analysis_df_fixed[\"Duration (s)\"] > 0].copy()\n",
    "\n",
    "if len(valid_duration_failures) > 0:\n",
    "    print(f\" DURATION STATISTICS:\")\n",
    "    print(f\"  Failures with duration data: {len(valid_duration_failures)}\")\n",
    "    print(f\"  Average failure duration: {valid_duration_failures['Duration (s)'].mean():.2f} seconds\")\n",
    "    print(f\"  Median failure duration: {valid_duration_failures['Duration (s)'].median():.2f} seconds\") \n",
    "    print(f\"  Max failure duration: {valid_duration_failures['Duration (s)'].max():.2f} seconds\")\n",
    "    print(f\"  Min failure duration: {valid_duration_failures['Duration (s)'].min():.2f} seconds\")\n",
    "    \n",
    "    # Duration percentiles\n",
    "    percentiles = [25, 50, 75, 90, 95, 99]\n",
    "    print(f\"\\n  Duration Percentiles:\")\n",
    "    for p in percentiles:\n",
    "        value = valid_duration_failures['Duration (s)'].quantile(p/100)\n",
    "        print(f\"    {p}th percentile: {value:.2f}s\")\n",
    "    \n",
    "    # Longest running failures\n",
    "    print(f\"\\n TOP 10 LONGEST RUNNING FAILURES:\")\n",
    "    longest_failures = valid_duration_failures.nlargest(10, \"Duration (s)\")\n",
    "    for idx, row in longest_failures.iterrows():\n",
    "        print(f\"  {row['Duration (s)']:.1f}s - {row['Workspace']} / {row['Item Name']} ({row['Item Type']})\")\n",
    "    \n",
    "    # Quick vs Long failures\n",
    "    quick_threshold = 30  # 30 seconds\n",
    "    long_threshold = 300  # 5 minutes\n",
    "    \n",
    "    quick_failures = len(valid_duration_failures[valid_duration_failures[\"Duration (s)\"] <= quick_threshold])\n",
    "    long_failures = len(valid_duration_failures[valid_duration_failures[\"Duration (s)\"] >= long_threshold])\n",
    "    \n",
    "    print(f\"\\n FAILURE CATEGORIES BY DURATION:\")\n",
    "    print(f\"  Quick failures (≤{quick_threshold}s): {quick_failures}\")\n",
    "    print(f\"  Long failures (≥{long_threshold}s): {long_failures}\")\n",
    "    print(f\"  Medium failures: {len(valid_duration_failures) - quick_failures - long_failures}\")\n",
    "    \n",
    "else:\n",
    "    print(\" No failures with valid duration data found\")\n",
    "\n",
    "print(f\"\\n Duration analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the fixed analysis DataFrame\n",
    "analysis_df_fixed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77905ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Advanced Visualizations - Install Required Libraries\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install visualization libraries if not available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import networkx as nx\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    print(\" All visualization libraries already available\")\n",
    "except ImportError as e:\n",
    "    print(f\"Installing missing visualization libraries...\")\n",
    "    missing_libs = []\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        missing_libs.append('matplotlib')\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "    except ImportError:\n",
    "        missing_libs.append('seaborn')\n",
    "    try:\n",
    "        import plotly.express as px\n",
    "    except ImportError:\n",
    "        missing_libs.append('plotly')\n",
    "    try:\n",
    "        import networkx as nx\n",
    "    except ImportError:\n",
    "        missing_libs.append('networkx')\n",
    "    try:\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "    except ImportError:\n",
    "        missing_libs.append('scikit-learn')\n",
    "    \n",
    "    if missing_libs:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + missing_libs)\n",
    "        print(f\" Installed: {', '.join(missing_libs)}\")\n",
    "\n",
    "# Import after installation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" Visualization environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86754775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Failure Timeline Analysis with Interactive Charts\n",
    "\n",
    "print(\" CREATING FAILURE TIMELINE ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Prepare data for timeline analysis\n",
    "timeline_df = analysis_df_fixed.copy()\n",
    "timeline_df['Start Time'] = pd.to_datetime(timeline_df['Start Time'])\n",
    "timeline_df['Date'] = timeline_df['Start Time'].dt.date\n",
    "timeline_df['Hour'] = timeline_df['Start Time'].dt.hour\n",
    "\n",
    "# Daily failure trends\n",
    "daily_failures = timeline_df.groupby('Date').size().reset_index(name='Failure Count')\n",
    "daily_failures['Date'] = pd.to_datetime(daily_failures['Date'])\n",
    "\n",
    "# Interactive timeline using Plotly\n",
    "fig_timeline = px.line(daily_failures, x='Date', y='Failure Count',\n",
    "                      title=' Daily Failure Trends',\n",
    "                      labels={'Failure Count': 'Number of Failures', 'Date': 'Date'})\n",
    "fig_timeline.update_traces(line_color='red', line_width=3)\n",
    "fig_timeline.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    showlegend=False\n",
    ")\n",
    "fig_timeline.show()\n",
    "\n",
    "# Hourly heatmap\n",
    "hourly_daily = timeline_df.groupby(['Date', 'Hour']).size().reset_index(name='Failures')\n",
    "hourly_pivot = hourly_daily.pivot(index='Date', columns='Hour', values='Failures').fillna(0)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(hourly_pivot, cmap='Reds', cbar_kws={'label': 'Number of Failures'})\n",
    "plt.title(' Failure Heatmap: Date vs Hour of Day', fontsize=16, pad=20)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Date', fontsize=12)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\" Timeline analysis complete - {len(daily_failures)} days analyzed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1137ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Duration Distribution Analysis with Advanced Charts\n",
    "\n",
    "print(\" CREATING DURATION DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "# Filter for valid durations\n",
    "duration_data = analysis_df_fixed[analysis_df_fixed[\"Duration (s)\"] > 0].copy()\n",
    "\n",
    "if len(duration_data) > 0:\n",
    "    # Duration histogram with multiple views\n",
    "    fig_duration = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Duration Histogram', 'Duration Box Plot', \n",
    "                       'Log Scale Duration', 'Duration by Item Type'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Histogram\n",
    "    fig_duration.add_trace(\n",
    "        go.Histogram(x=duration_data['Duration (s)'], name='Duration', \n",
    "                    nbinsx=50, marker_color='skyblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Box plot\n",
    "    fig_duration.add_trace(\n",
    "        go.Box(y=duration_data['Duration (s)'], name='Duration Distribution',\n",
    "               marker_color='lightcoral'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Log scale histogram\n",
    "    fig_duration.add_trace(\n",
    "        go.Histogram(x=duration_data['Duration (s)'], name='Log Duration', \n",
    "                    nbinsx=50, marker_color='lightgreen'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig_duration.update_xaxes(type=\"log\", row=2, col=1)\n",
    "    \n",
    "    # Duration by Item Type\n",
    "    item_type_duration = duration_data.groupby('Item Type')['Duration (s)'].mean().sort_values(ascending=True)\n",
    "    fig_duration.add_trace(\n",
    "        go.Bar(x=item_type_duration.values, y=item_type_duration.index, \n",
    "               orientation='h', name='Avg Duration by Type', marker_color='gold'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig_duration.update_layout(height=800, title_text=\" Comprehensive Duration Analysis\", showlegend=False)\n",
    "    fig_duration.show()\n",
    "    \n",
    "    # Static matplotlib version for duration distribution\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Duration histogram\n",
    "    ax1.hist(duration_data['Duration (s)'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Duration (seconds)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Duration Distribution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Duration vs Item Type violin plot\n",
    "    item_types = duration_data['Item Type'].unique()[:10]  # Top 10 types\n",
    "    filtered_data = duration_data[duration_data['Item Type'].isin(item_types)]\n",
    "    if len(filtered_data) > 0:\n",
    "        sns.violinplot(data=filtered_data, x='Item Type', y='Duration (s)', ax=ax2)\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax2.set_title('Duration by Item Type')\n",
    "    \n",
    "    # Duration percentile analysis\n",
    "    percentiles = range(10, 100, 10)\n",
    "    percentile_values = [duration_data['Duration (s)'].quantile(p/100) for p in percentiles]\n",
    "    ax3.plot(percentiles, percentile_values, marker='o', linewidth=2, color='red')\n",
    "    ax3.set_xlabel('Percentile')\n",
    "    ax3.set_ylabel('Duration (seconds)')\n",
    "    ax3.set_title('Duration Percentiles')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Duration scatter: Start Time vs Duration\n",
    "    ax4.scatter(duration_data['Start Time'], duration_data['Duration (s)'], alpha=0.6, color='purple')\n",
    "    ax4.set_xlabel('Start Time')\n",
    "    ax4.set_ylabel('Duration (seconds)')\n",
    "    ax4.set_title('Duration Over Time')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\" Duration analysis complete - {len(duration_data)} failures with duration data\")\n",
    "else:\n",
    "    print(\" No duration data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Workspace and Item Analysis with Heat Maps\n",
    "\n",
    "print(\" CREATING WORKSPACE & ITEM ANALYSIS\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Workspace failure matrix\n",
    "workspace_item_matrix = analysis_df_fixed.groupby(['Workspace', 'Item Type']).size().reset_index(name='Failures')\n",
    "workspace_pivot = workspace_item_matrix.pivot(index='Workspace', columns='Item Type', values='Failures').fillna(0)\n",
    "\n",
    "# Interactive heatmap for workspace vs item type\n",
    "fig_heatmap = px.imshow(workspace_pivot, \n",
    "                       title=' Workspace vs Item Type Failure Matrix',\n",
    "                       labels=dict(x=\"Item Type\", y=\"Workspace\", color=\"Failures\"),\n",
    "                       aspect=\"auto\", color_continuous_scale='Reds')\n",
    "fig_heatmap.update_layout(height=max(400, len(workspace_pivot) * 40))\n",
    "fig_heatmap.show()\n",
    "\n",
    "# Top failing workspace-item combinations\n",
    "print(\"\\n TOP WORKSPACE-ITEM FAILURE COMBINATIONS:\")\n",
    "top_combinations = analysis_df_fixed.groupby(['Workspace', 'Item Name', 'Item Type']).size().sort_values(ascending=False).head(15)\n",
    "for idx, ((workspace, item_name, item_type), count) in enumerate(top_combinations.items(), 1):\n",
    "    print(f\"{idx:2d}. {workspace} / {item_name} ({item_type}): {count} failures\")\n",
    "\n",
    "# Create sunburst chart for hierarchical view\n",
    "if len(analysis_df_fixed) > 0:\n",
    "    # Prepare data for sunburst\n",
    "    sunburst_data = analysis_df_fixed.groupby(['Workspace', 'Item Type', 'Item Name']).size().reset_index(name='Failures')\n",
    "    \n",
    "    fig_sunburst = px.sunburst(sunburst_data, \n",
    "                              path=['Workspace', 'Item Type', 'Item Name'], \n",
    "                              values='Failures',\n",
    "                              title=' Hierarchical Failure View: Workspace → Item Type → Item Name')\n",
    "    fig_sunburst.show()\n",
    "\n",
    "# User activity heatmap\n",
    "user_hour_matrix = timeline_df.groupby(['User Name', 'Hour']).size().reset_index(name='Failures')\n",
    "user_hour_pivot = user_hour_matrix.pivot(index='User Name', columns='Hour', values='Failures').fillna(0)\n",
    "\n",
    "# Show only top 15 users by failure count\n",
    "top_users = analysis_df_fixed['User Name'].value_counts().head(15).index\n",
    "user_hour_pivot_filtered = user_hour_pivot.loc[user_hour_pivot.index.isin(top_users)]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(user_hour_pivot_filtered, cmap='Oranges', cbar_kws={'label': 'Number of Failures'})\n",
    "plt.title(' User Activity Heatmap: Top Users vs Hour of Day', fontsize=16, pad=20)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('User Name', fontsize=12)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\" Workspace & item analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Error Analysis Dashboard with Interactive Visualizations\n",
    "\n",
    "print(\" CREATING ERROR ANALYSIS DASHBOARD\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Error message analysis\n",
    "error_analysis = analysis_df_fixed['Error Message'].value_counts()\n",
    "\n",
    "# Interactive error distribution pie chart\n",
    "fig_errors = px.pie(values=error_analysis.values[:15], names=error_analysis.index[:15],\n",
    "                   title=' Top 15 Error Message Distribution')\n",
    "fig_errors.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig_errors.show()\n",
    "\n",
    "# Error trends over time\n",
    "timeline_df['Error Message'] = timeline_df['Error Message'].fillna('Unknown Error')\n",
    "error_timeline = timeline_df.groupby(['Date', 'Error Message']).size().reset_index(name='Count')\n",
    "\n",
    "# Get top 5 errors for timeline\n",
    "top_errors = analysis_df_fixed['Error Message'].value_counts().head(5).index.tolist()\n",
    "error_timeline_filtered = error_timeline[error_timeline['Error Message'].isin(top_errors)]\n",
    "\n",
    "fig_error_timeline = px.line(error_timeline_filtered, x='Date', y='Count', \n",
    "                            color='Error Message',\n",
    "                            title=' Error Trends Over Time (Top 5 Errors)')\n",
    "fig_error_timeline.update_layout(template='plotly_white')\n",
    "fig_error_timeline.show()\n",
    "\n",
    "# Error correlation matrix with item types\n",
    "error_item_matrix = analysis_df_fixed.groupby(['Error Message', 'Item Type']).size().reset_index(name='Count')\n",
    "error_item_pivot = error_item_matrix.pivot(index='Error Message', columns='Item Type', values='Count').fillna(0)\n",
    "\n",
    "# Show top 10 errors\n",
    "top_10_errors = error_analysis.head(10).index\n",
    "error_item_pivot_filtered = error_item_pivot.loc[error_item_pivot.index.isin(top_10_errors)]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(error_item_pivot_filtered, cmap='Reds', annot=True, fmt='.0f', \n",
    "            cbar_kws={'label': 'Number of Occurrences'})\n",
    "plt.title(' Error Message vs Item Type Correlation Matrix', fontsize=16, pad=20)\n",
    "plt.xlabel('Item Type', fontsize=12)\n",
    "plt.ylabel('Error Message', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a comprehensive error insights table\n",
    "print(\"\\n ERROR INSIGHTS TABLE:\")\n",
    "error_insights = []\n",
    "for error_msg in error_analysis.head(10).index:\n",
    "    error_data = analysis_df_fixed[analysis_df_fixed['Error Message'] == error_msg]\n",
    "    \n",
    "    insights = {\n",
    "        'Error': error_msg[:50] + '...' if len(error_msg) > 50 else error_msg,\n",
    "        'Count': len(error_data),\n",
    "        'Avg_Duration': error_data[error_data['Duration (s)'] > 0]['Duration (s)'].mean() if len(error_data[error_data['Duration (s)'] > 0]) > 0 else 0,\n",
    "        'Top_Workspace': error_data['Workspace'].mode()[0] if len(error_data) > 0 else 'N/A',\n",
    "        'Top_Item_Type': error_data['Item Type'].mode()[0] if len(error_data) > 0 else 'N/A',\n",
    "        'Unique_Users': error_data['User Name'].nunique()\n",
    "    }\n",
    "    error_insights.append(insights)\n",
    "\n",
    "error_insights_df = pd.DataFrame(error_insights)\n",
    "error_insights_df['Avg_Duration'] = error_insights_df['Avg_Duration'].round(2)\n",
    "print(error_insights_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n Error analysis dashboard complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Performance Insights and Predictive Analytics\n",
    "\n",
    "print(\" CREATING PERFORMANCE INSIGHTS & PREDICTIVE ANALYTICS\")\n",
    "print(\"=\" * 58)\n",
    "\n",
    "# Performance metrics calculation\n",
    "perf_metrics = {}\n",
    "\n",
    "# Calculate failure rates by various dimensions\n",
    "total_records = len(analysis_df_fixed)\n",
    "perf_metrics['total_failures'] = total_records\n",
    "\n",
    "# Failure rate by workspace\n",
    "workspace_failure_rate = analysis_df_fixed.groupby('Workspace').size().sort_values(ascending=False)\n",
    "perf_metrics['worst_workspace'] = workspace_failure_rate.index[0] if len(workspace_failure_rate) > 0 else 'N/A'\n",
    "perf_metrics['worst_workspace_failures'] = workspace_failure_rate.iloc[0] if len(workspace_failure_rate) > 0 else 0\n",
    "\n",
    "# Failure rate by user\n",
    "user_failure_rate = analysis_df_fixed.groupby('User Name').size().sort_values(ascending=False)\n",
    "perf_metrics['most_errors_user'] = user_failure_rate.index[0] if len(user_failure_rate) > 0 else 'N/A'\n",
    "perf_metrics['most_errors_count'] = user_failure_rate.iloc[0] if len(user_failure_rate) > 0 else 0\n",
    "\n",
    "# Failure rate by item type\n",
    "item_type_failure_rate = analysis_df_fixed.groupby('Item Type').size().sort_values(ascending=False)\n",
    "\n",
    "# Performance dashboard\n",
    "fig_perf = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Failures by Workspace', 'Failures by Item Type', \n",
    "                   'Failures by User (Top 15)', 'Daily Failure Trend'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "# Workspace failures\n",
    "top_workspaces = workspace_failure_rate.head(10)\n",
    "fig_perf.add_trace(\n",
    "    go.Bar(x=top_workspaces.values, y=top_workspaces.index, orientation='h', \n",
    "           name='Workspace Failures', marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Item type pie chart\n",
    "fig_perf.add_trace(\n",
    "    go.Pie(labels=item_type_failure_rate.head(8).index, \n",
    "           values=item_type_failure_rate.head(8).values,\n",
    "           name='Item Type Distribution'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# User failures\n",
    "top_users_perf = user_failure_rate.head(15)\n",
    "fig_perf.add_trace(\n",
    "    go.Bar(x=top_users_perf.index, y=top_users_perf.values,\n",
    "           name='User Failures', marker_color='lightcoral'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Daily trend\n",
    "fig_perf.add_trace(\n",
    "    go.Scatter(x=daily_failures['Date'], y=daily_failures['Failure Count'],\n",
    "               mode='lines+markers', name='Daily Failures', line_color='darkred'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_perf.update_layout(height=900, title_text=\" Performance Dashboard Overview\", showlegend=False)\n",
    "fig_perf.update_xaxes(tickangle=45, row=2, col=1)\n",
    "fig_perf.show()\n",
    "\n",
    "# Predictive insights based on patterns\n",
    "print(\"\\n PREDICTIVE INSIGHTS:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Peak failure times\n",
    "peak_hour = timeline_df['Hour'].mode()[0] if len(timeline_df) > 0 else 'N/A'\n",
    "peak_day = timeline_df['Date'].value_counts().index[0] if len(timeline_df) > 0 else 'N/A'\n",
    "\n",
    "print(f\" Peak Failure Hour: {peak_hour}:00\")\n",
    "print(f\" Worst Failure Day: {peak_day}\")\n",
    "\n",
    "# Average failure patterns\n",
    "if len(duration_data) > 0:\n",
    "    avg_failure_duration = duration_data['Duration (s)'].mean()\n",
    "    print(f\" Average Failure Duration: {avg_failure_duration:.2f} seconds\")\n",
    "    \n",
    "    # Duration-based risk categorization\n",
    "    quick_failures = len(duration_data[duration_data['Duration (s)'] <= 30])\n",
    "    medium_failures = len(duration_data[(duration_data['Duration (s)'] > 30) & (duration_data['Duration (s)'] <= 300)])\n",
    "    long_failures = len(duration_data[duration_data['Duration (s)'] > 300])\n",
    "    \n",
    "    print(f\" Quick Failures (≤30s): {quick_failures} ({quick_failures/len(duration_data)*100:.1f}%)\")\n",
    "    print(f\" Medium Failures (30s-5m): {medium_failures} ({medium_failures/len(duration_data)*100:.1f}%)\")  \n",
    "    print(f\" Long Failures (>5m): {long_failures} ({long_failures/len(duration_data)*100:.1f}%)\")\n",
    "\n",
    "# Risk assessment\n",
    "print(f\"\\n RISK ASSESSMENT:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "risk_factors = []\n",
    "if perf_metrics['worst_workspace_failures'] > total_records * 0.3:\n",
    "    risk_factors.append(f\"High workspace concentration: {perf_metrics['worst_workspace']} has {perf_metrics['worst_workspace_failures']} failures\")\n",
    "\n",
    "if perf_metrics['most_errors_count'] > total_records * 0.2:\n",
    "    risk_factors.append(f\"High user concentration: {perf_metrics['most_errors_user']} has {perf_metrics['most_errors_count']} failures\")\n",
    "\n",
    "if len(risk_factors) > 0:\n",
    "    for i, factor in enumerate(risk_factors, 1):\n",
    "        print(f\"{i}. {factor}\")\n",
    "else:\n",
    "    print(\" No major risk concentrations detected\")\n",
    "\n",
    "print(f\"\\n Performance insights analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff618f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Advanced Correlation Analysis and Network Visualization\n",
    "\n",
    "print(\" CREATING CORRELATION & NETWORK ANALYSIS\")\n",
    "print(\"=\" * 46)\n",
    "\n",
    "# Create correlation analysis between different failure dimensions\n",
    "correlation_data = analysis_df_fixed.copy()\n",
    "\n",
    "# Install sklearn if not available and import\n",
    "try:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "except ImportError:\n",
    "    print(\"Installing scikit-learn for correlation analysis...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    print(\" scikit-learn installed successfully\")\n",
    "\n",
    "categorical_cols = ['Workspace', 'Item Name', 'Item Type', 'User Name', 'Error Message']\n",
    "encoded_data = correlation_data.copy()\n",
    "\n",
    "# Encode categorical variables for correlation analysis\n",
    "print(\" Encoding categorical variables for correlation analysis...\")\n",
    "for col in categorical_cols:\n",
    "    if col in encoded_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Handle missing values by converting to string first\n",
    "        encoded_data[f'{col}_encoded'] = le.fit_transform(encoded_data[col].astype(str).fillna('Unknown'))\n",
    "\n",
    "# Calculate correlations between numeric columns\n",
    "numeric_cols = ['Duration (s)'] + [f'{col}_encoded' for col in categorical_cols if f'{col}_encoded' in encoded_data.columns]\n",
    "correlation_matrix = encoded_data[numeric_cols].corr()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(' Failure Factor Correlation Matrix', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Network analysis - connection between workspaces and item types\n",
    "print(\"\\n Creating network visualization...\")\n",
    "\n",
    "# Install networkx if not available\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    print(\"Installing networkx for network analysis...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'networkx'])\n",
    "    import networkx as nx\n",
    "    print(\" networkx installed successfully\")\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between workspaces and item types weighted by failure count\n",
    "workspace_item_connections = analysis_df_fixed.groupby(['Workspace', 'Item Type']).size().reset_index(name='weight')\n",
    "\n",
    "for _, row in workspace_item_connections.iterrows():\n",
    "    if row['weight'] > 1:  # Only include connections with more than 1 failure\n",
    "        G.add_edge(f\"WS: {row['Workspace']}\", f\"IT: {row['Item Type']}\", weight=row['weight'])\n",
    "\n",
    "# Create network visualization\n",
    "if len(G.nodes()) > 0:\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "\n",
    "    # Draw nodes\n",
    "    workspace_nodes = [n for n in G.nodes() if n.startswith('WS:')]\n",
    "    itemtype_nodes = [n for n in G.nodes() if n.startswith('IT:')]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=workspace_nodes, node_color='lightblue', \n",
    "                          node_size=1000, alpha=0.8, label='Workspaces')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=itemtype_nodes, node_color='lightcoral', \n",
    "                          node_size=800, alpha=0.8, label='Item Types')\n",
    "\n",
    "    # Draw edges with thickness based on weight\n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['weight'] for u, v in edges]\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    normalized_weights = [max(1, w/max_weight * 5) for w in weights]  # Normalize for better visualization\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, width=normalized_weights, alpha=0.6, edge_color='gray')\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "    plt.title(' Workspace-ItemType Failure Network', fontsize=16, pad=20)\n",
    "    plt.legend()\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\" No network connections found (need >1 failure per workspace-itemtype pair)\")\n",
    "\n",
    "# Co-occurrence analysis - which errors tend to happen together\n",
    "print(\"\\n ERROR CO-OCCURRENCE ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Group by user and time window to find errors that occur together\n",
    "correlation_data['Time_Window'] = correlation_data['Start Time'].dt.floor('H')  # Hour windows\n",
    "error_cooccurrence = correlation_data.groupby(['User Name', 'Time_Window'])['Error Message'].apply(list).reset_index()\n",
    "\n",
    "# Find pairs of errors that occur together frequently\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "error_pairs = []\n",
    "for error_list in error_cooccurrence['Error Message']:\n",
    "    if len(error_list) > 1:\n",
    "        pairs = list(combinations(set(error_list), 2))  # Get unique pairs\n",
    "        error_pairs.extend(pairs)\n",
    "\n",
    "if error_pairs:\n",
    "    pair_counts = Counter(error_pairs)\n",
    "    top_pairs = pair_counts.most_common(10)\n",
    "\n",
    "    print(\"Top 10 Error Co-occurrences (same user, same hour):\")\n",
    "    for i, ((error1, error2), count) in enumerate(top_pairs, 1):\n",
    "        error1_short = error1[:30] + '...' if len(error1) > 30 else error1\n",
    "        error2_short = error2[:30] + '...' if len(error2) > 30 else error2\n",
    "        print(f\"{i:2d}. {error1_short}  {error2_short} ({count} times)\")\n",
    "else:\n",
    "    print(\"ℹ No error co-occurrences found (errors don't typically happen together)\")\n",
    "\n",
    "print(f\"\\n Correlation & network analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Executive Summary Dashboard and Key Metrics\n",
    "\n",
    "print(\" CREATING EXECUTIVE SUMMARY DASHBOARD\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Calculate key executive metrics\n",
    "exec_metrics = {}\n",
    "\n",
    "# Overall health metrics\n",
    "total_failures = len(analysis_df_fixed)\n",
    "unique_workspaces = analysis_df_fixed['Workspace'].nunique()\n",
    "unique_users = analysis_df_fixed['User Name'].nunique()\n",
    "unique_items = analysis_df_fixed['Item Name'].nunique()\n",
    "date_range = (timeline_df['Date'].max() - timeline_df['Date'].min()).days + 1\n",
    "\n",
    "exec_metrics.update({\n",
    "    'total_failures': total_failures,\n",
    "    'unique_workspaces': unique_workspaces,\n",
    "    'unique_users': unique_users,\n",
    "    'unique_items': unique_items,\n",
    "    'analysis_period_days': date_range,\n",
    "    'daily_avg_failures': total_failures / date_range if date_range > 0 else 0\n",
    "})\n",
    "\n",
    "# Duration metrics (if available)\n",
    "duration_available = len(analysis_df_fixed[analysis_df_fixed['Duration (s)'] > 0])\n",
    "exec_metrics['duration_coverage'] = (duration_available / total_failures * 100) if total_failures > 0 else 0\n",
    "\n",
    "if duration_available > 0:\n",
    "    duration_stats = analysis_df_fixed[analysis_df_fixed['Duration (s)'] > 0]['Duration (s)']\n",
    "    exec_metrics.update({\n",
    "        'avg_failure_duration': duration_stats.mean(),\n",
    "        'max_failure_duration': duration_stats.max(),\n",
    "        'median_failure_duration': duration_stats.median()\n",
    "    })\n",
    "\n",
    "# Create executive dashboard\n",
    "fig_exec = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=(' Failure Volume Overview', ' Duration Analysis', \n",
    "                   ' Top Affected Workspaces', ' User Impact Distribution',\n",
    "                   ' Daily Failure Trend', ' Key Performance Indicators'),\n",
    "    specs=[[{\"type\": \"indicator\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"pie\"}], \n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"table\"}]]\n",
    ")\n",
    "\n",
    "# KPI Indicators\n",
    "fig_exec.add_trace(\n",
    "    go.Indicator(\n",
    "        mode=\"number+gauge+delta\",\n",
    "        value=total_failures,\n",
    "        title={\"text\": \"Total Failures\"},\n",
    "        gauge={'axis': {'range': [None, total_failures * 1.5]},\n",
    "               'bar': {'color': \"darkred\"},\n",
    "               'steps': [{'range': [0, total_failures * 0.5], 'color': \"lightgray\"},\n",
    "                        {'range': [total_failures * 0.5, total_failures], 'color': \"gray\"}],\n",
    "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
    "                           'thickness': 0.75, 'value': total_failures}}\n",
    "    ), row=1, col=1\n",
    ")\n",
    "\n",
    "# Duration distribution\n",
    "if duration_available > 0:\n",
    "    duration_bins = ['<30s', '30s-5m', '5m-1h', '>1h']\n",
    "    duration_counts = [\n",
    "        len(duration_stats[duration_stats <= 30]),\n",
    "        len(duration_stats[(duration_stats > 30) & (duration_stats <= 300)]),\n",
    "        len(duration_stats[(duration_stats > 300) & (duration_stats <= 3600)]),\n",
    "        len(duration_stats[duration_stats > 3600])\n",
    "    ]\n",
    "    \n",
    "    fig_exec.add_trace(\n",
    "        go.Bar(x=duration_bins, y=duration_counts, name='Duration Distribution',\n",
    "               marker_color=['green', 'yellow', 'orange', 'red']),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Top workspaces\n",
    "top_ws = analysis_df_fixed['Workspace'].value_counts().head(8)\n",
    "fig_exec.add_trace(\n",
    "    go.Bar(x=top_ws.index, y=top_ws.values, name='Workspace Failures',\n",
    "           marker_color='lightblue'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# User impact pie chart\n",
    "user_impact_bins = ['1 failure', '2-5 failures', '6-10 failures', '>10 failures']\n",
    "user_failures = analysis_df_fixed['User Name'].value_counts()\n",
    "user_impact_counts = [\n",
    "    len(user_failures[user_failures == 1]),\n",
    "    len(user_failures[(user_failures >= 2) & (user_failures <= 5)]),\n",
    "    len(user_failures[(user_failures >= 6) & (user_failures <= 10)]),\n",
    "    len(user_failures[user_failures > 10])\n",
    "]\n",
    "\n",
    "fig_exec.add_trace(\n",
    "    go.Pie(labels=user_impact_bins, values=user_impact_counts, name='User Impact'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Daily trend\n",
    "fig_exec.add_trace(\n",
    "    go.Scatter(x=daily_failures['Date'], y=daily_failures['Failure Count'],\n",
    "               mode='lines+markers', name='Daily Failures', \n",
    "               line=dict(color='red', width=3)),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# KPI Table\n",
    "kpi_data = [\n",
    "    ['Total Failures', f\"{exec_metrics['total_failures']:,}\"],\n",
    "    ['Analysis Period', f\"{exec_metrics['analysis_period_days']} days\"],\n",
    "    ['Daily Average', f\"{exec_metrics['daily_avg_failures']:.1f}\"],\n",
    "    ['Affected Workspaces', f\"{exec_metrics['unique_workspaces']}\"],\n",
    "    ['Affected Users', f\"{exec_metrics['unique_users']}\"], \n",
    "    ['Affected Items', f\"{exec_metrics['unique_items']}\"],\n",
    "    ['Duration Coverage', f\"{exec_metrics['duration_coverage']:.1f}%\"]\n",
    "]\n",
    "\n",
    "if 'avg_failure_duration' in exec_metrics:\n",
    "    kpi_data.extend([\n",
    "        ['Avg Duration', f\"{exec_metrics['avg_failure_duration']:.1f}s\"],\n",
    "        ['Max Duration', f\"{exec_metrics['max_failure_duration']:.1f}s\"]\n",
    "    ])\n",
    "\n",
    "fig_exec.add_trace(\n",
    "    go.Table(\n",
    "        header=dict(values=['Metric', 'Value'], fill_color='lightblue'),\n",
    "        cells=dict(values=list(zip(*kpi_data)), fill_color='white')\n",
    "    ), row=3, col=2\n",
    ")\n",
    "\n",
    "fig_exec.update_layout(height=1200, title_text=\" Executive Summary Dashboard\", showlegend=False)\n",
    "fig_exec.update_xaxes(tickangle=45, row=2, col=1)\n",
    "fig_exec.show()\n",
    "\n",
    "# Summary insights\n",
    "print(f\"\\n EXECUTIVE SUMMARY:\")\n",
    "print(\"=\" * 22)\n",
    "print(f\" Analysis Period: {exec_metrics['analysis_period_days']} days\")\n",
    "print(f\" Total Failures: {exec_metrics['total_failures']:,}\")\n",
    "print(f\" Daily Average: {exec_metrics['daily_avg_failures']:.1f} failures/day\")\n",
    "print(f\" Workspaces Affected: {exec_metrics['unique_workspaces']}\")\n",
    "print(f\" Users Affected: {exec_metrics['unique_users']}\")\n",
    "print(f\" Items Affected: {exec_metrics['unique_items']}\")\n",
    "\n",
    "if 'avg_failure_duration' in exec_metrics:\n",
    "    print(f\" Average Failure Duration: {exec_metrics['avg_failure_duration']:.1f} seconds\")\n",
    "    print(f\" Duration Data Coverage: {exec_metrics['duration_coverage']:.1f}%\")\n",
    "\n",
    "print(f\"\\n Executive summary dashboard complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Export Analysis Results and Generate Reports\n",
    "\n",
    "print(\" CREATING EXPORT & REPORTING FUNCTIONALITY\")\n",
    "print(\"=\" * 48)\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create comprehensive results dictionary for export\n",
    "export_results = {\n",
    "    'analysis_metadata': {\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'total_failures_analyzed': len(analysis_df_fixed),\n",
    "        'analysis_period_days': date_range,\n",
    "        'duration_data_coverage_pct': exec_metrics['duration_coverage'],\n",
    "        'notebook_version': 'Monitor_Hub_Analysis_Fix_v1.0'\n",
    "    },\n",
    "    \n",
    "    'summary_statistics': {\n",
    "        'total_failures': exec_metrics['total_failures'],\n",
    "        'unique_workspaces': exec_metrics['unique_workspaces'],\n",
    "        'unique_users': exec_metrics['unique_users'],\n",
    "        'unique_items': exec_metrics['unique_items'],\n",
    "        'daily_average_failures': exec_metrics['daily_avg_failures']\n",
    "    },\n",
    "    \n",
    "    'top_insights': {\n",
    "        'worst_workspace': workspace_failure_rate.index[0] if len(workspace_failure_rate) > 0 else 'N/A',\n",
    "        'worst_workspace_failures': int(workspace_failure_rate.iloc[0]) if len(workspace_failure_rate) > 0 else 0,\n",
    "        'most_affected_user': user_failure_rate.index[0] if len(user_failure_rate) > 0 else 'N/A',\n",
    "        'most_affected_user_failures': int(user_failure_rate.iloc[0]) if len(user_failure_rate) > 0 else 0,\n",
    "        'most_common_error': error_analysis.index[0] if len(error_analysis) > 0 else 'N/A',\n",
    "        'most_common_error_count': int(error_analysis.iloc[0]) if len(error_analysis) > 0 else 0,\n",
    "        'peak_failure_hour': int(peak_hour) if peak_hour != 'N/A' else None,\n",
    "        'worst_failure_day': str(peak_day) if peak_day != 'N/A' else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add duration statistics if available\n",
    "if 'avg_failure_duration' in exec_metrics:\n",
    "    export_results['duration_statistics'] = {\n",
    "        'average_duration_seconds': exec_metrics['avg_failure_duration'],\n",
    "        'median_duration_seconds': exec_metrics['median_failure_duration'],\n",
    "        'max_duration_seconds': exec_metrics['max_failure_duration'],\n",
    "        'records_with_duration': duration_available,\n",
    "        'quick_failures_under_30s': quick_failures if 'quick_failures' in locals() else 0,\n",
    "        'medium_failures_30s_to_5m': medium_failures if 'medium_failures' in locals() else 0,\n",
    "        'long_failures_over_5m': long_failures if 'long_failures' in locals() else 0\n",
    "    }\n",
    "\n",
    "# Generate detailed tables for export\n",
    "print(\" GENERATING DETAILED EXPORT TABLES:\")\n",
    "\n",
    "# Top 20 failing items\n",
    "top_failing_items = analysis_df_fixed.groupby(['Workspace', 'Item Name', 'Item Type']).size().sort_values(ascending=False).head(20)\n",
    "export_results['top_failing_items'] = [\n",
    "    {\n",
    "        'workspace': workspace,\n",
    "        'item_name': item_name, \n",
    "        'item_type': item_type,\n",
    "        'failure_count': int(count)\n",
    "    }\n",
    "    for (workspace, item_name, item_type), count in top_failing_items.items()\n",
    "]\n",
    "\n",
    "# Top 20 error messages\n",
    "top_errors_export = error_analysis.head(20)\n",
    "export_results['top_error_messages'] = [\n",
    "    {\n",
    "        'error_message': error_msg,\n",
    "        'occurrence_count': int(count),\n",
    "        'percentage_of_total': round((count / len(analysis_df_fixed)) * 100, 2)\n",
    "    }\n",
    "    for error_msg, count in top_errors_export.items()\n",
    "]\n",
    "\n",
    "# User impact analysis\n",
    "user_impact_export = user_failure_rate.head(20)\n",
    "export_results['top_affected_users'] = [\n",
    "    {\n",
    "        'user_name': user,\n",
    "        'failure_count': int(count),\n",
    "        'percentage_of_total': round((count / len(analysis_df_fixed)) * 100, 2)\n",
    "    }\n",
    "    for user, count in user_impact_export.items()\n",
    "]\n",
    "\n",
    "# Daily failure trends\n",
    "daily_trends_export = daily_failures.copy()\n",
    "daily_trends_export['Date'] = daily_trends_export['Date'].astype(str)\n",
    "export_results['daily_failure_trends'] = daily_trends_export.to_dict('records')\n",
    "\n",
    "# Save results to JSON file\n",
    "output_file = f\"{OUTPUT_DIR}/failure_analysis_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "try:\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(export_results, f, indent=2, default=str)\n",
    "    print(f\" Analysis results exported to: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\" Could not save to file system: {e}\")\n",
    "    print(\" Results available in memory as 'export_results' variable\")\n",
    "\n",
    "# Create CSV exports for key tables\n",
    "try:\n",
    "    # Export main analysis data\n",
    "    csv_file = f\"{OUTPUT_DIR}/failure_analysis_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    analysis_df_fixed.to_csv(csv_file, index=False)\n",
    "    print(f\" Detailed failure data exported to: {csv_file}\")\n",
    "    \n",
    "    # Export summary tables\n",
    "    summary_file = f\"{OUTPUT_DIR}/failure_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    summary_data = []\n",
    "    summary_data.append(['Metric', 'Value'])\n",
    "    for key, value in export_results['summary_statistics'].items():\n",
    "        summary_data.append([key.replace('_', ' ').title(), value])\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data[1:], columns=summary_data[0])\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\" Summary statistics exported to: {summary_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" Could not save CSV files: {e}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n ANALYSIS COMPLETE - COMPREHENSIVE REPORT GENERATED\")\n",
    "print(\"=\" * 54)\n",
    "print(f\" Total Visualizations Created: 16+ charts and graphs\")\n",
    "print(f\" Analysis Types: Timeline, Heatmaps, Distributions, Correlations, Networks\")\n",
    "print(f\" Insights Generated: Performance, Predictive, Executive Summary\")\n",
    "print(f\" Export Formats: JSON, CSV, In-Memory DataFrames\") \n",
    "print(f\" Smart Merge Technology: Utilized throughout all analyses\")\n",
    "\n",
    "# Show what's available for further analysis\n",
    "print(f\"\\n AVAILABLE DATA OBJECTS FOR FURTHER ANALYSIS:\")\n",
    "print(f\"  - analysis_df_fixed: Main analysis DataFrame ({len(analysis_df_fixed)} records)\")\n",
    "print(f\"  - export_results: Complete analysis results dictionary\")\n",
    "print(f\"  - duration_data: Failures with duration information ({len(duration_data)} records)\")\n",
    "print(f\"  - timeline_df: Time-based analysis data\")\n",
    "print(f\"  - daily_failures: Daily aggregated failure counts\")\n",
    "\n",
    "print(f\"\\n Export and reporting functionality complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
