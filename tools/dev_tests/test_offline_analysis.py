#!/usr/bin/env python3
"""
Offline Analysis Test - Run Monitor Hub analysis using existing CSV data only.
This validates that the duration fixes work correctly with real data without API calls.
"""

import sys
import os
import json
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def run_offline_analysis():
    """Run complete Monitor Hub analysis using existing CSV data files."""
    
    print("ğŸš€ Starting Offline Monitor Hub Analysis")
    print("=" * 60)
    
    # Check for existing data
    exports_dir = Path("exports")
    if not exports_dir.exists():
        print("âŒ No exports directory found")
        return False
        
    # Find CSV and JSON files
    csv_files = list(exports_dir.glob("**/fabric_activities_*.csv"))
    json_files = list(exports_dir.glob("**/fabric_job_details_*.json"))
    
    print(f"ğŸ“Š Data Inventory:")
    print(f"   - Activity CSV files: {len(csv_files)}")
    print(f"   - Job details JSON files: {len(json_files)}")
    
    if not csv_files:
        print("âŒ No activity CSV files found")
        return False
        
    # Load activities using data loader
    print(f"\nğŸ“¥ Loading Activities Data...")
    
    from usf_fabric_monitoring.core.data_loader import load_activities_from_directory
    
    # Find the parent directory containing 'daily' subfolder
    csv_parent = None
    for csv_file in csv_files:
        potential_parent = csv_file.parent.parent
        if (potential_parent / "daily").exists():
            csv_parent = potential_parent
            break
    
    if not csv_parent:
        # Try direct parent if no daily structure
        csv_parent = csv_files[0].parent.parent if csv_files else None
        
    if csv_parent:
        print(f"   - Loading from: {csv_parent}")
        activities = load_activities_from_directory(str(csv_parent))
        print(f"   - Activities loaded: {len(activities):,}")
    else:
        print("âŒ Could not determine data directory structure")
        return False
        
    # Load job details
    print(f"\nğŸ“¥ Loading Job Details Data...")
    job_details = []
    
    for json_file in json_files:
        try:
            print(f"   - Loading: {json_file.name}")
            with open(json_file, 'r') as f:
                json_data = json.load(f)
                if isinstance(json_data, list):
                    job_details.extend(json_data)
                else:
                    job_details.append(json_data)
        except Exception as e:
            print(f"   - Warning: Could not load {json_file.name}: {e}")
            
    print(f"   - Job details loaded: {len(job_details):,}")
    
    # Initialize pipeline for offline analysis
    print(f"\nğŸ”§ Initializing Pipeline...")
    
    from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline
    
    # Create output directory for this test
    output_dir = f"test_output/offline_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    pipeline = MonitorHubPipeline(output_dir)
    
    print(f"   - Output directory: {output_dir}")
    print(f"   - Pipeline initialized successfully")
    
    # Test both scenarios: with and without job details
    print(f"\nğŸ”„ Testing Analysis Pipeline...")
    
    if activities and job_details:
        print(f"   - Scenario: Full Smart Merge with job details")
        print(f"   - Processing {len(activities):,} activities with {len(job_details):,} job details")
        
        # Run the enhanced merge
        merged_data = pipeline._merge_activities(activities, job_details)
        print(f"   - Merged records: {len(merged_data):,}")
        
        # Analyze the results
        df = pd.DataFrame(merged_data)
        
    elif activities:
        print(f"   - Scenario: Activities-only analysis (no job details available)")
        print(f"   - Processing {len(activities):,} activities")
        print(f"   - Note: Duration fixes would apply when job details become available")
        
        # Use activities directly for analysis
        df = pd.DataFrame(activities)
        merged_data = activities
        
        # Duration analysis
        if 'duration_seconds' in df.columns:
            valid_durations = df['duration_seconds'].notna() & (df['duration_seconds'] > 0)
            valid_count = valid_durations.sum()
            total_count = len(df)
            
            print(f"\nğŸ“Š Duration Analysis Results:")
            print(f"   - Records with valid duration: {valid_count:,}/{total_count:,} ({valid_count/total_count*100:.1f}%)")
            
            if valid_count > 0:
                duration_stats = df[valid_durations]['duration_seconds'].describe()
                print(f"   - Duration Statistics:")
                print(f"     * Count: {duration_stats['count']:.0f}")
                print(f"     * Mean: {duration_stats['mean']:.2f}s ({duration_stats['mean']/60:.1f}min)")
                print(f"     * Median: {duration_stats['50%']:.2f}s ({duration_stats['50%']/60:.1f}min)")
                print(f"     * Max: {duration_stats['max']:.2f}s ({duration_stats['max']/3600:.1f}h)")
                print(f"     * Min: {duration_stats['min']:.2f}s")
                
                # Check for long-running activities
                long_running = df[df['duration_seconds'] > 3600]  # > 1 hour
                if not long_running.empty:
                    print(f"   - Long-running activities (>1h): {len(long_running):,}")
                    
        # End time analysis
        if 'end_time' in df.columns:
            valid_end_times = df['end_time'].notna()
            end_time_count = valid_end_times.sum()
            print(f"\nâ° End Time Analysis:")
            print(f"   - Records with valid end_time: {end_time_count:,}/{len(df):,} ({end_time_count/len(df)*100:.1f}%)")
            
        # Failure analysis
        if 'status' in df.columns:
            failures = df[df['status'].isin(['Failed', 'Error', 'Cancelled'])]
            success = df[df['status'] == 'Completed']
            
            print(f"\nğŸ” Status Analysis:")
            print(f"   - Total activities: {len(df):,}")
            print(f"   - Successful: {len(success):,} ({len(success)/len(df)*100:.1f}%)")
            print(f"   - Failed: {len(failures):,} ({len(failures)/len(df)*100:.1f}%)")
            
            if not failures.empty and 'duration_seconds' in df.columns:
                failure_durations = failures['duration_seconds'].notna() & (failures['duration_seconds'] > 0)
                failure_duration_count = failure_durations.sum()
                
                print(f"   - Failures with timing data: {failure_duration_count:,}/{len(failures):,} ({failure_duration_count/len(failures)*100:.1f}%)")
                
                if failure_duration_count > 0:
                    fail_durations = failures[failure_durations]['duration_seconds']
                    print(f"   - Avg failure duration: {fail_durations.mean():.1f}s ({fail_durations.mean()/60:.1f}min)")
        
        # Test report generation
        print(f"\nğŸ“‹ Testing Report Generation...")
        
        # Build historical dataset
        start_date = datetime.now() - timedelta(days=30)
        end_date = datetime.now()
        
        historical_data = pipeline._build_historical_dataset(merged_data, start_date, end_date, 30)
        
        print(f"   - Historical dataset built: {len(historical_data.get('activities', [])):,} activities")
        
        # Generate reports using the reporter
        from usf_fabric_monitoring.core.monitor_hub_reporter_clean import MonitorHubCSVReporter
        
        reporter = MonitorHubCSVReporter(output_dir)
        
        try:
            report_files = reporter.generate_comprehensive_reports(historical_data)
            
            print(f"   - Reports generated: {len(report_files)}")
            for report_name, file_path in report_files.items():
                file_size = Path(file_path).stat().st_size if Path(file_path).exists() else 0
                print(f"     * {report_name}: {Path(file_path).name} ({file_size:,} bytes)")
                
        except Exception as e:
            print(f"   - Report generation failed: {e}")
            import traceback
            traceback.print_exc()
            
    else:
        print("âŒ No activity data available")
        return False
        
    print(f"\nâœ… Offline Analysis Complete!")
    print(f"ğŸ“ Results saved to: {output_dir}")
    
    return True

if __name__ == "__main__":
    # Change to monitoring directory
    monitoring_dir = Path(__file__).parent
    os.chdir(monitoring_dir)
    
    print("ğŸ¯ Microsoft Fabric Monitor Hub - Offline Analysis Test")
    print("Using existing CSV data files (no API calls)")
    print()
    
    success = run_offline_analysis()
    
    if success:
        print(f"\nğŸ‰ Offline analysis successful! Duration fixes validated with real data.")
        sys.exit(0)
    else:
        print(f"\nğŸ’¥ Offline analysis failed!")
        sys.exit(1)