import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import pandas as pd

# Opt into the future pandas behaviour to suppress FutureWarning on fillna
pd.set_option("future.no_silent_downcasting", True)

# Lazy-load extraction functions from repo-level scripts.
# These scripts live outside the installed package tree, so we use
# importlib.util to load them by file path‚Äîno sys.path mutation needed.
_run_historical_extraction = None
_run_item_details_extraction = None

_project_root = Path(__file__).resolve().parent.parent.parent.parent  # core -> usf_fabric_monitoring -> src -> root
_scripts_dir = _project_root / "scripts"


def _import_script_function(script_name: str, func_name: str):
    """Dynamically import a function from a script in the scripts directory."""
    import importlib.util

    script_path = _scripts_dir / f"{script_name}.py"
    if not script_path.exists():
        raise ImportError(f"Script not found: {script_path}")
    spec = importlib.util.spec_from_file_location(script_name, script_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return getattr(module, func_name)


def _get_historical_extraction():
    global _run_historical_extraction
    if _run_historical_extraction is None:
        _run_historical_extraction = _import_script_function("extract_historical_data", "extract_historical_data")
    return _run_historical_extraction


def _get_item_details_extraction():
    global _run_item_details_extraction
    if _run_item_details_extraction is None:
        _run_item_details_extraction = _import_script_function(
            "extract_fabric_item_details", "run_item_details_extraction"
        )
    return _run_item_details_extraction


from usf_fabric_monitoring.core.data_loader import load_activities_from_directory
from usf_fabric_monitoring.core.logger import setup_logging
from usf_fabric_monitoring.core.monitor_hub_reporter_clean import MonitorHubCSVReporter
from usf_fabric_monitoring.core.utils import resolve_path


class MonitorHubPipeline:
    """Complete Monitor Hub analysis pipeline"""

    def __init__(self, output_directory: str = None):
        """Initialize the pipeline"""
        # Configure logging for the entire package so scripts inherit it
        self.logger = setup_logging(name="usf_fabric_monitoring", log_file="logs/monitor_hub_pipeline.log")

        # Use environment configuration for output directory
        if output_directory is None:
            # Default to exports/monitor_hub_analysis, resolved to Lakehouse if in Fabric
            self.output_directory = resolve_path(os.getenv("EXPORT_DIRECTORY", "exports/monitor_hub_analysis"))
        else:
            # Resolve user provided path to ensure it lands in Lakehouse if relative
            self.output_directory = resolve_path(output_directory)

        self.output_directory.mkdir(parents=True, exist_ok=True)

        self.reporter = None
        self.max_days = int(os.getenv("MAX_HISTORICAL_DAYS", "28"))

        self.logger.info("Monitor Hub Pipeline initialized")

    def run_complete_analysis(self, days: int = 90, *, tenant_wide: bool = True) -> dict[str, Any]:
        """
        Run complete Monitor Hub analysis pipeline

        Args:
            days: Number of days of historical data to analyze

        Returns:
            Dictionary with analysis results and report file paths
        """
        resolved_days = self._resolve_days(days)
        self.logger.info(f"Starting Monitor Hub analysis for {resolved_days} days (API max {self.max_days})")

        try:
            start_date, end_date = self._calculate_date_range(resolved_days)
            extraction_dir = self._prepare_extraction_directory()

            self.logger.info("Step 1: Extracting historical activities from Fabric APIs")
            run_historical_extraction = _get_historical_extraction()
            extraction_result = run_historical_extraction(
                start_date=start_date,
                end_date=end_date,
                output_dir=str(extraction_dir),
                tenant_wide=tenant_wide,
            )

            if extraction_result.get("status") != "success":
                message = extraction_result.get("message", "Historical extraction failed")
                self.logger.warning(message)
                return {"status": "error", "message": message, "report_files": {}}

            if extraction_result.get("failed_days"):
                self.logger.warning(
                    f"‚ö†Ô∏è  Failed to extract data for {len(extraction_result['failed_days'])} days: {extraction_result['failed_days']}"
                )

            self.logger.info("Step 1b: Extracting detailed job history (for accurate failure tracking)")
            # We use a separate directory for item details, but we can pass it if needed.
            # The default is exports/fabric_item_details, which matches what we load later.
            details_output_dir = self.output_directory / "fabric_item_details"

            # Check for recent detailed job extraction (8-hour cache logic)
            details_cache_valid = self._check_recent_job_details_extraction(details_output_dir)

            if details_cache_valid:
                self.logger.info("‚úÖ Using cached detailed job data (within 8 hours)")
                details_result = {"status": "success", "cached": True, "jobs_count": "cached"}
            else:
                run_item_details_extraction = _get_item_details_extraction()
                details_result = run_item_details_extraction(output_dir=str(details_output_dir))

            if details_result.get("status") != "success":
                self.logger.warning(f"Detailed job extraction failed: {details_result.get('message')}")
                # We proceed, but warn that failure data might be incomplete
            else:
                if details_result.get("cached"):
                    self.logger.info("‚úÖ Using cached detailed job records")
                else:
                    self.logger.info(f"‚úÖ Extracted {details_result.get('jobs_count', 0)} new job records")

            if os.getenv("USF_FABRIC_MONITORING_SHOW_PROGRESS", "1") == "1":
                self.logger.info("=" * 40)
                self.logger.info("‚úÖ Extraction Complete. Starting Analysis...")
                self.logger.info("=" * 40)

            self.logger.info("Step 2: Loading enriched activity exports")
            activities = load_activities_from_directory(str(extraction_dir))

            self.logger.info("Step 2b: Loading detailed job history (for accurate status)")
            detailed_jobs = self._load_detailed_jobs()
            activities = self._merge_activities(activities, detailed_jobs)

            if not activities:
                self.logger.warning("No activities loaded from exports")
                return {"status": "no_data", "message": "No activities data available for analysis"}

            historical_data = self._build_historical_dataset(activities, start_date, end_date, resolved_days)
            self.logger.info(f"‚úÖ Loaded {len(activities)} activities for analysis")

            self.logger.info("Step 2c: Persisting merged data to Parquet (Source of Truth)")
            self._save_to_parquet(historical_data)

            self.logger.info("Step 3: Generating comprehensive CSV reports")
            self.reporter = MonitorHubCSVReporter(str(self.output_directory))
            report_files = self.reporter.generate_comprehensive_reports(historical_data)
            self.logger.info(f"‚úÖ Generated {len(report_files)} comprehensive reports")

            pipeline_summary = self._create_pipeline_summary(historical_data, report_files, resolved_days)

            return {
                "status": "success",
                "summary": pipeline_summary,
                "report_files": report_files,
                "raw_data_summary": {
                    "total_activities": len(historical_data["activities"]),
                    "analysis_period_days": resolved_days,
                    "workspaces_analyzed": len(historical_data.get("workspaces", [])),
                    "items_analyzed": len(historical_data.get("items", [])),
                },
            }

        except Exception as e:
            self.logger.error(f"Pipeline failed: {str(e)}")
            return {"status": "error", "message": str(e), "report_files": {}}

    def _save_to_parquet(self, historical_data: dict[str, Any]) -> None:
        """
        Save merged data to Parquet files for Delta Table ingestion.

        Uses microsecond timestamp precision for Spark compatibility.

        Args:
            historical_data: The dictionary containing merged activities, workspaces, and items.
        """
        parquet_dir = self.output_directory / "parquet"
        parquet_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Parquet write options for Spark compatibility
        # Spark doesn't support TIMESTAMP(NANOS,true) - must use microseconds
        parquet_kwargs = {
            "index": False,
            "coerce_timestamps": "us",  # Microsecond precision
            "allow_truncated_timestamps": True,
        }

        # 1. Save Activities
        activities = historical_data.get("activities", [])
        if activities:
            try:
                df_activities = pd.DataFrame(activities)
                # Ensure datetime columns are properly typed for Parquet
                for col in ["start_time", "end_time", "creation_time"]:
                    if col in df_activities.columns:
                        df_activities[col] = pd.to_datetime(df_activities[col], errors="coerce")

                activities_path = parquet_dir / f"activities_{timestamp}.parquet"
                df_activities.to_parquet(activities_path, **parquet_kwargs)
                self.logger.info(f"Saved {len(activities)} activities to {activities_path}")
            except Exception as e:
                self.logger.error(f"Failed to save activities to parquet: {e}")

        # 2. Save Workspaces
        workspaces = historical_data.get("workspaces", [])
        if workspaces:
            try:
                df_workspaces = pd.DataFrame(workspaces)
                workspaces_path = parquet_dir / f"workspaces_{timestamp}.parquet"
                df_workspaces.to_parquet(workspaces_path, **parquet_kwargs)
                self.logger.info(f"Saved {len(workspaces)} workspaces to {workspaces_path}")
            except Exception as e:
                self.logger.error(f"Failed to save workspaces to parquet: {e}")

        # 3. Save Items
        items = historical_data.get("items", [])
        if items:
            try:
                df_items = pd.DataFrame(items)
                items_path = parquet_dir / f"items_{timestamp}.parquet"
                df_items.to_parquet(items_path, **parquet_kwargs)
                self.logger.info(f"Saved {len(items)} items to {items_path}")
            except Exception as e:
                self.logger.error(f"Failed to save items to parquet: {e}")

    def _create_pipeline_summary(
        self, historical_data: dict[str, Any], report_files: dict[str, str], days: int
    ) -> dict[str, Any]:
        """Create summary of pipeline execution"""

        # Calculate basic statistics
        activities = historical_data.get("activities", [])
        total_activities = len(activities)

        if total_activities > 0:
            failed_activities = sum(1 for a in activities if a.get("status") == "Failed")
            success_rate = ((total_activities - failed_activities) / total_activities) * 100

            # Calculate duration statistics
            durations = [a.get("duration_seconds", 0) for a in activities]
            total_duration_seconds = sum(durations)
            avg_duration_seconds = total_duration_seconds / total_activities if total_activities > 0 else 0
        else:
            failed_activities = 0
            success_rate = 0
            total_duration_seconds = 0
            avg_duration_seconds = 0

        # Get unique counts
        workspaces = set(a.get("workspace_id") for a in activities if a.get("workspace_id"))
        users = set(a.get("submitted_by") for a in activities if a.get("submitted_by"))
        domains = set(a.get("domain") for a in activities if a.get("domain"))
        item_types = set(a.get("item_type") for a in activities if a.get("item_type"))

        summary = {
            "pipeline_execution": {
                "executed_at": datetime.now().isoformat(),
                "analysis_period_days": days,
                "total_reports_generated": len(report_files),
            },
            "key_measurables": {
                "total_activities": total_activities,
                "failed_activities": failed_activities,
                "success_rate_percent": round(success_rate, 2),
                "total_duration_hours": round(total_duration_seconds / 3600, 2),
            },
            "analysis_scope": {
                "unique_workspaces": len(workspaces),
                "unique_users": len(users),
                "unique_domains": len(domains),
                "unique_item_types": len(item_types),
            },
            "report_files": report_files,
        }

        # Save summary to file
        summary_file = self.output_directory / f"pipeline_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, default=str)

        self.logger.info(f"Pipeline summary saved to: {summary_file}")

        return summary

    def _resolve_days(self, requested_days: int | None) -> int:
        default_days = int(os.getenv("DEFAULT_ANALYSIS_DAYS", "7"))
        if not requested_days:
            requested_days = default_days
        return min(max(1, requested_days), self.max_days)

    def _calculate_date_range(self, days: int) -> tuple[datetime, datetime]:
        end_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=1)
        start_date = end_date - timedelta(days=days - 1)
        return start_date, end_date

    def _prepare_extraction_directory(self) -> Path:
        # Use a persistent directory for raw data to enable caching/incremental updates
        extraction_dir = self.output_directory / "raw_data"
        extraction_dir.mkdir(parents=True, exist_ok=True)
        return extraction_dir

    def _check_recent_job_details_extraction(self, details_output_dir: Path, hours_threshold: int = 8) -> bool:
        """Check if we have recent detailed job data within the threshold."""
        try:
            if not details_output_dir.exists():
                return False

            # Find all jobs_*.json files
            job_files = list(details_output_dir.glob("jobs_*.json"))
            if not job_files:
                return False

            # Get the most recent file by modification time - using sorted to avoid max() key issues
            job_files_with_time = [(f, f.stat().st_mtime) for f in job_files]
            job_files_with_time.sort(key=lambda x: x[1], reverse=True)  # Sort by time, newest first
            most_recent_file, _ = job_files_with_time[0]

            # Check if it's within the threshold
            file_time = datetime.fromtimestamp(most_recent_file.stat().st_mtime)
            current_time = datetime.now()
            hours_old = (current_time - file_time).total_seconds() / 3600

            if hours_old <= hours_threshold:
                self.logger.info(f"Found recent job details file: {most_recent_file.name} ({hours_old:.1f} hours old)")
                return True
            else:
                self.logger.info(
                    f"Most recent job details file is {hours_old:.1f} hours old (threshold: {hours_threshold}h)"
                )
                return False

        except Exception as e:
            self.logger.warning(f"Error checking job details cache: {e}")
            return False

    def _load_detailed_jobs(self) -> list[dict[str, Any]]:
        """Load all detailed jobs exports"""
        export_dir = self.output_directory / "fabric_item_details"
        if not export_dir.exists():
            self.logger.warning(f"Detailed jobs directory not found: {export_dir}")
            return []

        # Find all jobs_*.json files
        job_files = list(export_dir.glob("jobs_*.json"))
        if not job_files:
            self.logger.warning("No detailed job files found")
            return []

        all_jobs = []
        for job_file in job_files:
            self.logger.info(f"Loading detailed jobs from {job_file}")
            try:
                with open(job_file) as f:
                    jobs = json.load(f)
                    all_jobs.extend(jobs)
            except Exception as e:
                self.logger.error(f"Failed to load detailed jobs from {job_file}: {e}")

        self.logger.info(f"Total detailed jobs loaded: {len(all_jobs)}")
        return all_jobs

    def _merge_activities(
        self, activities: list[dict[str, Any]], detailed_jobs: list[dict[str, Any]]
    ) -> list[dict[str, Any]]:
        """
        Smart Merge: Correlate Activity Events with Detailed Job History.
        Uses pandas merge_asof to align events by ItemId and Time (within 5 mins).
        """
        if not activities:
            return []
        if not detailed_jobs:
            return activities

        self.logger.info("Starting Smart Merge of Activities and Detailed Jobs...")

        # 1. Prepare Activities DataFrame
        df_activities = pd.DataFrame(activities)

        # Ensure timestamps are datetime
        time_cols = ["start_time", "end_time", "creation_time"]
        for col in time_cols:
            if col in df_activities.columns:
                df_activities[col] = pd.to_datetime(df_activities[col], utc=True, errors="coerce")

        # Ensure item_id is string and handle NaNs
        if "item_id" in df_activities.columns:
            df_activities["item_id"] = df_activities["item_id"].astype(str)

        # Sort for merge_asof
        df_activities = df_activities.sort_values("start_time")

        # 2. Prepare Jobs DataFrame
        df_jobs = pd.DataFrame(detailed_jobs)

        # Map Job columns to friendly names
        job_rename_map = {
            "id": "job_instance_id",
            "startTimeUtc": "job_start_time",
            "endTimeUtc": "job_end_time",
            "status": "job_status",
            "failureReason": "job_failure_reason",
            "itemId": "item_id",
            "jobType": "job_type",
            "invoker": "job_invoker",
        }
        df_jobs = df_jobs.rename(columns=job_rename_map)

        # Ensure timestamps are datetime
        job_time_cols = ["job_start_time", "job_end_time"]
        for col in job_time_cols:
            if col in df_jobs.columns:
                df_jobs[col] = pd.to_datetime(df_jobs[col], utc=True, errors="coerce")

        # Ensure item_id is string
        if "item_id" in df_jobs.columns:
            df_jobs["item_id"] = df_jobs["item_id"].astype(str)

        # Drop rows without start time or item_id
        df_jobs = df_jobs.dropna(subset=["job_start_time", "item_id"])

        # Sort for merge_asof
        df_jobs = df_jobs.sort_values("job_start_time")

        # 3. Perform Merge
        # We want to keep all activities, and find the nearest job execution
        try:
            merged_df = pd.merge_asof(
                df_activities,
                df_jobs,
                left_on="start_time",
                right_on="job_start_time",
                by="item_id",
                tolerance=pd.Timedelta("5min"),
                direction="nearest",
                suffixes=("", "_job"),
            )

            # 4. Enrich Data

            # Update Status: If Job Failed, mark Activity as Failed
            # (Activity logs often say "Completed" even if the internal job failed)
            mask_job_failed = merged_df["job_status"] == "Failed"
            merged_df.loc[mask_job_failed, "status"] = "Failed"

            # Enrich Failure Reason
            # If activity doesn't have a failure reason, take it from the job
            if "failure_reason" not in merged_df.columns:
                merged_df["failure_reason"] = None

            merged_df["failure_reason"] = (
                merged_df["failure_reason"].fillna(merged_df["job_failure_reason"]).infer_objects(copy=False)
            )

            # Create explicit error_message column if not exists
            if "error_message" not in merged_df.columns:
                merged_df["error_message"] = merged_df["failure_reason"]
            else:
                merged_df["error_message"] = (
                    merged_df["error_message"].fillna(merged_df["failure_reason"]).infer_objects(copy=False)
                )

            # Enrich User (if job has invoker info)
            # Note: Job history often doesn't have user info, but sometimes 'invoker' field exists
            if "job_invoker" in merged_df.columns:
                merged_df["submitted_by"] = (
                    merged_df["submitted_by"].fillna(merged_df["job_invoker"]).infer_objects(copy=False)
                )

            # Fix Duration Calculation: Use job end times when activity end times are missing
            self.logger.info("Applying duration calculation fix...")
            original_missing_end_time = merged_df["end_time"].isna().sum()

            if original_missing_end_time > 0 and "job_end_time" in merged_df.columns:
                # Fill missing end_time with job_end_time
                merged_df["end_time"] = (
                    merged_df["end_time"].fillna(merged_df["job_end_time"]).infer_objects(copy=False)
                )

                after_fix_missing_end_time = merged_df["end_time"].isna().sum()
                fixed_count = original_missing_end_time - after_fix_missing_end_time
                self.logger.info(f"Fixed {fixed_count} missing end times using job data")

                # Recalculate duration_seconds
                def calculate_duration(start_time, end_time):
                    if pd.isna(start_time) or pd.isna(end_time):
                        return 0.0
                    try:
                        duration = (end_time - start_time).total_seconds()
                        return max(0.0, duration)  # Ensure non-negative duration
                    except Exception:
                        return 0.0

                merged_df["duration_seconds"] = merged_df.apply(
                    lambda row: calculate_duration(row["start_time"], row["end_time"]), axis=1
                )

                # Log improvement statistics
                original_zero_duration = (
                    pd.DataFrame(activities).get("duration_seconds", pd.Series([0.0] * len(activities))) == 0.0
                ).sum()
                fixed_zero_duration = (merged_df["duration_seconds"] == 0.0).sum()
                duration_improvement = original_zero_duration - fixed_zero_duration
                if duration_improvement > 0:
                    self.logger.info(f"Duration fix: restored duration data for {duration_improvement} records")

            # Fill remaining NaNs in key columns to avoid serialization issues
            merged_df["status"] = merged_df["status"].fillna("Unknown").infer_objects(copy=False)

            self.logger.info(f"Smart Merge Complete. Enriched {len(merged_df)} activities.")

            # Convert back to list of dicts
            # Handle datetime serialization by converting to ISO format strings
            for col in ["start_time", "end_time", "creation_time", "job_start_time", "job_end_time"]:
                if col in merged_df.columns:
                    merged_df[col] = merged_df[col].apply(lambda x: x.isoformat() if pd.notnull(x) else None)

            return merged_df.to_dict(orient="records")

        except Exception as e:
            self.logger.error(f"Smart Merge failed: {e}")
            self.logger.warning("Falling back to original activities list")
            return activities

    def _build_historical_dataset(
        self, activities: list[dict[str, Any]], start_date: datetime, end_date: datetime, days: int
    ) -> dict[str, Any]:
        workspace_lookup: dict[str, dict[str, Any]] = {}
        item_lookup: dict[str, dict[str, Any]] = {}

        # Filter for Fabric-only activities
        fabric_activities = [a for a in activities if self._is_fabric_activity(a)]
        self.logger.info(f"Filtered {len(activities)} total activities to {len(fabric_activities)} Fabric activities")

        for activity in fabric_activities:
            workspace_id = activity.get("workspace_id")
            if workspace_id and workspace_id not in workspace_lookup:
                workspace_lookup[workspace_id] = {
                    "id": workspace_id,
                    "displayName": activity.get("workspace_name") or "Unknown",
                }

            item_id = activity.get("item_id")
            if item_id and item_id not in item_lookup:
                item_lookup[item_id] = {
                    "id": item_id,
                    "displayName": activity.get("item_name") or "Unknown",
                    "type": activity.get("item_type") or "Unknown",
                }

        return {
            "analysis_period": {
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "days": days,
                "description": f"{days}-day analysis window",
            },
            "workspaces": list(workspace_lookup.values()),
            "items": list(item_lookup.values()),
            "activities": fabric_activities,
        }

    def _is_fabric_activity(self, activity: dict[str, Any]) -> bool:
        """Check if activity is related to a Fabric item type."""
        # If it comes from JobHistory, it is definitely a Fabric activity
        if activity.get("source") == "JobHistory":
            return True

        # Expanded list of known Fabric item types (covering all observed types)
        fabric_types = {
            # Compute
            "DataPipeline",
            "Pipeline",
            "Notebook",
            "SynapseNotebook",
            "SparkJobDefinition",
            "Dataflow",
            "DataFlow",
            "CopyJob",
            # Storage
            "Lakehouse",
            "Warehouse",
            "KQLDatabase",
            "KustoDatabase",
            "MirroredDatabase",
            "SnowflakeDatabase",
            # Analytics
            "KQLQueryset",
            "SemanticModel",
            "Dataset",
            "Report",
            "Dashboard",
            "Datamart",
            # Realtime
            "Eventstream",
            "Reflex",
            # Infrastructure
            "Environment",
            "MLModel",
            "MLExperiment",
            "GraphQLApi",
        }

        # List of known Fabric activity operations
        fabric_ops = {
            "RunArtifact",
            "ExecuteNotebook",
            "ExecutePipeline",
            "ExecuteSparkJob",
            "ExecuteDataflow",
            "ReadLakehouse",
            "WriteLakehouse",
        }

        item_type = activity.get("item_type")
        activity_type = activity.get("activity_type")

        if item_type and item_type in fabric_types:
            return True

        if activity_type and activity_type in fabric_ops:
            return True

        return False

    def print_results_summary(self, results: dict[str, Any]) -> None:
        """Print formatted results summary"""
        self.logger.info("=" * 80)
        self.logger.info("MICROSOFT FABRIC MONITOR HUB ANALYSIS RESULTS")
        self.logger.info("=" * 80)

        if results["status"] == "success":
            summary = results["summary"]

            self.logger.info("üìä KEY MEASURABLES (Statement of Work Requirements):")
            measurables = summary["key_measurables"]
            self.logger.info("   ‚Ä¢ Activities: %s", f"{measurables['total_activities']:,}")
            self.logger.info("   ‚Ä¢ Failed Activity: %s", f"{measurables['failed_activities']:,}")
            self.logger.info("   ‚Ä¢ Success %%: %s%%", measurables["success_rate_percent"])
            self.logger.info("   ‚Ä¢ Total Duration: %s hours", measurables["total_duration_hours"])

            self.logger.info("üîç ANALYSIS SCOPE:")
            scope = summary["analysis_scope"]
            self.logger.info("   ‚Ä¢ Period: %s days", summary["pipeline_execution"]["analysis_period_days"])
            self.logger.info("   ‚Ä¢ Workspaces: %s", f"{scope['unique_workspaces']:,}")
            self.logger.info("   ‚Ä¢ Users: %s", f"{scope['unique_users']:,}")
            self.logger.info("   ‚Ä¢ Domains: %s", f"{scope['unique_domains']:,}")
            self.logger.info("   ‚Ä¢ Item Types: %s", f"{scope['unique_item_types']:,}")

            self.logger.info("üìã REPORTS GENERATED (%d files):", len(results["report_files"]))
            for report_name, file_path in results["report_files"].items():
                file_name = Path(file_path).name
                self.logger.info("   ‚Ä¢ %s: %s", report_name, file_name)

            self.logger.info("üìÅ Export Directory: %s", self.output_directory)

        elif results["status"] == "no_data":
            self.logger.warning("‚ö†Ô∏è  %s", results["message"])

        else:
            self.logger.error("‚ùå Analysis failed: %s", results.get("message", "Unknown error"))

        self.logger.info("=" * 80)
